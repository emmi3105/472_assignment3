---
title: "Assignment 3"
author: "Student ID: 201903536"
date: "7 December 2023"
output: html_document
---

```{r setup, include=FALSE} 
#####################################
# SETUP
#####################################

knitr::opts_chunk$set(echo = FALSE) 

#####################################
# Install/load packages
#####################################

library(rvest)
library(tidyverse)
library(purrr)
library(DBI)
library(RSQLite)
library(RSelenium)
library(netstat)
library(httr)
library(jsonlite)
library(tidycensus)
library(tigris)
library(stringr)
library(tmap)
library(sf)

```


## GitHub

The GitHub repository for **this assignment** can be found [here](https://github.com/emmi3105/472_assignment3).


## Exercise 1

```{r start exercise 1}

#####################################
# EXERCISE 1
#####################################

```


### Create a local relational database

```{r create database, echo=TRUE}

# Create database
db <- dbConnect(RSQLite::SQLite(), "universities_db.sqlite")
```


### Check for the existence of the database

```{r check existence of database, echo=TRUE}

# Check existence of the database
print(file.exists("universities_db.sqlite"))
```

**Note**: The primary key that will be used throughout this assignment is the variable `Institution`, which refers to the name of the university.

## Exercise 2

### a. Gathering structured data 

```{r start exercise 2a and 2b}

#####################################
# EXERCISE 2
#####################################
# 2A and 2B
#####################################

```


**Note**: The solution to Exercise 2a can be found under the solution for Exercise 2b.

### b. Gathering unstructured data

#### Automatic webscraping function for 2a and b

I combined exercise 2a and 2b and directly wrote and executed a webscraping function called `scrape_wikipedia_tables()` that returns a data frame with the following eight variables: 

i. `Institution`: The university’s name, which will be used as the primary key
ii. `Control`: The university’s status (public or private)
iii. `City`: The city in which the university is located
iv. `State`: The state in which the university is located
v. `Wikipedia_URL`: The URL of the university’s dedicated Wikipedia page
vi. `Geographic_Location`: The geographic coordinates of the (main) university campus
vii. `Endowment`: The endowment of the university in USD dollars
viii. `Students`: The total number of students (including both undergraduate and postgraduate)

The resulting data frame is stored under the name `universities_table`.


```{r wikipedia web scraping function}

scrape_wikipedia_tables <- function(url) {
  
  ### Exercise 2a: scrape information on the name, status, city, state and the URL to the Wikipedia page
  table_indices <- c(19, 27)
  
  # Storing the URL's HTML code
  html_content <- read_html(url)
  
  # Extracting all tables in the document 
  tab <- html_table(html_content, fill = TRUE)

  # Extract only the tables of interest given the input table_indices
  selected_tables <- tab[1:length(table_indices)]

  # Iterate over tables and bind rows
  empty_tibble <- tibble()
  for (i in 1:length(selected_tables)) {
    current_table <- as_tibble(selected_tables[[i]][, 1:4])
    empty_tibble <- bind_rows(empty_tibble, current_table)
  }

  # Initialise an empty list to store hyperlinks
  hyperlinks_list <- list()

  # Iterate over the table indices and extract hyperlinks
  for (index in table_indices) {
    table_nodes <- html_content %>%
      html_nodes(paste("#mw-content-text > div.mw-content-ltr.mw-parser-output > table:nth-child(", index, ")", sep = ""))
  
    current_node_set <- table_nodes[[1]]
  
    hyperlinks_current_node <- current_node_set %>%
      html_nodes("td:nth-child(1) a") %>%
      html_attr("href")
  
    # Append the hyperlinks to the list
    hyperlinks_list <- c(hyperlinks_list, hyperlinks_current_node)
  }

  # Unlist to flatten the list
  hyperlinks_list <- unlist(hyperlinks_list, recursive = FALSE)

  # Concatenate the base URL to the extracted hyperlinks
  hyperlinks_list <- paste0("https://en.wikipedia.org/", hyperlinks_list)
  
  # Add a new column 'Wikipedia_URL' with the extracted URLs
  empty_tibble$Wikipedia_URL <- hyperlinks_list

    
  ### Exercise 2b: Add the geographic location, endowment and students
  
  
  #### Geographic location
  geographic_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the geographic location using the class selector 'geo_default'
    geographic_location <- university_page %>%
      html_nodes(".geo-dec") %>%
      html_text() %>%
      unique()
 
    # Store the scraped information in a list
    geographic_data <- append(geographic_data, list(geographic_location))
  }

  # Add a new column 'Geographic_Location' with the scraped data
  empty_tibble$Geographic_Location <- geographic_data

  # Keep the first set of coordinates only
  empty_tibble <- empty_tibble %>%
    mutate(Geographic_Location = map_chr(Geographic_Location, ~ .[1]))
  
  
  #### Endowment and budget
  endowment_data <- list()

  # Clean text by removing text within brackets
  clean_text <- function(text) {
    # Remove text containing brackets within brackets
    text <- gsub("\\([^()]*\\)", "", text)
    # Remove text within square brackets
    text <- gsub("\\[.*?\\]", "", text)
    # Remove text within round brackets
    text <- gsub("\\(.*?\\)", "", text)
    return(text)
  }

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the endowment based on text content (entries with a dollar sign)
    endowment <- university_page %>%
      html_nodes("table.infobox th:contains('Endowment') + td") %>%
      html_text() %>%
      unique() %>%
      clean_text()
  

    # Check if 'Endowment' entry exists, if not, extract 'Budget'
    if (length(endowment) == 0) {
      budget <- university_page %>%
        html_nodes("table.infobox th:contains('Budget') + td") %>%
        html_text() %>%
        unique() %>%
        clean_text()

      endowment_data <- append(endowment_data, list(budget))
    } else {
      endowment_data <- append(endowment_data, list(endowment))
    }
  }

  # Add a new column 'Endowment' with the scraped data
  empty_tibble$Endowment <- endowment_data
  empty_tibble <- empty_tibble %>%
    mutate(Endowment = map_chr(Endowment, ~ .[1]))


  #### Students
  
  # To avoid issues with students being spread on different campuses, scrape total student information by adding information on all undergraduates to all postgraduates
  undergraduates_data <- list()
  postgraduates_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
  
    # Undergraduates 
    # Extract the number of undergraduate students
    undergraduates <- university_page %>%
      html_nodes("table.infobox th:contains('Undergraduates') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_undergraduates <- str_extract(undergraduates, "[\\d,]+")
  
    # Assign NA if cleaned_undergraduates is empty
    cleaned_undergraduates <- ifelse(length(cleaned_undergraduates) == 0, NA, cleaned_undergraduates)
    undergraduates_data <- append(undergraduates_data, list(cleaned_undergraduates))

    # Postgraduates 
    # Extract the number of postgraduate students
    postgraduates <- university_page %>%
      html_nodes("table.infobox th:contains('Postgraduates') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_postgraduates <- str_extract(postgraduates, "[\\d,]+")
  
    # Assign NA if cleaned_postgraduates is empty
    cleaned_postgraduates <- ifelse(length(cleaned_postgraduates) == 0, NA, cleaned_postgraduates)
    postgraduates_data <- append(postgraduates_data, list(cleaned_postgraduates))
  }

  # Remove commas from the extracted data and convert to numeric
  empty_tibble$Undergraduates <- as.numeric(gsub(",", "", unlist(undergraduates_data)))
  empty_tibble$Postgraduates <- as.numeric(gsub(",", "", unlist(postgraduates_data)))

  # Create a new column 'Students' as the sum of Undergraduates and Postgraduates
  empty_tibble$Students <- ifelse(is.na(empty_tibble$Undergraduates), 
                                  empty_tibble$Postgraduates, 
                                  ifelse(is.na(empty_tibble$Postgraduates), 
                                        empty_tibble$Undergraduates, 
                                        empty_tibble$Undergraduates + empty_tibble$Postgraduates))

  # If the website neither holds information on undergraduates nor on postgraduates, look for information on total students
  students_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the students
    students <- university_page %>%
      html_nodes("table.infobox th:contains('Students') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_students <- as.numeric(gsub(",", "", str_extract(students, "[\\d,]+")))
    students_data <- append(students_data, list(cleaned_students))
  }

  # Check if any element in 'Students' is NA before assigning
  if (any(is.na(empty_tibble$Students))) {
    empty_tibble$Students[is.na(empty_tibble$Students)] <- unlist(students_data)
  }

  # Remove columns named 'Undergraduates' and 'Postgraduates'
  empty_tibble <- select(empty_tibble, -Undergraduates, -Postgraduates)
  
  
  # Finally, return the table including the scraped information
  return(empty_tibble)
}
```

```{r call the scrape_wikipedia_tables() webscraper,  eval = FALSE}

# Run the function using the URL from the Wikipedia page
url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"
scraped_wikipedia_df <- scrape_wikipedia_tables(url)

# Save it as a global variable
assign("scraped_wikipedia_df", scraped_wikipedia_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_wikipedia_df, file = "scraped_wikipedia_df.RData")
```

```{r create a copy of scraped_wikipedia_universities}

# Load global variables
load("scraped_wikipedia_df.RData")

# Copy the scraped table to avoid modifying the original data
universities_table <- scraped_wikipedia_df
```


```{r transform the endowment values}

# Function to transform endowment values
transform_endowment <- function(endowment_value) {
  # Extract numeric part and multiplier
  numeric_part <- as.numeric(gsub("[^0-9.]", "", endowment_value))
  multiplier <- ifelse(grepl("billion", tolower(endowment_value)), 1e9,
                       ifelse(grepl("million", tolower(endowment_value)), 1e6, 1))
  
  # Return the transformed numeric value
  return(numeric_part * multiplier)
}

# Apply the transformation to the Endowment column
universities_table$Endowment <- sapply(universities_table$Endowment, transform_endowment)
```

#### Notes on the function

**Geographic Location**: Some universities have several sets of coordinates on their Wikipedia sites. For instance, there are universities with multiple campuses whose geographic locations are all noted down on Wikipedia. Therefore, the function above includes a line of code that specifies that only the very first set of coordinates is appended to the data frame. Thereby, the geographic location column holds simplified data that potentially might not reflect the location of the entire university.

**Endowment**: Not all universities we are interested in include information on the `endowment` on their Wikipedia website. However, some university Wikipedia articles include data on the university's `budget.` Since `endowment` and `budget` likely refer to the same variable, we have included an if-statement in the function above, that specifies the following: If a university's Wikipedia article does not hold information on the 
`endowment`, the function will look whether there is information on the `budget` and use this data instead. Nonetheless, there are a few universities that have neither the `endowment` nor the `budget` declared on Wikipedia. For those instances, the endowment variable in our table will hold a missing value as specified in the section below.

**Students**: On some websites, the total student body is not declared in a comprehensive way because it is split up by the different campuses of the university. Therefore, it is more efficient to scrape the data by adding the number of postgraduates and undergraduates instead of writing a function that can add all of the student bodies from different campuses of the individual universities. To ensure that also universities that only have postgraduates or undergraduates, and universities which only have a number for the total student body on their website, the code is modified. Firstly, an if-statement will ensure that for those instances with NA's for either postgraduates or undergraduates, the number from the variable without the NA is declared as the total student body. Secondly, for the instances that only include data on the total student body on the website, another statement is formulated that scrapes the `Students` data from the Wikipedia website and then adds this to the variable in our scraped data set.

#### Notes on missing values

**Geographic Location**: There are missing values in the geographic location column. To be exact, the [University of Colorado Denver](https://en.wikipedia.org//wiki/University_of_Colorado_Denver) and the [University of Mississippi](https://en.wikipedia.org//wiki/University_of_Mississippi) do not have information on the geographic location on their Wikipedia websites. Missing values are declared as NA in our data frame.

**Endowment**: For some universities, the Wikipedia website neither includes data on `endowment` nor on `budget`. More precisely, the [Airforce Institute of Technology](https://en.wikipedia.org//wiki/Air_Force_Institute_of_Technology), [Azusa Pacific University](https://en.wikipedia.org/wiki/Azusa_Pacific_University), [Long Island University](https://en.wikipedia.org//wiki/LIU_Post	), and the [University of Missouri–St. Louis](https://en.wikipedia.org//wiki/University_of_Missouri%E2%80%93St._Louis	) do not have information on the website on endowment/budget. Missing values are declared as NA in our data frame.


### c. Data munging


```{r start exercise 2c}

#####################################
# 2C
#####################################

```


#### The `ivyleagues` data frame

```{r read in ivyleague.csv}

# Read the CSV file into a data frame
ivyleagues <- read.csv("ivyleague.csv")
```

We have stored `ivyleague.csv` as a data frame called `ivyleagues`. Which has eight observations and the following four variables:

i. `uni_name`: a **shortened** version of each university's name
ii. `county`: the county in which the university's main campus is located 
iii. `state`: the state in which the university's main campus is located
iv. `ein`: the Employer Identification Number

As the primary key - `Institution` - in the relational database that we will create later is the full name of the university, we created a new variable in the `ivyleagues` data frame that gives the full name instead of the shortened version for each Ivy League. We store this variable under the column name `Institution`. This ensures that we can later properly gather and bind data from different tables within the relational database. 

```{r add the full names to the universities in ivyleagues}

# Add a column called Institution with the full name of the Ivy Leagues
ivyleagues <- ivyleagues %>%
  mutate(
    Institution = case_when(
      uni_name == "Penn" ~ "University of Pennsylvania",
      uni_name == "Brown" ~ "Brown University",
      uni_name == "Columbia" ~ "Columbia University",
      uni_name == "Cornell" ~ "Cornell University",
      uni_name == "Dartmouth" ~ "Dartmouth College",
      uni_name == "Harvard" ~ "Harvard University",
      uni_name == "Princeton" ~ "Princeton University",
      uni_name == "Yale" ~ "Yale University",
    )
  )
```

#### Ivy League Indicator, County and EIN

Now, we use the `iveleagues` data frame to create three new variables in the `universities_table` data frame, namely:

ix. `Ivy_League`: An binary variable indicating whether the university is an Ivy League institution
x. `County`: The university's county and state (only for Ivy Leagues, for all others, `County` is NA)
xi. `EIN`: The university's EIN (only for Ivy Leagues, for all others, `EIN` is NA)

```{r clean the ivyleagues table}

# Clean the ivyleagues table
ivyleagues <- ivyleagues %>%
  mutate(County = ifelse(!is.na(county), paste(county, state, sep = ", "), state))

# Add the three variables Ivy_League, County, EIN to universities_table
universities_table <- universities_table %>%
  left_join(ivyleagues %>% select(Institution, ein, County), by = c("Institution" = "Institution")) %>%
  mutate(Ivy_League = ifelse(Institution %in% ivyleagues$Institution, 1, 0)) %>%
  rename(EIN = ein)
```


### d. Writing to your relational database


```{r start exercise 2d}

#####################################
# 2D
#####################################

```


#### Check if the data is tidy

We check whether the data is tidy in two steps:

##### 1. Each row is a unique university

```{r check if each row holds one unique observation}

# Check if the rows in universities_table each hold one unique university 
if (length(unique(universities_table$Institution)) == length(universities_table$Institution)) {
  answer <- sprintf("There are %d unique universities in the dataframe 'universities_df' that are each represented by exactly one row. The data is tidy.", length(unique(universities_table$Institution)))
  print(answer)
} else {
  print("Not every row holds the data of a unique university. The data is not tidy.")
}
```

##### 2. Each column is a variable

```{r check if each variable is represented by one column}

# Check if the columns in universities_table each represent one of the 11 variables
if (ncol(universities_table) == 11) {
  column_names <- colnames(universities_table)
  column_answer <- sprintf("The dataframe has %d columns, which each represent one variable:", ncol(universities_table))
  formatted_names <- sprintf("Column %d: %s", 
                             seq_along(column_names), 
                             column_names)
  cat(column_answer, formatted_names, sep = "\n")
} else {
  print("The dataframe does not have 11 columns. It is not tidy.")
}
```

As we can see, the data is tidy. Each row represents one of the 279 unique universities and each column holds one of the 11 variables. The column `Institution` holds the unique name of each university and will be used as the primary key.

#### Write universities_table to the relational database

We write `universities_table` to the relational database `db` under the name `universities_df`. The primary key is the university name, which can be accessed via the `Institution` variable.

```{r write universities_table to the relational database}

# Write universities_table to the relational database
dbWriteTable(db, "universities_df", universities_table, overwrite = TRUE)
```

Now, we want to check the existence and dimensionality of `universities_df`. We do so by defining a function `check_table()` that returns the number of rows and columns and the column names of the table if it exists in our relational database.

```{r write a function that checks the existence and correct dimensionality of the table, echo=TRUE}

# Check_table is a function that checks the existence and dimensionality of a table in db
check_table <- function(db, a_table){
  
  # Check if the "a_table" exists
  if (a_table %in% dbListTables(db)) {
    
    # Get the row count
    query <- paste("SELECT COUNT(*) FROM", a_table)
    row_count <- dbGetQuery(db, query)[1, 1]
  
    # Get the column names
    column_names <- as.character(dbListFields(db, a_table))
  
    # Get the column count
    column_count <- length(unlist(column_names))
    
    # Print out dimensions and column names
    formatted_string <- sprintf("The table %s exists and has the following dimensions: \nNumber of rows: %s \nNumber of columns: %s", a_table, row_count, column_count)
    column_answer <- sprintf("Column names: %s", paste(column_names, collapse = ", "))
    
    return(cat(formatted_string, column_answer, sep = "\n"))
    
  } else {
    # If "a_table" does not exist, return this statement:
    return(cat("The table does not exist."))
  }
}

# Call check_table on "universities_df"
check_table(db, "universities_df")
```



## Exercise 3

```{r start exercise 3a}

#####################################
# EXERCISE 3
#####################################
# 3A
#####################################

```


### a. Scraping annual rank

#### Scrape the world ranking for Ivy League universities

We create a webscraping function `scrape_university_ranking_global()` that scrapes the world ranking for each Ivy League university from the [Academic Ranking of World Universities Website](https://www.shanghairanking.com/) for the years 2003, 2013, and 2023. It returns a data frame `ivyleagues_ranks` that has the following three variables:

i. `Institution`: The name of the Ivy League university
ii. `Year`: The year for which the ranking has been scraped (2003, 2013, or 2023)
iii. `Rank`: The ARWU ranking for the university for the specific year

The data frame is in tidy long format where each row represents a combination of university and year (e.g., Harvard-2013). Since there are eight Ivy Leagues and three years that we are interested in, the data frame `ivyleagues_ranks` should have 24 rows. The code for the webscraper can be seen in the Appendix.

```{r launch the browser, results='hide', message=FALSE, eval=FALSE}

# Launch the driver and browser
invisible(capture.output({
  rD <- rsDriver(browser=c("firefox"), port = free_port(random = TRUE), chromever = NULL) 
  driver <- rD$client
}))
```


```{r navigate to the website, eval=FALSE}

# Navigate to the website
url <- "https://www.shanghairanking.com/"
driver$navigate(url)

# Navigate to the 2023 ranking 
academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[2]/div[1]/button')
academic_ranking_23$clickElement()
```


```{r function that searches for the ivys}

# A function that searches for each ivy league university 
search_for <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[1]/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(1)
  search_field$sendKeysToElement(list(key = "enter"))
  
}
```


```{r create a function that navigates to a different year }

# A function that navigates to a different year
navigate_year <- function() {
  # Find the element using the provided XPath
  year_selector <- driver$findElement(using = "xpath", value =  '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[1]/img')
  
  # Click on the element
  year_selector$clickElement()
}
```


```{r functions to extract the world rank and the year}

# A function that extracts the rank from the website
extract_rank <- function() {
  # Find the element on the website and transform it to text directly
  current_university_rank <- driver$findElement(using = "xpath",
                                                value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/table/tbody/tr/td[1]/div')$getElementText()[[1]]
  
  return(current_university_rank)
}
```


```{r create a dataframe with ivy leagues}

# Create a data frame that holds all ivy league universities as displayed in the ivyleagues data
ivyleagues_df <- data.frame(
  Institution = ivyleagues$Institution,
  # Add an empty column for the year 
  Year = numeric(length(ivyleagues$Institution)),
  # Add an empty column for the Rank
  Rank = numeric(length(ivyleagues$Institution)),
  stringsAsFactors = FALSE
)

# Create three copies of the ivyleagues_df data frame for each of the three years that we will later scrape ranking data from
ivyleagues_03 <- ivyleagues_df
ivyleagues_13 <- ivyleagues_df
ivyleagues_23 <- ivyleagues_df
```


```{r extract the world rank of each ivy league}

# Scrape the rank for each ivy league university from the website for the years 2003, 2013, 2023

scrape_university_ranking_global <- function() {
  ##########
  ## 2023 ##
  ##########

  # Navigate to 2023
  navigate_year()

  academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[1]')
  academic_ranking_23$clickElement()

  # Loop through all ivy league universities in the ivyleagues data set and extract the ranking for each university
  for (i in seq_along(ivyleagues_23$Institution)) {
    search_for(ivyleagues_23$Institution[i])
    Sys.sleep(2)  
    ivyleagues_23$Rank[i] <- extract_rank()
    ivyleagues_23$Year[i] <- "2023"
  }

  ##########
  ## 2013 ##
  ##########

  # Navigate to 2013
  navigate_year()

  academic_ranking_13 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[11]')
  academic_ranking_13$clickElement()

  # Loop through all ivy league universities in the ivyleagues data set and extract the ranking for each university
  for (i in seq_along(ivyleagues_13$Institution)) {
    search_for(ivyleagues_13$Institution[i])
    Sys.sleep(2)  
    ivyleagues_13$Rank[i] <- extract_rank()
    ivyleagues_13$Year[i] <- "2013"
  }

  ##########
  ## 2003 ##
  ##########

  # Navigate to 2003
  navigate_year()

  academic_ranking_03 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[21]')
  academic_ranking_03$clickElement()

  # Loop through all ivy league universities in the ivyleagues data set and extract the ranking for each university
  for (i in seq_along(ivyleagues_03$Institution)) {
    search_for(ivyleagues_03$Institution[i])
    Sys.sleep(2)  
    ivyleagues_03$Rank[i] <- extract_rank()
    ivyleagues_03$Year[i] <- "2003"
  }
  
  # Creating a list of data frames
  scraped_ivyleagues_df <- rbind(ivyleagues_03, ivyleagues_13, ivyleagues_23)

  return(scraped_ivyleagues_df)

}
```


```{r call the scrape_university_ranking_global() webscraper, eval = FALSE}

# Call the global rank scraping function
scraped_ivyleagues_df <- scrape_university_ranking_global()

# Save it as a global variable
assign("scraped_ivyleagues_df", scraped_ivyleagues_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_ivyleagues_df, file = "scraped_ivyleagues_df.RData")
```


```{r create a copy of scraped_ivyleagues_df}

# Load global variables
load("scraped_ivyleagues_df.RData")

# Copy the scraped table to avoid modifying the original data
ivyleagues_ranks <- scraped_ivyleagues_df
```


**Note**: For some Ivy League universities, the ranking is reported as a range instead of a single number. For these observations, we have calculated the midpoint of the reported range and used that as the value for our `Rank` column.

```{r calculate the midpoint for global rank}

# Function to calculate midpoint from a range
calculate_midpoint <- function(rank) {
  for (i in seq_along(rank)) {
  range_midpoint <- mean(as.numeric(strsplit(rank[i], "-")[[1]])) 
  return(range_midpoint)}
}

# Calculate the midpoint
ivyleagues_ranks$Rank <- sapply(ivyleagues_ranks$Rank, calculate_midpoint)
```


#### Write it to the database

Under the name `ivyleagues_ranks_df`, we write our table `ivyleagues_ranks` to the relational database `db` and check for its existence and correct dimensionality using the function `check_table()` from Exercise 2d.


```{r write the ivyleagues_ranks table to the relational database}

# Adding the table to the relational database
dbWriteTable(db, "ivyleagues_ranks_df", ivyleagues_ranks, overwrite = TRUE)
```


```{r check the existence of the ivyleagues_ranks table, echo=TRUE}

# Check the existence of the ivyleagues_ranks table
check_table(db, "ivyleagues_ranks_df")
```
The table `ivyleagues_ranks_df` has successfully been written to the relational database and has the dimensions and column names that we expected. Again, the primary key is the `Institution` variable.


```{r navigate to the home page, eval=FALSE}

# Navigate to the home page to make it easier to keep working on this website
home_page <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[1]/a')

home_page$clickElement()
```


### b. Scraping subject ranks for 2023

```{r start exercise 3b}

#####################################
# 3B
#####################################

```



#### Scrape the social science rankings for Ivy League universities for 2023

We create another webscraping function `extract_social_science_rank()` that extracts the ranking for every social science for which the university has been ranked for the year 2023. Again, we are only interested in the eight Ivy League universities. The webscraper returns a data frame called `social_science_ranks` that has three variables:

i. `Institution`: The name of the Ivy League university
ii. `Subject`: The social science for which we extract the ranking
iii. `Number`: The ranking for the university for the specific subject

The data frame is in tidy long format where each row represents a combination of university and discipline (e.g., Harvard-Economics). Before scraping, we do not know for how many social science subjects each Ivy League university has been ranked. Therefore, we do not know how many rows to expect in the data frame before extracting the rankings. The code for the webscraper can be seen in the Appendix.


```{r function that navigates to the subject rankings}

# A function that navigates to the page with the subject rankings
navigate_universities <- function() {
  universities_ranks <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[3]')
  universities_ranks$clickElement()
}
```

```{r navigate to the page with the subject rankings, eval=FALSE}

navigate_universities()
```


```{r function that searches for the ivys in the general ranking and selects it}

# A function that searches for each ivy league university 
search_general_ranking <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[1]/div/div/div/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(2)
  search_field$sendKeysToElement(list(key = "enter"))
  
  # Select the first element
  Sys.sleep(2)
  first_result <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[1]/div[2]') 
  first_result$clickElement()

}
```


```{r filter for social sciences}

# A function that filters for the social sciences in the ranking options (with js)
filter_social_sciences <- function() {
  
  # Click the filter field
  filter_subject <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div/div[2]/div[2]/div[2]/div[1]/div[1]/div[2]/div/div[1]/img')
  filter_subject$clickElement()

  
  partial_inner_html <- "Social Sciences"

  # Construct an XPath that searches for an element containing the specified inner HTML
  xpath_expression <- sprintf('//*/text()[contains(., "%s")]/parent::*', partial_inner_html)

  # Find the element using the constructed XPath
  social_sciences_element <- driver$findElement(using = "xpath", value = xpath_expression)

  # Click on the found element
  social_sciences_element$clickElement()
  
}
```


```{r extract the social science ranks}

# Create a df that will later hold the rankings for the social sciences
social_science_ranks <- data.frame(Institution = character(),
                          Subject = character(),
                          Number = character(),
                          stringsAsFactors = FALSE)

# A function that extracts the social science rankings and appends them to the data frame "social_science_ranks"
extract_social_science_rank <- function(institution_name, existing_table) {
  
  # entire table with ranks
  rank_table <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div/div[2]/div[2]/div[2]/div[1]/div[2]/table/tbody')$getElementText()[[1]]
  
  # Split the data into subjects and numbers
  split_data <- unlist(strsplit(rank_table, "\n"))

  # Add the scraped data to the existing table
  existing_table <- rbind(existing_table,
                          data.frame(Institution = rep(institution_name, length(split_data) / 2),
                                     Subject = split_data[seq(1, length(split_data), by = 2)],
                                     Number = split_data[seq(2, length(split_data), by = 2)],
                                     stringsAsFactors = FALSE))
  
  return(existing_table)
}

```


```{r scrape social science rankings, eval=FALSE}

# Scrape social science rankings

navigate_universities()
Sys.sleep(2)  

# University of Pennsylvania
search_general_ranking("University of Pennsylvania")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("University of Pennsylvania", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Brown University
search_general_ranking("Brown University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Brown University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Columbia University
search_general_ranking("Columbia University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Columbia University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Cornell University
search_general_ranking("Cornell University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Cornell University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Dartmouth College
search_general_ranking("Dartmouth College")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Dartmouth College", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)


# Harvard University
search_general_ranking("Harvard University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Harvard University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)


#	Princeton University
search_general_ranking("Princeton University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Princeton University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)


#	Yale University
search_general_ranking("Yale University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Yale University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)

# Save it as a global variable
assign("social_science_ranks", social_science_ranks, envir = .GlobalEnv)

# Save the global variable to an RData file
save(social_science_ranks, file = "social_science_ranks.RData")
```


```{r create a copy of social_science_ranks}

# Load global variables
load("social_science_ranks.RData")

# Copy the scraped table to avoid modifying the original data
social_science_ranks_df <- social_science_ranks
```

**Note**: As in Exercise 3a, some rankings were reported as ranges. Again, we used the midpoint for these instances to report the ranking in our data frame.

```{r calculate the midpoint for the ranks in social_science_ranks_df}

# Calculate the midpoint for the social science rankings that were displayed as a range
social_science_ranks_df$Number <- sapply(social_science_ranks_df$Number, calculate_midpoint)
```


#### Write the table to the database

Under the name `social_science_ranks_df`, we write our table `social_science_ranks` to the relational database `db` and check for its existence and correct dimensionality using the function `check_table()` from Exercise 2d.

```{r write the social_science_ranks_df table to the relational database}

# Adding the table to the relational database
dbWriteTable(db, "social_science_ranks_df", social_science_ranks_df, overwrite = TRUE)
```


```{r check the existence of the social_science_ranks_df table, echo=TRUE}

# Check the existence of the social_science_ranks table
check_table(db, "social_science_ranks_df")
```
The table `social_science_ranks_df` has successfully been written to the relational database and has the dimensions and column names that we expected. Again, the primary key is the `Institution` variable.

```{r close the RSelenium process, eval=FALSE}

# Close the RSelenium processes:
driver$close()
# Close the associated Java processes
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
```





## Exercise 4

```{r start exercise 4a}

#####################################
# EXERCISE 4
#####################################
# 4A
#####################################

```


### a. Gathering financial data from a raw API

#### Extract total revenue and total assets for Ivy League universities

We use `hhtr` to gather financial data on the Ivy League universities from the [ProPublica API](https://projects.propublica.org/nonprofits/api). More precisely, we create a function `get_finance_data()` that takes the `EIN` of the Ivy Leagues as input factors, retrieves the name of the university and queries the API for the data corresponding to said university. Then, we extract the variables of interest and store them in a data frame `finance_df`. It has the following five columns:

i. `Institution`: The name of the Ivy League university (primary key)
ii. `ein`: The Employer Identificaton Number as extracted from the `ivyleagues.csv` file
iii. `year`: The year for which the financial information has been extracted from the API
iv. `total_revenue`: The total revenue of the university in the specified year, in USD
v. `total_assets`: The total assets of the university in the specified year, in USD

The data frame is in tidy long format where each row represents a combination of university and year (e.g., Harvard-2013). As we are interested in data for the years 2011-2021 (inclusive) for eight Ivy League universities, we should expect `finance_df` to have 88 rows.^[Note that in the original assignment, Exercise 4a asks for the years 2010 - 2020. However, the ProPublica API has been updated and now contains data until including 2021. Thereby, we just scraped the 11 most recent years, which are 2011-2021.]


```{r create an empty dataframe finance_df}

# Create an empty dataframe for the financial information
finance_df <- data.frame(
  Institution = NA,
  # Add a column for the EIN
  ein = NA,
  # Add an empty column for the year
  year = NA,
  # Add an empty column for the total revenue
  total_revenue = NA,
  # Add an empty column for the total assets
  total_assets = NA,
  stringsAsFactors = FALSE
)
```


```{r get the university name given the ein}

# Function that gets the university name given the EIN
get_university_name <- function(ein) {
  # Look for the EIN in the dataframe  
  matching_row <- ivyleagues[ivyleagues$ein == ein, ]
  # Check if a match is found
  if (nrow(matching_row) > 0) {
    # Return the corresponding university name
    return(matching_row$Institution)
  } else {
    # Return a message indicating no match found
    return("University not found")
  }
}
```


```{r get the data from the API given the ein}

# Function that queries the API for each university's data given their EIN
get_data <- function(ein) {
  # Create a URL given the EIN
  url <- paste0("https://projects.propublica.org/nonprofits/api/v2/organizations/", ein, ".json")
  response <- GET(url)
  api_data <- content(response, "parsed")
  
  return(api_data)
}
```


```{r get the finance data using the previously defined functions}

# function that gets the finance data given the ein and using the functions get_university_name and get_data
get_finance_data <- function(ein) {
  # Get the university name given the ein
  name <- get_university_name(ein)
  
  # Get the data given the ein
  dt <- get_data(ein)
  
  # Loop to append values
  for (i in seq(length(dt$filings_with_data))) {
    # Append the values to the dataframe
    finance_df <- rbind(finance_df, data.frame(Institution = name,
                                               ein = ein,
                                               year = dt$filings_with_data[[i]]$tax_prd_yr,
                                               total_assets = dt$filings_with_data[[i]]$totassetsend,
                                               total_revenue = dt$filings_with_data[[i]]$totrevenue))
  }

  # Return the resulting dataframe
  return(finance_df)
}
```


```{r call the get_finance_data() function, eval=FALSE}

# Call the "get_finance_data()" function
result_list <- lapply(ivyleagues$ein, get_finance_data)
scraped_finance_df <- do.call(rbind, result_list)

# Save it as a global variable
assign("scraped_finance_df", scraped_finance_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_finance_df, file = "scraped_finance_df.RData")
```


```{r load scraped_finance_df.RData}

# Load global variables
load("scraped_finance_df.RData")

# Copy the scraped table to avoid modifying the original data
finance_df <- scraped_finance_df
```


```{r format the finance_df}

# Delete rows that only consist of missing values
finance_df <- finance_df %>%
  na.omit() 

# Fill NA values for missing years
complete_df <- expand.grid(year = 2011:2021, 
                           ein = unique(finance_df$ein))

# Add the "Institution" column to complete_df
complete_df <- complete_df %>%
  left_join(unique(finance_df[, c("ein", "Institution")]), by = "ein") %>%
  arrange(ein, year)

# Merge with the original dataframe
finance_df <- complete_df %>%
  left_join(finance_df, by = c("year", "ein")) %>%
  arrange(ein, year)

finance_df <- finance_df %>%
  select(-Institution.y, Institution = Institution.x)
```

**Note**: The final data frame has missing values. For instance, the data for `total_revenue` and `total_assets` of Harvard University is unavailable for the year 2020. The observations with missing values for `total_revenue` and `total_assets` were simply not extracted from the API and thus did not appear in the data frame `finance_df`. Therefore, we computed these rows and added NA's for their financial information for the sake of completeness.


#### Write it to the database

Under the name `finance_df`, we write our table `finance_df` to the relational database `db` and check for its existence and correct dimensionality using the function `check_table()` from Exercise 2d.

```{r write the finance_df table to the relational database}

# Adding the table to the relational database
dbWriteTable(db, "finance_df", finance_df, overwrite = TRUE)
```


```{r check the existence of the finance_df table, echo=TRUE}

# Check the existence of the ivyleagues_ranks table
check_table(db, "finance_df")
```
The table `finance_df` has successfully been written to the relational database and has the dimensions and column names that we expected.


### b. Gathering local economic data from a packaged API 

```{r start exercise 4b}

#####################################
# 4B
#####################################

```


```{r set up the api key, message=FALSE}

readRenviron("../../Documents/R_Environs/api_census.env")
apikey <- Sys.getenv("KEY")
census_api_key(apikey)
```

#### Extract the median estimated household income of all counties in the US, 2015 and 2020

We use the `tidycensus` package to access the API of the US Census Bureau. We write a function that retrieves the names of all the Counties in the US and their estimated median household income for every county for both 2015 and 2020 (based on the American Community Survey (ACS)). 

To a table `census_df`, we add the data retrieved from the US Census Bureau API for each Ivy League university. More precisely, `census_df` has the following four columns:

i. `Institution`: The name of the Ivy League university (primary key)
ii. `County`: The county in which the university’s main campus is located
iii. `Year`: The year for which the data has been retrieved (either 2015 or 2020)
iv. `Median_Household_Income`: The estimated median household income for the county, in which the Ivy League is located, in USD

The data is in tidy long format where each row represents a combination of university and year (e.g., Harvard-2015). As we are interested in two years, 2015 and 2020, for eight Ivy League universities, we should expect `census_df` to have 16 rows.

```{r retrieve median estimated household income, eval=FALSE}

# Median household income 2015
median_household_income_15 <- get_acs(geography = "county", year = 2015, variables = "B19013_001")
median_household_income_20 <- get_acs(geography = "county", year = 2020, variables = "B19013_001")

# Save it as a global variable
assign("median_household_income_15", median_household_income_15, envir = .GlobalEnv)
assign("median_household_income_20", median_household_income_20, envir = .GlobalEnv)

# Save the global variable to an RData file
save(median_household_income_15, file = "median_household_income_15.RData")
save(median_household_income_20, file = "median_household_income_20.RData")
```


```{r load median_household_income_15.RData and median_household_income_20.RData}

# Load global variables
load("median_household_income_15.RData")
load("median_household_income_20.RData")

# Copy the scraped table to avoid modifying the original data
income_15_df <- median_household_income_15
income_20_df <- median_household_income_20
```


```{r create a dataframe census_df}

# Create an empty dataframe
census_df <- data.frame(
  Institution = ivyleagues$Institution,
  County = ivyleagues$County,
  stringsAsFactors = FALSE
)

# Add the information on the median household income from the census API for 2015 and 2020
census_df <- census_df %>%
  
  # For 2015
  left_join(select(income_15_df, NAME, estimate), by = c("County" = "NAME")) %>%
  mutate("2015" = estimate) %>%
  select(-estimate)  %>%
  
  # For 2020
  left_join(select(income_20_df, NAME, estimate), by = c("County" = "NAME")) %>%
  mutate("2020" = estimate) %>%
  select(-estimate)

# Pivot longer
census_df <- census_df %>%
  pivot_longer(cols = c("2015", "2020"),
               names_to = "Year",
               values_to = "Median_Household_Income")

# Set the year column as numeric
census_df$Year <- as.numeric(census_df$Year)
``` 


#### Write it to the database

Under the name `census_df`, we write our table `census_df` to the relational database `db` and check for its existence and correct dimensionality.

```{r write the census_df table to the relational database}

# Write the table to the relational database
dbWriteTable(db, "census_df", census_df, overwrite = TRUE)
```


```{r check the existence of the census_df table, echo=TRUE}

# Check the existence of the census_df table
check_table(db, "census_df")
```
The table `census_df` has successfully been written to the relational database and has the dimensions and column names that we expected.


## Exercise 5

```{r start exercise 5a}

#####################################
# EXERCISE 5
#####################################
# 5A
#####################################

```


### a. Analysis and visualisation

#### Analysis table

Based on the data on Ivy League universities written to our relational database, we create a data frame `analysis_df`, which has the following columns:

i. `Institution`: The name of the Ivy League University
ii. `Average_Rank`: The average rank of the university across 2003, 2013, and 2023
iii. `Avg_Econ_PolSc_Soc`: The average rank of the university's Economics, Political Science, and Sociology programmes (if they were ranked)
iv. `Endowment_Per_Student`: The current endowment per student (total endowment divided by total number of students), in USD
v. `Rev_Per_Student_2015_2020`: The average total revenue per student across the years 2015 - 2020, in USD
vi. `Avg_Household_Income_2015_2020`: The average of the median household income for the County across the years 2015 and 2020, in USD

The SQL queries for the extraction and calculation of the variables can be found in the Appendix.

```{r sql queries for the analysis table}

# Second column: The average rank of the university across 2003, 2013, and 2023
second_query <- "
  SELECT Institution, AVG(Rank) AS Average_Rank
  FROM ivyleagues_ranks_df
  WHERE Year IN (2003, 2013, 2023)
  GROUP BY Institution;
"

# Execute the query
second_result <- dbGetQuery(db, second_query)


# Third column: The average rank of the university's Economics, Political Science, and Sociology programs, if they were ranked
third_query <- "
  SELECT Institution, AVG(Number) AS Avg_Econ_PolSc_Soc
  FROM social_science_ranks_df
  WHERE Subject IN ('Economics', 'Political Sciences', 'Sociology')
  GROUP BY Institution;
"

# Execute the query
third_result <- dbGetQuery(db, third_query)


# Fourth column: The current endowment per student (total endowment divided by total number of students), in USD
fourth_query <- "
  SELECT Institution, Endowment / Students AS Endowment_Per_Student
  FROM universities_df
  WHERE Ivy_League = 1;
"

# Execute the query
fourth_result <- dbGetQuery(db, fourth_query)


# Fifth column: The average total revenue per student across the years 2015 - 2020, in USD
fifth_query_students <- "
  SELECT Institution, Students
  FROM universities_df
  WHERE Ivy_League = 1;
"

fifth_query_revenue <- "
  SELECT Institution, AVG(total_revenue) AS Avg_Revenue_2015_2020
  FROM finance_df
  WHERE year BETWEEN 2015 AND 2020
  GROUP BY Institution;
"

# Execute the queries
fifth_result_students<- dbGetQuery(db, fifth_query_students)
fifth_result_revenue <- dbGetQuery(db, fifth_query_revenue)

fifth_result <- merge(fifth_result_students, fifth_result_revenue,
                      by = "Institution", all = TRUE) %>%
  mutate(Rev_Per_Student_2015_2020 = Avg_Revenue_2015_2020 / Students) %>%
  select(Institution, Rev_Per_Student_2015_2020)


# Sixth column: The average of the median household income for the County across the years 2015 and 2020, in USD
sixth_query <- "
  SELECT Institution, AVG(Median_Household_Income) AS Avg_Household_Income_2015_2020
  FROM census_df
  WHERE year IN (2015, 2020)
  GROUP BY Institution;
"

# Execute the queries
sixth_result <- dbGetQuery(db, sixth_query)
```


```{r combine the tables resulting from the six queries}

# Merge the five dataframes retrieved through the sql queries by Institution
list_of_dataframes <- list(second_result, third_result, fourth_result, fifth_result, sixth_result)
analysis_df <- reduce(list_of_dataframes, merge, by = "Institution", all = TRUE)
```



#### Visualisations with `ggplot`

We create four plots that each show the relationship between two variables from the `analysis_df`.

##### 1. Average university ranking vs. average Econ/PS/Soc ranking

```{r first plot average university ranking and average Econ/PS/Soc ranking, message = FALSE}

# First ggplot: average university ranking and average Econ/PS/Soc ranking
first_plot <- ggplot(analysis_df, aes(x = Average_Rank, y = Avg_Econ_PolSc_Soc, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  # Add labels
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  # Add titles
  labs(title = "Average Overall Ranking vs. Subject Ranking for Ivy League Universities",
       x = "Average Overall Ranking (2003, 2013, 2023)",
       y = "Average Ranking for Economics, Political Sciences, and Sociology",
       color = "Ivy League University") +
  theme_minimal() +
  # Adjust the font size for the title and the axes
  theme(
    axis.text = element_text(size = 10),     
    axis.title = element_text(size = 10),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(1, 750), expand = c(0.02, 0)) +  # Adjust the limits and expand
  scale_y_log10(limits = c(1, 200), expand = c(0.02, 0))

# Display the first plot
first_plot
```

##### 1A: The plot

**a. Logarithmic scale**: Note that we have used a logarithmic scale for the x and y axes. The reason for this is that the data is mostly concentrated in a small range of values, yet has some significant outliers (particularly Dartmouth College). When dealing with skewed data or data with wide data ranges, it can be helpful to use logarithmic scales for the axes. A linear scale may compress the data at one end and stretch it at the other, making it difficult to visualise patterns. A logarithmic scale ensures that data points across the entire range are more evenly distributed. If the data is skewed, it can help in visualising details in the tail of the distribution that might be compressed in a linear scale.

**b. Regression line**: Note that we have added a yellow line representing the linear regression $lm(y \sim x)$  with

- $y$ = Average Ranking for Economics, Political Sciences, and Sociology, and 
- $x$ = Average University Ranking (2003, 2013, 2023).

Although we are not able to show causality with a simple linear regression, it can help to include the regression line to visualise the correlation of the x and y variables. However, since we only have a very small data set with merely eight observations, we have added the confidence interval as a grey shaded area, which shows the range of uncertainty associated with the estimated regression parameters. It provides a range within which we can be reasonably confident that the true regression line lies. Hence, if the grey area is relatively wide, the location of the regression line should be put into question. Note that with smaller data sets, confidence intervals tend to be wider, indicating a higher level of uncertainty.

**c. R squared**: We have also included the R-squared of the plot. However, it must be taken into consideration that with small data sets, the R-squared value can be highly sensitive towards singular values. More so, when removing or changing one value, the impact on the R-squared value can be quite large. Hence, the value of the R-squared should be interpreted with caution. 

```{r R squared for the first plot}

# Fit a linear regression model
model1 <- lm(Avg_Econ_PolSc_Soc ~ Average_Rank, data = analysis_df)

# View the R-squared value
rsquared_value_1 <- summary(model1)$r.squared
cat("R-squared Plot 1:", rsquared_value_1, "\n")
```

##### 1B: The findings 

There seems to be a positive correlation between the overall ranking and the average ranking for the subjects Economics, Political Sciences, and Sociology, as seen by the positive slope of the regression line. This is not surprising. A university that is ranked highly overall is expected to have high rankings in most subjects as well. It should be noted, though, that the overall ranking displayed on the x-axis is averaged over the years 2003, 2013, and 2023, whilst the average subject ranking displayed on the y-axis only refers to the most recent rankings in the subjects Economics, Political Sciences, and Sociology. Thereby, the possibility that the overall ranking changed significantly since 2003 and thus influenced the overall ranking (either positively or negatively) must thereby be considered. It would probably be more informative to retrieve the subject rankings for the three years of interest and then calculate the average. Furthermore, the analysis has some limitations such as the small size of the data set, which is especially important when interpreting R-squared. In this case, the R-squared is around 0.81 and thereby quite high. However, the small sample size prevents us from deducting causality or even generalisable correlations from the R-squared. 


##### 2. Average university ranking vs. endowment per student

```{r second plot average university ranking and endowment per student, message = FALSE}

# Second ggplot: average university ranking and endowment per student
second_plot <- ggplot(analysis_df, aes(x = Average_Rank, y = Endowment_Per_Student/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  # Add labels
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  # Add titles
  labs(title = "Average Overall Ranking vs. Endowment Per Student",
       x = "Average University Ranking (2003, 2013, 2023)",
       y = "Endowment per Student, in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  # Adjust the font size for the title and the axes
  theme(
    axis.text = element_text(size = 10),     
    axis.title = element_text(size = 10),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(1, 750), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(100, 7500), expand = c(0.02, 0))

# Display the second plot
second_plot
```

##### 2A: The plot

**a. Logarithmic scale**: Again, we have used logarithmic scales for the x and y axes due to the reasons described in the first plot.

**b. Regression line**: Again, we have added a yellow line representing the linear regression $lm(y \sim x)$ with

- $y$ = Endowment per Student, in thousand USD, and
- $x$ = Average University Ranking (2003, 2013, 2023).

**c. R squared**:

```{r R squared for the second plot}

# Fit a linear regression model
model2 <- lm(Endowment_Per_Student ~ Average_Rank, data = analysis_df)

# View the R-squared value
rsquared_value_2 <- summary(model2)$r.squared
cat("R-squared Plot 2:", rsquared_value_2, "\n")
```

##### 2B: The findings 

The plot shows a slightly negative correlation between the average university ranking and the endowment per student. The findings are somewhat surprising. One might expect universities with high rankings to have larger endowments per students as they firstly might be able to charge higher student fees due to their good reputation and secondly might be ranked so highly due to the availability of large resources that they can invest in the facilities and the quality of teaching. However, this expectation is flawed as it does not consider other factors that influence the relationship between the endowment per student and the overall ranking: 

a. Resource Allocation: A high endowment doesn't necessarily guarantee effective resource allocation that directly influences rankings. Some universities might have large endowments but allocate resources differently, prioritising spending on specific aspects such as faculty, research, infrastructure, or student programmes. This might impact their rankings differently.

b. Ranking Criteria: University rankings are based on various criteria, including academic reputation, faculty-to-student ratio, research output, and more. Endowment per student is just one financial aspect and might not capture the broader factors influencing rankings.

c. Other factors: Many other factors might explain the size of the endowment in relationship to the university ranking. For instance, high endowments might stem from historical funding and donations, Alumni support, or financial aid and tuition fees. Possibly, these factors do not significantly or even negatively correlate with the university ranking. The analysis of the correlation between ranking and endowment per student necessitates a more complex analysis of the factors explaining the composition of rankings and endowments.

Note, however, that the slope of the regression is relatively small, the confidence interval is quite wide, and the R-squared is nearly zero. Without expanding the analysis (e.g., by varying the sample size, retrieving the adjusted R-squared, or extending the simple to a multiple regression), the meaningfullness of the correlation can strongly be questioned.

##### 3. Average endowment per student vs average median household income

```{r third plot average endowment per student and average median household income, message=FALSE}

# Third ggplot: average endowment per student vs average median household income
third_plot <- ggplot(analysis_df, aes(y = Avg_Household_Income_2015_2020/1000, x = Endowment_Per_Student/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  # Add labels
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  # Add titles
  labs(title = "Endowment Per Student vs. Average Median Household Income",
       x = "Endowment per Student, in thousand USD",
       y = "Average Median Household Income for the County (2015 and 2020), in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  # Adjust the font size for the title and the axes
  theme(
    axis.text = element_text(size = 8),     
    axis.title = element_text(size = 8),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(300, 6500), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(35, 150), expand = c(0.02, 0))

# Display the third plot
third_plot
```


##### 3A: The plot

**a. Logarithmic scale**: Again, we have used logarithmic scales for the x and y axes due to the reasons described in the first plot.

**b. Regression line**: Again, we have added a yellow line representing the linear regression $lm(y \sim x)$ with

- $y$ = Average Median Household Income for the County (2015 and 2020), in thousand USD, and
- $x$ = Endowment per Student, in thousand USD.

**c. R squared**:

```{r R squared for the third plot}

# Fit a linear regression model
model3 <- lm(Avg_Household_Income_2015_2020 ~ Endowment_Per_Student, data = analysis_df)

# View the R-squared value
rsquared_value_3 <- summary(model3)$r.squared
cat("R-squared Plot 3:", rsquared_value_3, "\n")
```

##### 3B: The findings 

There seems to be a slight positive correlation between the average median household income for the county and the endowment per student, which is not surprising. A rich county might facilitate an attractive environment for wealthy institutions of higher educations or even positively contribute to a university's financial means. Potential explanations for that could be:

a. Wealthy Donors and Alumni: Universities located in affluent areas may have a higher likelihood of attracting wealthy donors from the region and alumni who contribute to the endowment as they might prefer to financially support local institutions or their alma mater.

b. Local Economic Conditions: The economic prosperity of the local community could influence the financial health of the university. A thriving local economy may lead to increased donations and support for the university, contributing to a higher endowment.

c. Tuition and Enrollment: Universities in wealthier areas may have higher tuition fees, contributing to a larger revenue stream that can be directed to the endowment. A university's ability to attract students willing to pay higher tuition may also be influenced by the economic demographics of the area.

The other way around, it might also be reasonable to expect universities with higher endowments to positively contribute to the regional economy to a larger extent than universities with smaller endowments. For instance, wealthy universities might positively affect the local real estate development, contribute to job creation in the region, facilitate research and innovation as well as entrepreneurship and create higher visitor and student spendings.

Again, the R-squared of our plotted regression line is quite small and there seems to be a lot of volatility and uncertainty in the relationship of the two variables as shown by the wide confidence interval. Hence, it is not determinable, which variable will affect the other or whether the positive relationship is random or not.

##### 4. Average revenue per student vs average median household income

```{r fourth plot average revenue per student and average median household income, message=FALSE}

# Fourth ggplot: average revenue per student vs. average median household income
fourth_plot <- ggplot(analysis_df, aes(y = Avg_Household_Income_2015_2020/1000, x = Rev_Per_Student_2015_2020/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  # Add labels
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  # Add titles
  labs(title = "Average Revenue Per Student vs. Average Median Household Income",
       x = "Average Revenue Per Student (2015 - 2020), in thousand USD",
       y = "Average Median Household Income for the County (2015 and 2020), in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  # Adjust the font size for the title and the axes
  theme(
    axis.text = element_text(size = 8),     
    axis.title = element_text(size = 8),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(110, 600), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(25, 150), expand = c(0.02, 0))

# Display the fourth plot
fourth_plot
```

##### 4A: The plot

**a. Logarithmic scale**: Again, we have used logarithmic scales for the x and y axes due to the reasons described in the first plot.

**b. Regression line**: Again, we have added a yellow line representing the linear regression $lm(y \sim x)$ with

- $y$ = Average Median Household Income for the County (2015 and 2020), in thousand USD, and
- $x$ = Average Revenue Per Student (2015 - 2020), in thousand USD.

**c. R squared**:

```{r R squared for the fourth plot}

# Fit a linear regression model
model4 <- lm(Avg_Household_Income_2015_2020 ~ Rev_Per_Student_2015_2020, data = analysis_df)

# View the R-squared value
rsquared_value_4 <- summary(model4)$r.squared
cat("R-squared Plot 4:", rsquared_value_4, "\n")
```


##### 4B: The findings

Again, we find a slight positive correlation between the average revenue per student and the average median household income of the county that the university is located in. Revenue and endowment of a university are likely to be positively correlated with one another as universities with higher average revenues may have larger endowments and investment portfolios. Since we found a positive correlation between the average endowment per student and the average median household income in the previous plot, these findings are not surprising. The explanations for the positive relationship could be similar to the ones previously elaborated on: 

a. Research Funding and Grants: Universities in affluent regions may have greater access to research funding and grants, leading to additional revenue.

b. Economic Stability: The overall economic stability of the region can impact the ability of families to invest in education. In economically stable areas, families may be more willing to allocate funds for higher education.

c. Tuition and Fees: Universities in wealthier areas may be able to set higher tuition and fees, contributing to higher revenue per student. Affluent students in these regions may be more willing and able to pay higher tuition costs.

As in the previous plots, we cannot rule out that these observations are random, neither are generalised conclusions possible due to a low R squared, a small data set and a large confidence interval.



### b. Visualisation of geographic data


```{r start exercise 5b}

#####################################
# 5B
#####################################

```

Based on the data on all R1 and R2 universities written to our relational database, we create a data frame `analysis_table_geography`, which has the following columns:

i. `Institution`: The name of the university
ii. `Geographic_Location`: The geographic coordinates of the (main) university campus
iii. `Control`: The university's status (public or private)
iv. `Ivy_League`: A binary variable indicating whether the university is an Ivy League institution

The SQL queries for the extraction and calculation of the variables can be found in the Appendix.

```{r sql queries for the geography table}

# Select columns: University name, geographic coordinates, status (private or public), Ivy League indicator
query_geography <- "
  SELECT Institution, Control, Geographic_Location, Ivy_League
  FROM universities_df;
"

# Execute the query
analysis_table_geography <- dbGetQuery(db, query_geography)

analysis_table_coordinates <- separate(analysis_table_geography, Geographic_Location, into = c("latitude", "longitude"), sep = " ", convert = TRUE)

# Extract numeric values from the coordinates
analysis_table_coordinates <- analysis_table_coordinates %>%
  mutate(
    latitude = as.numeric(str_extract(latitude, "([0-9.]+)")),
    longitude = as.numeric(str_extract(longitude, "([0-9.]+)"))
  ) %>%
  mutate(longitude = -longitude)

analysis_table_coordinates <- analysis_table_coordinates[complete.cases(analysis_table_coordinates$latitude, analysis_table_coordinates$longitude), ]

# Convert analysis_table geography into an sf object
geography_sf <- st_as_sf(analysis_table_coordinates, coords = c("longitude", "latitude"), crs = 4326)
```

Then, we use the `tmap` package to plot the universities on a map of the US. The map shows:

i. every R1 and R2 university, excluding the Ivy League institutions, as a point
ii. where the colour of the points varies by status:
    - Private (non-profit) universities: <span style="color: #FFC20A;">**orange**</span> word.
    - Public universities: <span style="color: #F781BF;">**pink**</span> word.
iii. Ivy League universities as larger <span style="color: #FF0000;">**red**</span> points 



```{r get the US states shapefile, message = FALSE}

invisible(capture.output({
  # Shapefile with US 
  us_map <- nation()
}))
```


```{r create the map plot, message = FALSE}

# Set the tmap mode
tmap_mode("view")

# Set the bounding box for the US
us_bbox <- c(-175, 17, -63, 73)

# Plot the regions
geographic_plot <- tm_shape(us_map) +
  tm_polygons("NAME", palette=c("United States"="#8DD08D"), alpha = 0.5, legend.show = FALSE) +
  
  # Dots for non-Ivy League universities
  tm_shape(geography_sf[geography_sf$Ivy_League == 0, ]) +
  tm_dots(size=0.05, 
          col="Control", palette=c("Public"='#F781BF', "Private (non-profit)"='#FFC20A'), 
          title="University Status") +
  
  # Dots for Ivy League universities
  tm_shape(geography_sf[geography_sf$Ivy_League == 1, ]) +
  tm_dots(size = 0.1, 
          col = "Ivy_League",
          palette=c("1"= "#FF0000", "0" = "grey"), 
          legend.show = TRUE,
          title = "Ivy League",
          labels = c("1" = "Ivy League", "0" = "Non-Ivy")) +
  
  tm_layout(title = "US R1 and R2 Universities") +
  
  # Set the view to the US bounding box
  tm_view(bbox = us_bbox)

geographic_plot

```



Is there any notable pattern to where the Ivy League universities concentrated? What about private and public universities? Do any parts of the United States appear particularly under-resourced in terms of research universities? How might you explain the patterns you observe?

#### Discussion of the Map

##### 1. Concentration of the Ivy Leagues on the East Coast

As seen in the map above, the Ivy League universities are all concentrated in the East Coast of the United States, particularly in New England and the Mid-Atlantic region.

a. Colonial History: Many Ivy League universities were established during the colonial period when the East Coast was a primary center of European settlement. For example, Harvard University, the oldest Ivy League institution, was founded in 1636 in Cambridge, Massachusetts (Source: [Wikipedia](https://en.wikipedia.org/wiki/Harvard_University)).

b. Cultural and Economic Centers: Cities along the East Coast, such as Boston, New York City, and Philadelphia, have historically been cultural and economic centers. The presence of these cities provided Ivy League institutions with access to resources, cultural activities, and influential networks.

c. Historical Development of Endowments and Philanthropy: The East Coast has been home to significant wealth and philanthropy throughout history (Source: [Wikipedia](https://en.wikipedia.org/wiki/Economic_history_of_the_United_States)). Potentially, wealthy individuals and families in the region have contributed to the establishment and growth of Ivy League universities through endowments and donations.

d. Network Effects: The East Coast's concentration of elite universities likely created a network effect, attracting students, faculty, and researchers to the region. The presence of multiple prestigious institutions might have inspired the establishments of other prestigious institutions of higher education in the area.

##### 2. Concentration of private universities in urban areas

Public and private universities are seemingly spread across the country. However, when displaying the map in the mode `OpenStreetMap` instead of the default mode `Esri.WorldGrayCanvas`, and zooming in, we can notice that most private universities are located in close proximity to larger cities, such as Los Angeles, Dallas, Chicago, Washington DC, New York City, and Boston.

Several factors might explain this phenomenon, such as:

a. Access to Resources: Larger cities might offer better access to resources such as libraries, research facilities, cultural institutions, and industry partnerships. Private universities, which often rely on tuition and private funding, may be attracted to urban areas where these resources are abundant.

b. Industry Connections: Proximity to major urban centers provides opportunities for private universities to establish strong connections with industries, businesses, and organisations.

c. Networking Opportunities: Being close to a city facilitates networking opportunities for students and alumni. Private universities often declare networking and alumni relations as their unique selling point.

d. International Attraction: Cities tend to attract a diverse population, including international students. Private universities, which may have a higher reliance on tuition revenue, could be attracted to urban areas for the potential to attract a more international student body.


##### 3. The Midwest as an under-researched area

It is noticeable, that the universities are mainly concentrated along the coastlines and in the East of the US or along the coastlines. The Midwest/West and Alaska are relatively under-researched. Several factors might explain that observation:

a. Historical Development: As previously mentioned, many of the most prestigious universities in the United States, such as Harvard, were established in the colonial period on the East Coast. Potentially, the presence of these early institutions might have influenced the establishment of additional universities in nearby areas.

b. Population Centers: The eastern and western coasts of the United States are home to some of the country's largest and most populous cities (Source: [Wikipedia](https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population)). Potentially, Universities might tend to develop in areas with large population centers.

c. Economic Opportunities: Coastal areas might to have larger economic opportunities, including access to industries, businesses, and financial centers, potentially inspiring the establishment of higher education institutions.

d. Transportation Infrastructure: The public transportation infrastructure is better developed in the Eastern part of the US compared to the Midwest (See: [Railway Map](https://www.arcgis.com/apps/mapviewer/index.html?webmap=96ec03e4fc8546bd8a864e39a2c3fc41)). Potentially, access to public transportation is a motivating factor for establishing higher education institutions at specific places.


## Data

The data scraped from [Wikipedia](https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States) and the [Academic Ranking of World Universities](https://www.shanghairanking.com/) as well as the data retrieved from the [ProPublica API](https://projects.propublica.org/nonprofits/api) and the [US Census Bureau API](https://walker-data.com/tidycensus/) were saved as global variables after scraping and stored as RData files. When running this code on a local machine, please refer to the [GitHub repository](https://github.com/emmi3105/472_assignment3) to retrieve the RData files. 


## Code Sources

[1] ggplot2 package: [RDocumentation](https://www.rdocumentation.org/packages/ggplot2/versions/3.4.4)

[2] httr package: [RDocumentation](https://www.rdocumentation.org/packages/httr/versions/1.4.7)

[3] RSelenium package: [RDocumentation](https://www.rdocumentation.org/packages/RSelenium/versions/1.7.9)

[4] tigris package: [RDocumentation](https://www.rdocumentation.org/packages/tigris/versions/2.0.4)

[5] tmap package: [cran.r-project](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html#multiple-shapes-and-layers)


## Appendix: All code in this assignment

```{r close the database connection}

# Close the database connection
dbDisconnect(db)
```


```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 


```
