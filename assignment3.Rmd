---
title: "Assignment 3"
author: "Student ID: 201903536"
date: "30 November 2023"
output: html_document
---

```{r setup, include=FALSE} 
#####################################
# SETUP
#####################################

knitr::opts_chunk$set(echo = FALSE) 

#####################################
# Install/load packages
#####################################

library(rvest)
library(tidyverse)
library(purrr)
library(DBI)
library(RSQLite)
library(RSelenium)
library(netstat)
library(httr)
library(jsonlite)
library(tidycensus)
library(tigris)
library(stringr)
library(tmap)
library(sf)
library(htmlwidgets)

```


## GitHub

The GitHub repository for **this assignment** can be found [here](https://github.com/emmi3105/472_assignment3).


## Exercise 1

```{r start exercise 1}

#####################################
# EXERCISE 1
#####################################

```


### Create a local relational database

```{r create database, echo=TRUE}

# Create database
db <- dbConnect(RSQLite::SQLite(), "universities_db.sqlite")
```


### Check for the existence of the database

```{r check existence of database, echo=TRUE}

# Check existence of the database
print(file.exists("universities_db.sqlite"))
```

As the output of the code chunk above is "TRUE", the database `universities_db.sqlite` exists.

**Note**: The primary key that will be used throughout this assignment is the variable "Institution", which refers to the name of the university.

## Exercise 2

### a. Gathering structured data 

```{r start exercise 2a and 2b}

#####################################
# EXERCISE 2
#####################################
# 2A and 2B
#####################################

```


**Note**: The solution to Exercise 2a can be found under the solution for Exercise 2b.

### b. Gathering unstructured data

#### Automatic webscraping function for 2a and b

I combined exercise 2a and 2b and directly wrote and executed a webscraping function called `scrape_wikipedia_tables()` that returns a data frame with the following eight variables: 

i. `Institution`: The university’s name, which will be used as the primary key
ii. `Control`: The university’s status (public or private)
iii. `City`: The city in which the university is located
iv. `State`: The state in which the university is located
v. `Wikipedia_URL`: The URL of the university’s dedicated Wikipedia page
vi. `Geographic_Location`: The geographic coordinates of the (main) university campus
vii. `Endowment`: The endowment of the university in USD dollars
viii. `Students`: The total number of students (including both undergraduate and postgraduate)

The resulting data frame is stored under the name `universities_table`.


```{r wikipedia web scraping function}

scrape_wikipedia_tables <- function(url) {
  
  ### Exercise 2a: scrape information on the name, status, city, state and the URL to the Wikipedia page
  table_indices <- c(19, 27)
  
  # Storing the URL's HTML code
  html_content <- read_html(url)
  
  # Extracting all tables in the document 
  tab <- html_table(html_content, fill = TRUE)

  # Extract only the tables of interest given the input table_indices
  selected_tables <- tab[1:length(table_indices)]

  # Iterate over tables and bind rows
  empty_tibble <- tibble()
  for (i in 1:length(selected_tables)) {
    current_table <- as_tibble(selected_tables[[i]][, 1:4])
    empty_tibble <- bind_rows(empty_tibble, current_table)
  }

  # Initialise an empty list to store hyperlinks
  hyperlinks_list <- list()

  # Iterate over the table indices and extract hyperlinks
  for (index in table_indices) {
    table_nodes <- html_content %>%
      html_nodes(paste("#mw-content-text > div.mw-content-ltr.mw-parser-output > table:nth-child(", index, ")", sep = ""))
  
    current_node_set <- table_nodes[[1]]
  
    hyperlinks_current_node <- current_node_set %>%
      html_nodes("td:nth-child(1) a") %>%
      html_attr("href")
  
    # Append the hyperlinks to the list
    hyperlinks_list <- c(hyperlinks_list, hyperlinks_current_node)
  }

  # Unlist to flatten the list
  hyperlinks_list <- unlist(hyperlinks_list, recursive = FALSE)

  # Concatenate the base URL to the extracted hyperlinks
  hyperlinks_list <- paste0("https://en.wikipedia.org/", hyperlinks_list)
  
  # Add a new column 'Wikipedia_URL' with the extracted URLs
  empty_tibble$Wikipedia_URL <- hyperlinks_list

    
  ### Exercise 2b: Add the geographic location, endowment and students
  
  
  #### Geographic location
  geographic_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the geographic location using the class selector 'geo_default'
    geographic_location <- university_page %>%
      html_nodes(".geo-dec") %>%
      html_text() %>%
      unique()
 
    # Store the scraped information in a list
    geographic_data <- append(geographic_data, list(geographic_location))
  }

  # Add a new column 'Geographic_Location' with the scraped data
  empty_tibble$Geographic_Location <- geographic_data

  # Keep the first set of coordinates only
  empty_tibble <- empty_tibble %>%
    mutate(Geographic_Location = map_chr(Geographic_Location, ~ .[1]))
  
  
  #### Endowment and budget
  endowment_data <- list()

  # Clean text by removing text within brackets
  clean_text <- function(text) {
    # Remove text containing brackets within brackets
    text <- gsub("\\([^()]*\\)", "", text)
    # Remove text within square brackets
    text <- gsub("\\[.*?\\]", "", text)
    # Remove text within round brackets
    text <- gsub("\\(.*?\\)", "", text)
    return(text)
  }

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the endowment based on text content (entries with a dollar sign)
    endowment <- university_page %>%
      html_nodes("table.infobox th:contains('Endowment') + td") %>%
      html_text() %>%
      unique() %>%
      clean_text()
  

    # Check if 'Endowment' entry exists, if not, extract 'Budget'
    if (length(endowment) == 0) {
      budget <- university_page %>%
        html_nodes("table.infobox th:contains('Budget') + td") %>%
        html_text() %>%
        unique() %>%
        clean_text()

      endowment_data <- append(endowment_data, list(budget))
    } else {
      endowment_data <- append(endowment_data, list(endowment))
    }
  }

  # Add a new column 'Endowment' with the scraped data
  empty_tibble$Endowment <- endowment_data
  empty_tibble <- empty_tibble %>%
    mutate(Endowment = map_chr(Endowment, ~ .[1]))


  #### Students
  
  # To avoid issues with students being spread on different campuses, scrape total student information by adding information on all undergraduates to all postgraduates
  undergraduates_data <- list()
  postgraduates_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
  
    # Undergraduates 
    # Extract the number of undergraduate students
    undergraduates <- university_page %>%
      html_nodes("table.infobox th:contains('Undergraduates') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_undergraduates <- str_extract(undergraduates, "[\\d,]+")
  
    # Assign NA if cleaned_undergraduates is empty
    cleaned_undergraduates <- ifelse(length(cleaned_undergraduates) == 0, NA, cleaned_undergraduates)
    undergraduates_data <- append(undergraduates_data, list(cleaned_undergraduates))

    # Postgraduates 
    # Extract the number of postgraduate students
    postgraduates <- university_page %>%
      html_nodes("table.infobox th:contains('Postgraduates') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_postgraduates <- str_extract(postgraduates, "[\\d,]+")
  
    # Assign NA if cleaned_postgraduates is empty
    cleaned_postgraduates <- ifelse(length(cleaned_postgraduates) == 0, NA, cleaned_postgraduates)
    postgraduates_data <- append(postgraduates_data, list(cleaned_postgraduates))
  }

  # Remove commas from the extracted data and convert to numeric
  empty_tibble$Undergraduates <- as.numeric(gsub(",", "", unlist(undergraduates_data)))
  empty_tibble$Postgraduates <- as.numeric(gsub(",", "", unlist(postgraduates_data)))

  # Create a new column 'Students' as the sum of Undergraduates and Postgraduates
  empty_tibble$Students <- ifelse(is.na(empty_tibble$Undergraduates), 
                                  empty_tibble$Postgraduates, 
                                  ifelse(is.na(empty_tibble$Postgraduates), 
                                        empty_tibble$Undergraduates, 
                                        empty_tibble$Undergraduates + empty_tibble$Postgraduates))

  # If the website neither holds information on undergraduates nor on postgraduates, look for information on total students
  students_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the students
    students <- university_page %>%
      html_nodes("table.infobox th:contains('Students') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_students <- as.numeric(gsub(",", "", str_extract(students, "[\\d,]+")))
    students_data <- append(students_data, list(cleaned_students))
  }

  # Check if any element in 'Students' is NA before assigning
  if (any(is.na(empty_tibble$Students))) {
    empty_tibble$Students[is.na(empty_tibble$Students)] <- unlist(students_data)
  }

  # Remove columns named 'Undergraduates' and 'Postgraduates'
  empty_tibble <- select(empty_tibble, -Undergraduates, -Postgraduates)
  
  
  # Finally, return the table including the scraped information
  return(empty_tibble)
}
```

```{r call the scrape_wikipedia_tables() webscraper,  eval = FALSE}

# Run the function using the URL from the Wikipedia page
url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"
scraped_wikipedia_df <- scrape_wikipedia_tables(url)

# Save it as a global variable
assign("scraped_wikipedia_df", scraped_wikipedia_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_wikipedia_df, file = "scraped_wikipedia_df.RData")
```

```{r create a copy of scraped_wikipedia_universities}

# Load global variables
load("scraped_wikipedia_df.RData")

# Copy the scraped table to avoid modifying the original data
universities_table <- scraped_wikipedia_df
```


```{r transform the endowment values}

# Function to transform endowment values
transform_endowment <- function(endowment_value) {
  # Extract numeric part and multiplier
  numeric_part <- as.numeric(gsub("[^0-9.]", "", endowment_value))
  multiplier <- ifelse(grepl("billion", tolower(endowment_value)), 1e9,
                       ifelse(grepl("million", tolower(endowment_value)), 1e6, 1))
  
  # Return the transformed numeric value
  return(numeric_part * multiplier)
}

# Apply the transformation to the Endowment column
universities_table$Endowment <- sapply(universities_table$Endowment, transform_endowment)
```

#### Notes on the function

**Geographic Location**: Some universities have several sets of coordinates on their Wikipedia sites. For instance, there are universities with multiple campuses whose geographic locations are all noted down on Wikipedia. Therefore, the function above includes a line of code that specifies that only the very first set of coordinates is appended to the data frame. Thereby, the geographic location column holds simplified data that potentially might not reflect the location of the entire university.

**Endowment**: Not all universities we are interested in include information on the `endowment` on their Wikipedia website. However, some university Wikipedia articles include data on the university's `budget.` Since `endowment` and `budget` likely refer to the same variable, we have included an if-statement in the function above, that specifies the following: If a university's Wikipedia article does not hold information on the 
`endowment`, the function will look whether there is information on the `budget` and use this data instead. Nonetheless, there are a few universities that have neither the `endowment` nor the `budget` declared on Wikipedia. For those instances, the endowment variable in our table will hold a missing value as specified in the section below.

**Students**: On some websites, the total student body is not declared in a comprehensive way because it is split up by the different campuses of the university. Therefore, it is more efficient to scrape the data by adding the number of postgraduates and undergraduates instead of writing a function that can add all of the student bodies from different campuses of the individual universities. To ensure that also universities that only have postgraduates or undergraduates, and universities which only have a number for the total student body on their website, the code is modified. Firstly, an if-statement will ensure that for those instances with NA's for either postgraduates or undergraduates, the number from the variable without the NA is declared as the total student body. Secondly, for the instances that only include data on the total student body on the website, another statement is formulated that scrapes the `Students` data from the Wikipedia website and then adds this to the variable in our scraped data set.

#### Notes on missing values

**Geographic Location**: There are missing values in the geographic location column. To be exact, the [University of Colorado Denver](https://en.wikipedia.org//wiki/University_of_Colorado_Denver) and the [University of Mississippi](https://en.wikipedia.org//wiki/University_of_Mississippi) do not have information on the geographic location on their Wikipedia websites. Missing values are declared as NA in our data frame.

**Endowment**: For some universities, the Wikipedia website neither includes data on `endowment` nor on `budget`. More precisely, the [Airforce Institute of Technology](https://en.wikipedia.org//wiki/Air_Force_Institute_of_Technology), [Azusa Pacific University](https://en.wikipedia.org/wiki/Azusa_Pacific_University), [Long Island University](https://en.wikipedia.org//wiki/LIU_Post	), and the [University of Missouri–St. Louis](https://en.wikipedia.org//wiki/University_of_Missouri%E2%80%93St._Louis	) do not have information on the website on endowment/budget. Missing values are declared as NA in our data frame.


### c. Data munging


```{r start exercise 2c}

#####################################
# 2C
#####################################

```


#### The `ivyleagues` data frame

```{r read in ivyleague.csv}

# Read the CSV file into a data frame
ivyleagues <- read.csv("ivyleague.csv")
```

We have stored `ivyleague.csv` as a data frame called `ivyleagues`. Which has eight observations and the following four variables:

i. `uni_name`: a **shortened** version of each university's name
ii. `county`: the county in which the university's main campus is located 
iii. `state`: the state in which the university's main campus is located
iv. `ein`: the Employer Identification Number

As the primary key - `Institution` - in the relational database that we will create later is the full name of the university, we created a new variable in the `ivyleagues` data frame that gives the full name instead of the shortened version for each Ivy League. We store this variable under the column name `Institution`. This ensures that we can later properly gather and bind data from different tables within the relational database. 

```{r add the full names to the universities in ivyleagues}

# Add a column called Institution with the full name of the Ivy Leagues
ivyleagues <- ivyleagues %>%
  mutate(
    Institution = case_when(
      uni_name == "Penn" ~ "University of Pennsylvania",
      uni_name == "Brown" ~ "Brown University",
      uni_name == "Columbia" ~ "Columbia University",
      uni_name == "Cornell" ~ "Cornell University",
      uni_name == "Dartmouth" ~ "Dartmouth College",
      uni_name == "Harvard" ~ "Harvard University",
      uni_name == "Princeton" ~ "Princeton University",
      uni_name == "Yale" ~ "Yale University",
    )
  )
```

#### Ivy League Indicator, County and EIN

Now, we use the `iveleagues` data frame to create three new variables in the `universities_table` data frame, namely:

ix. `Ivy_League`: An binary variable indicating whether the university is an Ivy League institution
x. `County`: The university's county and state (only for Ivy Leagues, for all others, `County` is NA)
xi. `EIN`: The university's EIN (only for Ivy Leagues, for all others, `EIN` is NA)

```{r clean the ivyleagues table}

# Clean the ivyleagues table
ivyleagues <- ivyleagues %>%
  mutate(County = ifelse(!is.na(county), paste(county, state, sep = ", "), state))

# Add the three variables Ivy_League, County, EIN to universities_table
universities_table <- universities_table %>%
  left_join(ivyleagues %>% select(Institution, ein, County), by = c("Institution" = "Institution")) %>%
  mutate(Ivy_League = ifelse(Institution %in% ivyleagues$Institution, 1, 0)) %>%
  rename(EIN = ein)
```


### d. Writing to your relational database


```{r start exercise 2d}

#####################################
# 2D
#####################################

```


#### Check if the data is tidy

We check whether the data is tidy in two steps:

##### 1. Each row is a unique university

```{r check if each row holds one unique observation}

# Check if the rows in universities_table each hold one unique university 
if (length(unique(universities_table$Institution)) == length(universities_table$Institution)) {
  answer <- sprintf("There are %d unique universities in the dataframe 'universities_df' that are each represented by exactly one row. The data is tidy.", length(unique(universities_table$Institution)))
  print(answer)
} else {
  print("Not every row holds the data of a unique university. The data is not tidy.")
}
```

##### 2. Each column is a variable

```{r check if each variable is represented by one column}

# Check if the columns in universities_table each represent one of the 11 variables
if (ncol(universities_table) == 11) {
  column_names <- colnames(universities_table)
  column_answer <- sprintf("The dataframe has %d columns, which each represent one variable:", ncol(universities_table))
  formatted_names <- sprintf("Column %d: %s", 
                             seq_along(column_names), 
                             column_names)
  cat(column_answer, formatted_names, sep = "\n")
} else {
  print("The dataframe does not have 11 columns. It is not tidy.")
}
```

As we can see, the data is tidy. Each row represents one of the 279 unique universities and each column holds one of the 11 variables. The column `Institution` holds the unique name of each university and will be used as the primary key.

#### Write universities_table to the relational database

We write `universities_table` to the relational database `db` under the name `universities_df`. The primary key is the university name, which can be accessed via the `Institution` variable.

```{r write universities_table to the relational database}

# Write universities_table to the relational database
dbWriteTable(db, "universities_df", universities_table, overwrite = TRUE)
```

Now, we want to check the existence and dimensionality of `universities_df`. We do so by defining a function `check_table()` that returns the number of rows and columns and the column names of the table if it exists in our relational database.

```{r write a function that checks the existence and correct dimensionality of the table, echo=TRUE}

# Check_table is a function that checks the existence and dimensionality of a table in db
check_table <- function(db, a_table){
  
  # Check if the "a_table" exists
  if (a_table %in% dbListTables(db)) {
    
    # Get the row count
    query <- paste("SELECT COUNT(*) FROM", a_table)
    row_count <- dbGetQuery(db, query)[1, 1]
  
    # Get the column names
    column_names <- as.character(dbListFields(db, a_table))
  
    # Get the column count
    column_count <- length(unlist(column_names))
    
    # Print out dimensions and column names
    formatted_string <- sprintf("The table %s exists and has the following dimensions: \nNumber of rows: %s \nNumber of columns: %s", a_table, row_count, column_count)
    column_answer <- sprintf("Column names: %s", paste(column_names, collapse = ", "))
    
    return(cat(formatted_string, column_answer, sep = "\n"))
    
  } else {
    # If "a_table" does not exist, return this statement:
    return(cat("The table does not exist."))
  }
}

# Call check_table on "universities_df"
check_table(db, "universities_df")
```



## Exercise 3

```{r start exercise 3a}

#####################################
# EXERCISE 3
#####################################
# 3A
#####################################

```


### a. Scraping annual rank

#### Scrape the world ranking for Ivy League universities

We create a webscraping function `scrape_university_ranking_global()` that scrapes the world ranking for each Ivy League university from the [Academic Ranking of World Universities Website](https://www.shanghairanking.com/) for the years 2003, 2013, and 2023. It returns a data frame `ivyleagues_ranks` that has the following three variables:

i. `Institution`: The name of the Ivy League university
ii. `Year`: The year for which the ranking has been scraped (2003, 2013, or 2023)
iii. `Rank`: The ARWU ranking for the university for the specific year

The data frame is in tidy long format where each row represents a combination of university and year (e.g., Harvard-2013). Since there are eight Ivy Leagues and three years that we are interested in, the data frame `ivyleagues_ranks` should have 24 rows. The code for the webscraper can be seen in the Appendix.

```{r launch the browser, results='hide', message=FALSE, eval=FALSE}

# Launch the driver and browser
invisible(capture.output({
  rD <- rsDriver(browser=c("firefox"), port = free_port(random = TRUE), chromever = NULL) 
  driver <- rD$client
}))
```


```{r navigate to the website, eval=FALSE}

# Navigate to the website
url <- "https://www.shanghairanking.com/"
driver$navigate(url)

# Navigate to the 2023 ranking 
academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[2]/div[1]/button')
academic_ranking_23$clickElement()
```


```{r function that searches for the ivys}

# A function that searches for each ivy league university 
search_for <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[1]/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(1)
  search_field$sendKeysToElement(list(key = "enter"))
  
}
```


```{r create a function that navigates to a different year }

# A function that navigates to a different year
navigate_year <- function() {
  # Find the element using the provided XPath
  year_selector <- driver$findElement(using = "xpath", value =  '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[1]/img')
  
  # Click on the element
  year_selector$clickElement()
}
```


```{r functions to extract the world rank and the year}

# A function that extracts the rank from the website
extract_rank <- function() {
  # Find the element on the website and transform it to text directly
  current_university_rank <- driver$findElement(using = "xpath",
                                                value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/table/tbody/tr/td[1]/div')$getElementText()[[1]]
  
  return(current_university_rank)
}
```


```{r create a dataframe with ivy leagues}

# Create a data frame that holds all ivy league universities as displayed in the ivyleagues data
ivyleagues_df <- data.frame(
  Institution = ivyleagues$Institution,
  # Add an empty column for the year 
  Year = numeric(length(ivyleagues$Institution)),
  # Add an empty column for the Rank
  Rank = numeric(length(ivyleagues$Institution)),
  stringsAsFactors = FALSE
)

# Create three copies of the ivyleagues_df data frame for each of the three years that we will later scrape ranking data from
ivyleagues_03 <- ivyleagues_df
ivyleagues_13 <- ivyleagues_df
ivyleagues_23 <- ivyleagues_df
```


```{r extract the world rank of each ivy league}

# Scrape the rank for each ivy league university from the website for the years 2003, 2013, 2023

scrape_university_ranking_global <- function() {
  ##########
  ## 2023 ##
  ##########

  # Navigate to 2023
  navigate_year()

  academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[1]')
  academic_ranking_23$clickElement()

  # Loop through all ivy league universities in the ivyleagues data set and extract the ranking for each university
  for (i in seq_along(ivyleagues_23$Institution)) {
    search_for(ivyleagues_23$Institution[i])
    Sys.sleep(2)  
    ivyleagues_23$Rank[i] <- extract_rank()
    ivyleagues_23$Year[i] <- "2023"
  }

  ##########
  ## 2013 ##
  ##########

  # Navigate to 2013
  navigate_year()

  academic_ranking_13 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[11]')
  academic_ranking_13$clickElement()

  # Loop through all ivy league universities in the ivyleagues data set and extract the ranking for each university
  for (i in seq_along(ivyleagues_13$Institution)) {
    search_for(ivyleagues_13$Institution[i])
    Sys.sleep(2)  
    ivyleagues_13$Rank[i] <- extract_rank()
    ivyleagues_13$Year[i] <- "2013"
  }

  ##########
  ## 2003 ##
  ##########

  # Navigate to 2003
  navigate_year()

  academic_ranking_03 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[21]')
  academic_ranking_03$clickElement()

  # Loop through all ivy league universities in the ivyleagues data set and extract the ranking for each university
  for (i in seq_along(ivyleagues_03$Institution)) {
    search_for(ivyleagues_03$Institution[i])
    Sys.sleep(2)  
    ivyleagues_03$Rank[i] <- extract_rank()
    ivyleagues_03$Year[i] <- "2003"
  }
  
  # Creating a list of data frames
  scraped_ivyleagues_df <- rbind(ivyleagues_03, ivyleagues_13, ivyleagues_23)

  return(scraped_ivyleagues_df)

}
```


```{r call the scrape_university_ranking_global() webscraper, eval = FALSE}

# Call the global rank scraping function
scraped_ivyleagues_df <- scrape_university_ranking_global()

# Save it as a global variable
assign("scraped_ivyleagues_df", scraped_ivyleagues_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_ivyleagues_df, file = "scraped_ivyleagues_df.RData")
```


```{r create a copy of scraped_ivyleagues_df}

# Load global variables
load("scraped_ivyleagues_df.RData")

# Copy the scraped table to avoid modifying the original data
ivyleagues_ranks <- scraped_ivyleagues_df
```


**Note**: For some Ivy League universities, the ranking is reported as a range instead of a single number. For these observations, we have calculated the midpoint of the reported range and used that as the value for our `Rank` column.

```{r calculate the midpoint for global rank}

# Function to calculate midpoint from a range
calculate_midpoint <- function(rank) {
  for (i in seq_along(rank)) {
  range_midpoint <- mean(as.numeric(strsplit(rank[i], "-")[[1]])) 
  return(range_midpoint)}
}

# Calculate the midpoint
ivyleagues_ranks$Rank <- sapply(ivyleagues_ranks$Rank, calculate_midpoint)
```


#### Write it to the database

Under the name `ivyleagues_ranks_df`, we write our table `ivyleagues_ranks` to the relational database `db` and check for its existence and correct dimensionality using the function `check_table()` from Exercise 2d.


```{r write the ivyleagues_ranks table to the relational database}

# Adding the table to the relational database
dbWriteTable(db, "ivyleagues_ranks_df", ivyleagues_ranks, overwrite = TRUE)
```


```{r check the existence of the ivyleagues_ranks table, echo=TRUE}

# Check the existence of the ivyleagues_ranks table
check_table(db, "ivyleagues_ranks_df")
```
The table `ivyleagues_ranks_df` has successfully been written to the relational database and has the dimensions and column names that we expected. Again, the primary key is the `Institution` variable.


```{r navigate to the home page, eval=FALSE}

# Navigate to the home page to make it easier to keep working on this website
home_page <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[1]/a')

home_page$clickElement()
```


### b. Scraping subject ranks for 2023

```{r start exercise 3b}

#####################################
# 3B
#####################################

```



#### Scrape the social science rankings for Ivy League universities for 2023

We create another webscraping function `extract_social_science_rank()` that extracts the ranking for every social science for which the university has been ranked for the year 2023. Again, we are only interested in the eight Ivy League universities. The webscraper returns a data frame called `social_science_ranks` that has three variables:

i. `Institution`: The name of the Ivy League university
ii. `Subject`: The social science for which we extract the ranking
iii. `Number`: The ranking for the university for the specific subject

The data frame is in tidy long format where each row represents a combination of university and discipline (e.g., Harvard-Economics). Before scraping, we do not know for how many social science subjects each Ivy League university has been ranked. Therefore, we do not know how many rows to expect in the data frame before extracting the rankings. The code for the webscraper can be seen in the Appendix.


```{r function that navigates to the subject rankings}

# A function that navigates to the page with the subject rankings
navigate_universities <- function() {
  universities_ranks <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[3]')
  universities_ranks$clickElement()
}
```

```{r navigate to the page with the subject rankings, eval=FALSE}

navigate_universities()
```


```{r function that searches for the ivys in the general ranking and selects it}

# A function that searches for each ivy league university 
search_general_ranking <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[1]/div/div/div/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(2)
  search_field$sendKeysToElement(list(key = "enter"))
  
  # Select the first element
  Sys.sleep(2)
  first_result <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[1]/div[2]') 
  first_result$clickElement()

}
```


```{r filter for social sciences}

# A function that filters for the social sciences in the ranking options (with js)
filter_social_sciences <- function() {
  
  # Click the filter field
  filter_subject <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div/div[2]/div[2]/div[2]/div[1]/div[1]/div[2]/div/div[1]/img')
  filter_subject$clickElement()

  
  partial_inner_html <- "Social Sciences"

  # Construct an XPath that searches for an element containing the specified inner HTML
  xpath_expression <- sprintf('//*/text()[contains(., "%s")]/parent::*', partial_inner_html)

  # Find the element using the constructed XPath
  social_sciences_element <- driver$findElement(using = "xpath", value = xpath_expression)

  # Click on the found element
  social_sciences_element$clickElement()
  
}
```


```{r extract the social science ranks}

# Create a df that will later hold the rankings for the social sciences
social_science_ranks <- data.frame(Institution = character(),
                          Subject = character(),
                          Number = character(),
                          stringsAsFactors = FALSE)

# A function that extracts the social science rankings and appends them to the data frame "social_science_ranks"
extract_social_science_rank <- function(institution_name, existing_table) {
  
  # entire table with ranks
  rank_table <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div/div[2]/div[2]/div[2]/div[1]/div[2]/table/tbody')$getElementText()[[1]]
  
  # Split the data into subjects and numbers
  split_data <- unlist(strsplit(rank_table, "\n"))

  # Add the scraped data to the existing table
  existing_table <- rbind(existing_table,
                          data.frame(Institution = rep(institution_name, length(split_data) / 2),
                                     Subject = split_data[seq(1, length(split_data), by = 2)],
                                     Number = split_data[seq(2, length(split_data), by = 2)],
                                     stringsAsFactors = FALSE))
  
  return(existing_table)
}

```


```{r scrape social science rankings, eval=FALSE}

# Scrape social science rankings

navigate_universities()
Sys.sleep(2)  

# University of Pennsylvania
search_general_ranking("University of Pennsylvania")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("University of Pennsylvania", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Brown University
search_general_ranking("Brown University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Brown University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Columbia University
search_general_ranking("Columbia University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Columbia University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Cornell University
search_general_ranking("Cornell University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Cornell University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Dartmouth College
search_general_ranking("Dartmouth College")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Dartmouth College", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)


# Harvard University
search_general_ranking("Harvard University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Harvard University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)


#	Princeton University
search_general_ranking("Princeton University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Princeton University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)


#	Yale University
search_general_ranking("Yale University")
Sys.sleep(2)  
filter_social_sciences()
social_science_ranks <- extract_social_science_rank("Yale University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)

# Save it as a global variable
assign("social_science_ranks", social_science_ranks, envir = .GlobalEnv)

# Save the global variable to an RData file
save(social_science_ranks, file = "social_science_ranks.RData")
```


```{r create a copy of social_science_ranks}

# Load global variables
load("social_science_ranks.RData")

# Copy the scraped table to avoid modifying the original data
social_science_ranks_df <- social_science_ranks
```

**Note**: As in Exercise 3a, some rankings were reported as ranges. Again, we used the midpoint for these instances to report the ranking in our data frame.

```{r calculate the midpoint for the ranks in social_science_ranks_df}

# Calculate the midpoint for the social science rankings that were displayed as a range
social_science_ranks_df$Number <- sapply(social_science_ranks_df$Number, calculate_midpoint)
```


#### Write the table to the database

Under the name `social_science_ranks_df`, we write our table `social_science_ranks` to the relational database `db` and check for its existence and correct dimensionality using the function `check_table()` from Exercise 2d.

```{r write the social_science_ranks_df table to the relational database}

# Adding the table to the relational database
dbWriteTable(db, "social_science_ranks_df", social_science_ranks_df, overwrite = TRUE)
```


```{r check the existence of the social_science_ranks_df table, echo=TRUE}

# Check the existence of the social_science_ranks table
check_table(db, "social_science_ranks_df")
```
The table `social_science_ranks_df` has successfully been written to the relational database and has the dimensions and column names that we expected. Again, the primary key is the `Institution` variable.

```{r close the RSelenium process, eval=FALSE}

# Close the RSelenium processes:
driver$close()
# Close the associated Java processes
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
```





## Exercise 4

```{r start exercise 4a}

#####################################
# EXERCISE 4
#####################################
# 4A
#####################################

```


### a. Gathering financial data from a raw API

#### Extract total revenue and total assets for Ivy League universities

We use `hhtr` to gather financial data on the Ivy League universities from the [ProPublica API](https://projects.propublica.org/nonprofits/api). More precisely, we create a function `get_finance_data()` that takes the `EIN` of the Ivy Leagues as input factors, retrieves the name of the university and queries the API for the data corresponding to said university. Then, we extract the variables of interest and store them in a data frame `finance_df`. It has the following five columns:

i. `Institution`: The name of the Ivy League university (primary key)
ii. `ein`: The Employer Identificaton Number as extracted from the `ivyleagues.csv` file
iii. `year`: The year for which the financial information has been extracted from the API
iv. `total_revenue`: The total revenue of the university in the specified year, in USD
v. `total_assets`: The total assets of the university in the specified year, in USD

The data frame is in tidy long format where each row represents a combination of university and year (e.g., Harvard-2013). As we are interested in data for the years 2011-2021 (inclusive) for eight Ivy League universities, we should expect `finance_df` to have 88 rows.^[Note that in the original assignment, Exercise 4a asks for the years 2010 - 2020. However, the ProPublica API has been updated and now contains data until including 2021. Thereby, we just scraped the 11 most recent years, which are 2011-2021.]


```{r create an empty dataframe finance_df}

# Create an empty dataframe for the financial information
finance_df <- data.frame(
  Institution = NA,
  # Add a column for the EIN
  ein = NA,
  # Add an empty column for the year
  year = NA,
  # Add an empty column for the total revenue
  total_revenue = NA,
  # Add an empty column for the total assets
  total_assets = NA,
  stringsAsFactors = FALSE
)
```


```{r get the university name given the ein}

# Function that gets the university name given the EIN
get_university_name <- function(ein) {
  # Look for the EIN in the dataframe  
  matching_row <- ivyleagues[ivyleagues$ein == ein, ]
  # Check if a match is found
  if (nrow(matching_row) > 0) {
    # Return the corresponding university name
    return(matching_row$Institution)
  } else {
    # Return a message indicating no match found
    return("University not found")
  }
}
```


```{r get the data from the API given the ein}

# Function that queries the API for each university's data given their EIN
get_data <- function(ein) {
  # Create a URL given the EIN
  url <- paste0("https://projects.propublica.org/nonprofits/api/v2/organizations/", ein, ".json")
  response <- GET(url)
  api_data <- content(response, "parsed")
  
  return(api_data)
}
```


```{r get the finance data using the previously defined functions}

# function that gets the finance data given the ein and using the functions get_university_name and get_data
get_finance_data <- function(ein) {
  # Get the university name given the ein
  name <- get_university_name(ein)
  
  # Get the data given the ein
  dt <- get_data(ein)
  
  # Loop to append values
  for (i in seq(length(dt$filings_with_data))) {
    # Append the values to the dataframe
    finance_df <- rbind(finance_df, data.frame(Institution = name,
                                               ein = ein,
                                               year = dt$filings_with_data[[i]]$tax_prd_yr,
                                               total_assets = dt$filings_with_data[[i]]$totassetsend,
                                               total_revenue = dt$filings_with_data[[i]]$totrevenue))
  }

  # Return the resulting dataframe
  return(finance_df)
}
```


```{r call the get_finance_data() function, eval=FALSE}

# Call the "get_finance_data()" function
result_list <- lapply(ivyleagues$ein, get_finance_data)
scraped_finance_df <- do.call(rbind, result_list)

# Save it as a global variable
assign("scraped_finance_df", scraped_finance_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_finance_df, file = "scraped_finance_df.RData")
```


```{r load scraped_finance_df.RData}

# Load global variables
load("scraped_finance_df.RData")

# Copy the scraped table to avoid modifying the original data
finance_df <- scraped_finance_df
```


```{r format the finance_df}

# Delete rows that only consist of missing values
finance_df <- finance_df %>%
  na.omit() 

# Fill NA values for missing years
complete_df <- expand.grid(year = 2011:2021, 
                           ein = unique(finance_df$ein))

# Add the "Institution" column to complete_df
complete_df <- complete_df %>%
  left_join(unique(finance_df[, c("ein", "Institution")]), by = "ein") %>%
  arrange(ein, year)

# Merge with the original dataframe
finance_df <- complete_df %>%
  left_join(finance_df, by = c("year", "ein")) %>%
  arrange(ein, year)

finance_df <- finance_df %>%
  select(-Institution.y, Institution = Institution.x)
```

**Note**: The final data frame has missing values. For instance, the data for `total_revenue` and `total_assets` of Harvard University is unavailable for the year 2020. The observations with missing values for `total_revenue` and `total_assets` were simply not extracted from the API and thus did not appear in the data frame `finance_df`. Therefore, we computed these rows and added NA's for their financial information for the sake of completeness.


#### Write it to the database

Under the name `finance_df`, we write our table `finance_df` to the relational database `db` and check for its existence and correct dimensionality using the function `check_table()` from Exercise 2d.

```{r write the finance_df table to the relational database}

# Adding the table to the relational database
dbWriteTable(db, "finance_df", finance_df, overwrite = TRUE)
```


```{r check the existence of the finance_df table, echo=TRUE}

# Check the existence of the ivyleagues_ranks table
check_table(db, "finance_df")
```
The table `finance_df` has successfully been written to the relational database and has the dimensions and column names that we expected.


### b. Gathering local economic data from a packaged API 

```{r start exercise 4b}

#####################################
# 4B
#####################################

```


```{r set up the api key, message=FALSE}

readRenviron("../../Documents/R_Environs/api_census.env")
apikey <- Sys.getenv("KEY")
census_api_key(apikey)
```

#### Extract the median estimated household income of all counties in the US, 2015 and 2020

We use the `tidycensus` package to access the API of the US Census Bureau. We write a function that retrieves the names of all the Counties in the US and their estimated median household income for every county for both 2015 and 2020 (based on the American Community Survey (ACS)). 

To a table `census_df`, we add the data retrieved from the US Census Bureau API for each Ivy League university. More precisely, `census_df` has the following four columns:

i. `Institution`: The name of the Ivy League university (primary key)
ii. `County`: The county in which the university’s main campus is located
iii. `Year`: The year for which the data has been retrieved (either 2015 or 2020)
iv. `Median_Household_Income`: The estimated median household income for the county, in which the Ivy League is located, in USD

The data is in tidy long format where each row represents a combination of university and year (e.g., Harvard-2015). As we are interested in two years, 2015 and 2020, for eight Ivy League universities, we should expect `census_df` to have 16 rows.

```{r retrieve median estimated household income, eval=FALSE}

# Median household income 2015
median_household_income_15 <- get_acs(geography = "county", year = 2015, variables = "B19013_001")
median_household_income_20 <- get_acs(geography = "county", year = 2020, variables = "B19013_001")

# Save it as a global variable
assign("median_household_income_15", median_household_income_15, envir = .GlobalEnv)
assign("median_household_income_20", median_household_income_20, envir = .GlobalEnv)

# Save the global variable to an RData file
save(median_household_income_15, file = "median_household_income_15.RData")
save(median_household_income_20, file = "median_household_income_20.RData")
```


```{r load median_household_income_15.RData and median_household_income_20.RData}

# Load global variables
load("median_household_income_15.RData")
load("median_household_income_20.RData")

# Copy the scraped table to avoid modifying the original data
income_15_df <- median_household_income_15
income_20_df <- median_household_income_20
```


```{r create a dataframe census_df}

# Create an empty dataframe
census_df <- data.frame(
  Institution = ivyleagues$Institution,
  County = ivyleagues$County,
  stringsAsFactors = FALSE
)

# Add the information on the median household income from the census API for 2015 and 2020
census_df <- census_df %>%
  
  # For 2015
  left_join(select(income_15_df, NAME, estimate), by = c("County" = "NAME")) %>%
  mutate("2015" = estimate) %>%
  select(-estimate)  %>%
  
  # For 2020
  left_join(select(income_20_df, NAME, estimate), by = c("County" = "NAME")) %>%
  mutate("2020" = estimate) %>%
  select(-estimate)

# Pivot longer
census_df <- census_df %>%
  pivot_longer(cols = c("2015", "2020"),
               names_to = "Year",
               values_to = "Median_Household_Income")

# Set the year column as numeric
census_df$Year <- as.numeric(census_df$Year)
``` 


#### Write it to the database

Under the name `census_df`, we write our table `census_df` to the relational database `db` and check for its existence and correct dimensionality.

```{r write the census_df table to the relational database}

# Write the table to the relational database
dbWriteTable(db, "census_df", census_df, overwrite = TRUE)
```


```{r check the existence of the census_df table, echo=TRUE}

# Check the existence of the census_df table
check_table(db, "census_df")
```
The table `census_df` has successfully been written to the relational database and has the dimensions and column names that we expected.


## Exercise 5

```{r start exercise 5a}

#####################################
# EXERCISE 5
#####################################
# 5A
#####################################

```


### a. Analysis and visualisation

#### Analysis table

We create a table "analysis_table" that includes the following variables for Ivy League institutions only from the relational database:

i. University name
ii. The average rank of the university across 2003, 2013, and 2023
iii. The average rank of the university's Economics, Political Science, and Sociology programs, if they were ranked
iv. The current endowment per student (total endowment divided by total number of students), in USD
v. The average total revenue per student across the years 2015 - 2020, in USD
vi. The average of the median household income for the County across the years 2015 and 2020, in USD


```{r sql queries for the analysis table}

# Second column: The average rank of the university across 2003, 2013, and 2023
second_query <- "
  SELECT Institution, AVG(Rank) AS Average_Rank
  FROM ivyleagues_ranks_df
  WHERE Year IN (2003, 2013, 2023)
  GROUP BY Institution;
"

# Execute the query
second_result <- dbGetQuery(db, second_query)


# Third column: The average rank of the university's Economics, Political Science, and Sociology programs, if they were ranked
third_query <- "
  SELECT Institution, AVG(Number) AS Avg_Econ_PolSc_Soc
  FROM social_science_ranks_df
  WHERE Subject IN ('Economics', 'Political Sciences', 'Sociology')
  GROUP BY Institution;
"

# Execute the query
third_result <- dbGetQuery(db, third_query)


# Fourth column: The current endowment per student (total endowment divided by total number of students), in USD
fourth_query <- "
  SELECT Institution, Endowment / Students AS Endowment_Per_Student
  FROM universities_df
  WHERE Ivy_League = 1;
"

# Execute the query
fourth_result <- dbGetQuery(db, fourth_query)


# Fifth column: The average total revenue per student across the years 2015 - 2020, in USD
fifth_query_students <- "
  SELECT Institution, Students
  FROM universities_df
  WHERE Ivy_League = 1;
"

fifth_query_revenue <- "
  SELECT Institution, AVG(total_revenue) AS Avg_Revenue_2015_2020
  FROM finance_df
  WHERE year BETWEEN 2015 AND 2020
  GROUP BY Institution;
"

# Execute the queries
fifth_result_students<- dbGetQuery(db, fifth_query_students)
fifth_result_revenue <- dbGetQuery(db, fifth_query_revenue)

fifth_result <- merge(fifth_result_students, fifth_result_revenue,
                      by = "Institution", all = TRUE) %>%
  mutate(Rev_Per_Student_2015_2020 = Avg_Revenue_2015_2020 / Students) %>%
  select(Institution, Rev_Per_Student_2015_2020)


# Sixth column: The average of the median household income for the County across the years 2015 and 2020, in USD
sixth_query <- "
  SELECT Institution, AVG(Median_Household_Income) AS Avg_Household_Income_2015_2020
  FROM census_df
  WHERE year IN (2015, 2020)
  GROUP BY Institution;
"

# Execute the queries
sixth_result <- dbGetQuery(db, sixth_query)
```


```{r combine the tables resulting from the six queries}

# Merge the five dataframes retrieved through the sql queries by Institution
list_of_dataframes <- list(second_result, third_result, fourth_result, fifth_result, sixth_result)
analysis_df <- reduce(list_of_dataframes, merge, by = "Institution", all = TRUE)
```



#### Visualisations with `ggplot`

We create four plots that show the relationships between two variables from the analysis_table.

##### 1. Average university ranking vs. average Econ/PS/Soc ranking

```{r first plot average university ranking and average Econ/PS/Soc ranking, message = FALSE}

# First ggplot: average university ranking and average Econ/PS/Soc ranking
first_plot <- ggplot(analysis_df, aes(x = Average_Rank, y = Avg_Econ_PolSc_Soc, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  # Add labels
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  # Add titles
  labs(title = "Average Overall Ranking vs. Subject Ranking for Ivy League Universities",
       x = "Average Overall Ranking (2003, 2013, 2023)",
       y = "Average Ranking for Economics, Political Sciences, and Sociology",
       color = "Ivy League University") +
  theme_minimal() +
  # Adjust the font size for the title and the axes
  theme(
    axis.text = element_text(size = 10),     
    axis.title = element_text(size = 10),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(1, 750), expand = c(0.02, 0)) +  # Adjust the limits and expand
  scale_y_log10(limits = c(1, 200), expand = c(0.02, 0))

# Display the first plot
first_plot
```

**The plot**: Comment on the logarithmic scale of the axes.

**The findings**: There seems to be a positive correlation between the overall ranking and the average ranking for the subjects Economics, Political Sciences, and Sociology, which is not surprising. A university that is ranked highly overall is expected to have high rankings in most subjects as well. It should be noted, though, that the overall ranking displayed on the x-axis is averaged over the years 2003, 2013, and 2023, whilst the average subject ranking displayed on the y-axis only refers to the most recent rankings in the subjects Economics, Political Sciences, and Sociology. Thereby, the possibility that the overall ranking changed significantly since 2003 and thus influenced the overall ranking (either positively or negatively) must thereby be considered. It would probably be more informative to retrieve the subject rankings for the three years of interest and then calculate the average.

```{r R squared for the first plot}

# Fit a linear regression model
model1 <- lm(Avg_Econ_PolSc_Soc ~ Average_Rank, data = analysis_df)

# View the R-squared value
rsquared_value_1 <- summary(model1)$r.squared
cat("R-squared Plot 1:", rsquared_value_1, "\n")
```


##### 2. Average university ranking vs. endowment per student

```{r second plot average university ranking and endowment per student, message = FALSE}

# Second ggplot: average university ranking and endowment per student
second_plot <- ggplot(analysis_df, aes(x = Average_Rank, y = Endowment_Per_Student/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  # Add labels
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  # Add titles
  labs(title = "Average Overall Ranking vs. Endowment Per Student",
       x = "Average University Ranking (2003, 2013, 2023)",
       y = "Endowment per Student, in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  # Adjust the font size for the title and the axes
  theme(
    axis.text = element_text(size = 10),     
    axis.title = element_text(size = 10),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(1, 750), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(100, 7500), expand = c(0.02, 0))

# Display the second plot
second_plot
```

**The plot**: Comment on the logarithmic scale of the axes.

**The findings**: The findings are somewhat surprising. One might expect universities with high rankings to have larger endowments per students as they firstly might be able to charge higher student fees due to their good reputation and secondly might be ranked so highly due to the availability of large resources that they can invest in the facilities and the quality of teaching. However, this expectation is flawed as it does not consider other factors that influence the relationship between the endowment per student and the overall ranking: 

1. Resource Allocation: A high endowment doesn't necessarily guarantee effective resource allocation that directly influences rankings. Some universities might have large endowments but allocate resources differently, prioritising spending on specific aspects such as faculty, research, infrastructure, or student programmes. This might impact their rankings differently.

2. Ranking Criteria: University rankings are based on various criteria, including academic reputation, faculty-to-student ratio, research output, and more. Endowment per student is just one financial aspect and might not capture the broader factors influencing rankings.

3. Other factors: Many other factors might explain the size of the endowment in relationship to the university ranking. Notably, Princeton University has a very high endowment per student. Yet one should note that Princeton has a relatively small student body. However, the high endowment might stem from other factors such as historical funding and donations, Alumni support, financial aid and tuition fees etc. The analysis of the correlation between ranking and endowment per student necessitates a more complex analysis of the factors explaining the composition of rankings and endowments.

```{r R squared for the second plot}

# Fit a linear regression model
model2 <- lm(Endowment_Per_Student ~ Average_Rank, data = analysis_df)

# View the R-squared value
rsquared_value_2 <- summary(model2)$r.squared
cat("R-squared Plot 2:", rsquared_value_2, "\n")
```



##### 3. Average endowment per student vs average median household income

```{r third plot average endowment per student and average median household income, message=FALSE}

# Third ggplot: average endowment per student vs average median household income
third_plot <- ggplot(analysis_df, aes(y = Avg_Household_Income_2015_2020/1000, x = Endowment_Per_Student/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  # Add labels
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  # Add titles
  labs(title = "Endowment Per Student vs. Average Median Household Income",
       x = "Endowment per Student, in thousand USD",
       y = "Average Median Household Income for the County (2015 and 2020), in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  # Adjust the font size for the title and the axes
  theme(
    axis.text = element_text(size = 8),     
    axis.title = element_text(size = 8),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(300, 6500), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(35, 150), expand = c(0.02, 0))

# Display the third plot
third_plot
```


**The plot**: Comment on the logarithmic scale of the axes.

**The findings**: There seems to be a slight positive correlation between the average median household income for the county and the endowment per student.

```{r R squared for the third plot}

# Fit a linear regression model
model3 <- lm(Avg_Household_Income_2015_2020 ~ Endowment_Per_Student, data = analysis_df)

# View the R-squared value
rsquared_value_3 <- summary(model3)$r.squared
cat("R-squared Plot 3:", rsquared_value_3, "\n")
```


##### 4. Average revenue per student vs average median household income

```{r fourth plot average revenue per student and average median household income, message=FALSE}

# Fourth ggplot: average revenue per student vs. average median household income
fourth_plot <- ggplot(analysis_df, aes(y = Avg_Household_Income_2015_2020/1000, x = Rev_Per_Student_2015_2020/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  # Add labels
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  # Add titles
  labs(title = "Average Revenue Per Student vs. Average Median Household Income",
       x = "Average Revenue Per Student (2015 - 2020), in thousand USD",
       y = "Average Median Household Income for the County (2015 and 2020), in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  # Adjust the font size for the title and the axes
  theme(
    axis.text = element_text(size = 8),     
    axis.title = element_text(size = 8),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(110, 600), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(25, 150), expand = c(0.02, 0))

# Display the fourth plot
fourth_plot
```

```{r R squared for the fourth plot}

# Fit a linear regression model
model4 <- lm(Avg_Household_Income_2015_2020 ~ Rev_Per_Student_2015_2020, data = analysis_df)

# View the R-squared value
rsquared_value_4 <- summary(model4)$r.squared
cat("R-squared Plot 4:", rsquared_value_4, "\n")
```



### b. Visualisation of geographic data


```{r start exercise 5b}

#####################################
# 5B
#####################################

```

Using `SQL`, call into `R` from your relational database a table that includes, for **every R1 and R2 university**:

i. University name
ii. Geographic coordinates
iii. Status (public vs. private)
iv. Whether the university is an Ivy League institution


#### Analysis table 

```{r sql queries for the geography table}

# Select columns: University name, geographic coordinates, status (private or public), Ivy League indicator
query_geography <- "
  SELECT Institution, Control, Geographic_Location, Ivy_League
  FROM universities_df;
"

# Execute the query
analysis_table_geography <- dbGetQuery(db, query_geography)

analysis_table_coordinates <- separate(analysis_table_geography, Geographic_Location, into = c("latitude", "longitude"), sep = " ", convert = TRUE)

# Extract numeric values from the coordinates
analysis_table_coordinates <- analysis_table_coordinates %>%
  mutate(
    latitude = as.numeric(str_extract(latitude, "([0-9.]+)")),
    longitude = as.numeric(str_extract(longitude, "([0-9.]+)"))
  ) %>%
  mutate(longitude = -longitude)

analysis_table_coordinates <- analysis_table_coordinates[complete.cases(analysis_table_coordinates$latitude, analysis_table_coordinates$longitude), ]

# Convert analysis_table geography into an sf object
geography_sf <- st_as_sf(analysis_table_coordinates, coords = c("longitude", "latitude"), crs = 4326)
```

Retrieve a [shapefile](https://en.wikipedia.org/wiki/Shapefile) of the United States using the `tigris` package (or, if you prefer the `tidycensus` package, but `tigris` is easier). Using either the `tmap` package or the `ggmap` package, include in the main section of your `.html` a visually clear and compelling map that is appropriately labelled which shows:

i. every R1 and R2 university, excluding the Ivy League institutions, as a point
ii. where the colour of the points varies by status (public vs. private)
iii. Ivy League universities as contrasting points 



```{r get the US states shapefile, message = FALSE}

invisible(capture.output({
  # Shapefile with US 
  us_map <- nation()
}))
```


```{r create the map plot, message = FALSE}

# Set the tmap mode
tmap_mode("view")

# Set the bounding box for the US
us_bbox <- c(-175, 17, -63, 73)

# Plot the regions
geographic_plot <- tm_shape(us_map) +
  tm_polygons("NAME", palette=c("United States"="#8DD08D"), alpha = 0.5, legend.show = FALSE) +
  
  # Dots for non-Ivy League universities
  tm_shape(geography_sf[geography_sf$Ivy_League == 0, ]) +
  tm_dots(size=0.05, 
          col="Control", palette=c("Public"='#F781BF', "Private (non-profit)"='#FFC20A'), 
          title="University Status") +
  
  # Dots for Ivy League universities
  tm_shape(geography_sf[geography_sf$Ivy_League == 1, ]) +
  tm_dots(size = 0.1, 
          col = "Ivy_League",
          palette=c("1"= "#FF0000", "0" = "grey"), 
          legend.show = TRUE,
          title = "Ivy League",
          labels = c("1" = "Ivy League", "0" = "Non-Ivy")) +
  
  tm_layout(title = "US R1 and R2 Universities") +
  
  # Set the view to the US bounding box
  tm_view(bbox = us_bbox)

geographic_plot

```



Is there any notable pattern to where the Ivy League universities concentrated? What about private and public universities? Do any parts of the United States appear particularly under-resourced in terms of research universities? How might you explain the patterns you observe?

#### Discussion of the Map

##### 1. Ivy Leagues are all on the East Coast

As seen in the map above, the Ivy League universities are all concentrated in the East Coast of the United States.

##### 2. Distribution of public and private universities

Public and private universities seemingly spread across the country. However, when displaying the OpenStreetMap instead of the default Esri.WorldGrayCanvas (can be selected by clicking on the filter icon in the top left corner) and zooming in, we can notice that most private universities are located in close proximity to larger cities (such as Los Angeles, Dallas, Chicago, Washington DC, New York City, Boston).

Several factors might explain this phenomenon, such as:

i. Access to Resources: Larger cities often offer better access to resources such as libraries, research facilities, cultural institutions, and industry partnerships. Private universities, which often rely on tuition and private funding, may be attracted to urban areas where these resources are abundant.

ii. Industry Connections: Proximity to major urban centers provides opportunities for private universities to establish strong connections with industries, businesses, and organisations. This can lead to collaborative research, internships, and employment opportunities for students.

iii. Networking Opportunities: Being close to a city facilitates networking opportunities for students and alumni. Private universities often emphasize networking and alumni relations, and being situated near a city with a robust job market can enhance students' chances of securing internships and jobs.

iv. International Attraction: Cities tend to attract a diverse population, including international students. Private universities, which may have a higher reliance on tuition revenue, may be attracted to urban areas for the potential to attract a broader and more diverse student body.

v. Philanthropy and Donor Base: Major cities are often home to a concentration of wealthy individuals and corporations. Private universities may be strategically located in proximity to potential donors, facilitating fundraising efforts and building a strong financial foundation.

However, it should be noted that in order to explain why private universities seemingly choose more urban locations, further research of the proposed factors would be necessary.

##### 3. Under-researched areas

It is noticeable, that the universities are mainly concentrated along the coastlines and in the East of the US. The Midwest/West and Alaska are relatively under-researched. Several factors might explain that observation:

i. Historical Development: Many of the oldest and most prestigious universities in the United States, such as Harvard and Princeton, were established in the colonial period and early years of the nation's history (Source: [Wikipedia](https://en.wikipedia.org/wiki/History_of_higher_education_in_the_United_States#:~:text=Explosive%20growth%20in%20the%20number,regional%20campuses%20around%20the%20state.)). These institutions often originated in the eastern part of the country. Potentially, the presence of these early institutions might have influenced the establishment of additional universities in nearby areas.

ii. Population Centers: The eastern and western coasts of the United States are home to some of the country's largest and most populous cities (Source: [Wikipedia](https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population)). Potentially, Universities might tend to develop in areas with large population centers.

iii. Economic Opportunities: Coastal areas might to have larger economic opportunities, including access to industries, businesses, and financial centers, potentially inspiring the establishment of higher education institutions.

iv. Transportation Infrastructure: As visualised in this [map of the US railway system](https://github.com/emmi3105/472_assignment3/blob/385882f6fed3a176ddc109860cf51011527df8c8/railway_map.png) the US American public transportation infrastructure is better developed in the Eastern part of the US compared to the Midwest. Potentially, access to public transportation is a motivating factor for establishing higher education institutions at specific places.






## Data

```{r close the database connection}

# Close the database connection
dbDisconnect(db)
```


## Sources



## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 


```
