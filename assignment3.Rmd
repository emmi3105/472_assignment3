---
title: "Assignment 3"
author: "Student ID: 201903536"
date: "30 November 2023"
output: html_document
---

```{r setup, include=FALSE} 
#####################################
# SETUP
#####################################

knitr::opts_chunk$set(echo = FALSE) 

#####################################
# Install/load packages
#####################################

library(rvest)
library(tidyverse)
library(purrr)
library(DBI)
library(RSQLite)
library(RSelenium)
library(netstat)
```


## GitHub

The GitHub repository for **this assignment** can be found [here](https://github.com/emmi3105/472_assignment3).


## Exercise 1

This assignment will take us through a workflow that a data scientist might encounter in the real world, from data collection right through to analysis. Throughout this assignment we are going to use a local relational database to store a variety of different but related tables that we collect and may want to combine in various ways. We will want to ensure that each table within our database can be joined to every other table using a `primary key`. 

We will start by creating an empty local relational database. You should store this new database in a 'database' folder that you create within your assignment folder. Follow these steps:

1. Use the `DBI::dbConnect()` function in `R` to create a new `SQLite` database (either `YOUR_DB_NAME.sqlite` or `YOUR_DB_NAME.db`) in your database folder.
2. Use the `file.exists()` function in `R` to check for the existence of your relational database.  

Include in the main text of your `.html` submission the code that created the database **and** the code that checks for its existence **and** the output of that check.


### Create a local relational database

```{r create database, echo=TRUE}

# Create database
db <- dbConnect(RSQLite::SQLite(), "universities_db.sqlite")

# Use file.exists() to check the existence of the relational database
print(file.exists("universities_db.sqlite"))
```



## Exercise 2

### a. Gathering structured data
Write an automatic webscraping function in R that constructs a table (as e.g. a data frame, a tibble, or a data table) of all R1 (Very High Research Activity) **and** R2 (High Research Activity) Research Universities in the United States of America. These data can be found on [wikipedia](https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States). 

Your initial scraper should collect five variables:

i. The university’s name
ii. It’s status (public or private)
iii. The city in which it is located
iv. The state in which it is located
v. The URL of the university’s dedicated Wikipedia page


#### Write and run the automatic webscraping function


```{r combine the two tables using for loops with url}

scrape_wikipedia_tables <- function(url) {
  table_indices <- c(19, 27)
  
  # Storing the URL's HTML code
  html_content <- read_html(url)
  
  # Extracting all tables in the document 
  tab <- html_table(html_content, fill = TRUE)

  # Extract only the tables of interest given the input table_indices
  selected_tables <- tab[1:length(table_indices)]

  # Iterate over tables and bind rows
  empty_tibble <- tibble()
  for (i in 1:length(selected_tables)) {
    current_table <- as_tibble(selected_tables[[i]][, 1:4])
    empty_tibble <- bind_rows(empty_tibble, current_table)
  }

  # Initialize an empty list to store hyperlinks
  hyperlinks_list <- list()

  # Iterate over the table indices and extract hyperlinks

  for (index in table_indices) {
    table_nodes <- html_content %>%
      html_nodes(paste("#mw-content-text > div.mw-content-ltr.mw-parser-output > table:nth-child(", index, ")", sep = ""))
  
    # Assuming you want to extract hyperlinks from the first table
    current_node_set <- table_nodes[[1]]
  
    hyperlinks_current_node <- current_node_set %>%
      html_nodes("td:nth-child(1) a") %>%
      html_attr("href")
  
    # Append the hyperlinks to the list
    hyperlinks_list <- c(hyperlinks_list, hyperlinks_current_node)
  }

  # Unlist to flatten the list
  hyperlinks_list <- unlist(hyperlinks_list, recursive = FALSE)


  # Concatenate the base URL to the extracted hyperlinks
  hyperlinks_list <- paste0("https://en.wikipedia.org/", hyperlinks_list)
  
  # Add a new column 'Wikipedia_URL' with the extracted URLs
  empty_tibble$Wikipedia_URL <- hyperlinks_list

  return(empty_tibble)
}

# Run the function using the URL from the Wikipedia page
url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"

result <- scrape_wikipedia_tables(url)
print(head(result))

```


### b. Gathering unstructured data
Extend your webscraping function (or create a new function) so that it navigates to the dedicated Wikipedia page for each university, and captures three additional variables: 

vi. The geographic coordinates of the (main) university campus
vii. The endowment of the university in USD dollars
viii. The total number of students (including both undergraduate and postgraduate)


```{r code for creating the tibble of previous exercise}
url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"
table_indices <- c(19, 27)
  
# Storing the URL's HTML code
html_content <- read_html(url)
  
# Extracting all tables in the document 
tab <- html_table(html_content, fill = TRUE)

# Extract only the tables of interest given the input table_indices
selected_tables <- tab[1:length(table_indices)]

# Iterate over tables and bind rows
empty_tibble <- tibble()
for (i in 1:length(selected_tables)) {
  current_table <- as_tibble(selected_tables[[i]][, 1:4])
  empty_tibble <- bind_rows(empty_tibble, current_table)
}

# Initialize an empty list to store hyperlinks
hyperlinks_list <- list()

# Iterate over the table indices and extract hyperlinks
for (index in table_indices) {
  table_nodes <- html_content %>%
    html_nodes(paste("#mw-content-text > div.mw-content-ltr.mw-parser-output > table:nth-child(", index, ")", sep = ""))
  
  # Assuming you want to extract hyperlinks from the first table
  current_node_set <- table_nodes[[1]]

  hyperlinks_current_node <- current_node_set %>%
    html_nodes("td:nth-child(1) a") %>%
    html_attr("href")

  # Append the hyperlinks to the list
  hyperlinks_list <- c(hyperlinks_list, hyperlinks_current_node)
}

# Unlist to flatten the list
hyperlinks_list <- unlist(hyperlinks_list, recursive = FALSE)

# Concatenate the base URL to the extracted hyperlinks
hyperlinks_list <- paste0("https://en.wikipedia.org/", hyperlinks_list)
  
# Add a new column 'Wikipedia_URL' with the extracted URLs
empty_tibble$Wikipedia_URL <- hyperlinks_list
print(empty_tibble)
```






#### Geographic Location


```{r}

# Exercise 2b


### geographic location

university_data <- list()

for (hyperlink in hyperlinks_list) {
  university_page <- read_html(hyperlink)
   
  # Extract the geographic location using the class selector 'geo_default'
  geographic_location <- university_page %>%
    html_nodes(".geo-dec") %>%
    html_text() %>%
    unique()
 
  # Store the scraped information in a list
  university_data <- append(university_data, list(geographic_location))
  
}

#empty_tibble <- empty_tibble %>%
#  mutate(Geographic_Location = NA)

# Add a new column 'Geographic_Location' with the scraped data
empty_tibble$Geographic_Location <- university_data

# The new variable Geographic_Location is a list of coordinate sets. We only want to keep the first set of coordinates
# Assuming 'your_tibble' is your existing tibble with a "Geographic_Location" column
empty_tibble <- empty_tibble %>%
  mutate(Geographic_Location = map_chr(Geographic_Location, ~ .[1]))
```

**Note**: There are missing values. To be exact, the [University of Colorado Denver](https://en.wikipedia.org//wiki/University_of_Colorado_Denver) and the [University of Mississippi](https://en.wikipedia.org//wiki/University_of_Mississippi) do not have information on the geographic location on their wikipedia websites.


#### Endowment

```{r}
# Exercise 2b


### Endowment and budget

endowment_data <- list()

# Clean text by removing text within brackets
clean_text <- function(text) {
  # Remove text containing brackets within brackets
  text <- gsub("\\([^()]*\\)", "", text)
  # Remove text within square brackets
  text <- gsub("\\[.*?\\]", "", text)
  # Remove text within round brackets
  text <- gsub("\\(.*?\\)", "", text)
  return(text)
}

for (hyperlink in hyperlinks_list) {
  university_page <- read_html(hyperlink)
   
  # Extract the endowment based on text content (entries with a dollar sign)
  endowment <- university_page %>%
    html_nodes("table.infobox th:contains('Endowment') + td") %>%
    html_text() %>%
    unique() %>%
    clean_text()
  

  # Check if 'Endowment' entry exists, if not, extract 'Budget'
  if (length(endowment) == 0) {
    budget <- university_page %>%
      html_nodes("table.infobox th:contains('Budget') + td") %>%
      html_text() %>%
      unique() %>%
      clean_text()

    endowment_data <- append(endowment_data, list(budget))
  } else {
    endowment_data <- append(endowment_data, list(endowment))
  }
}

# Add a new column 'Endowment' with the scraped data
empty_tibble$Endowment <- endowment_data
empty_tibble <- empty_tibble %>%
  mutate(Endowment = map_chr(Endowment, ~ .[1]))


```


**Note**: For some unis, there is no data on the endowment that is available on the wikipedia website. More precisely, the [Airforce Institute of Technology](https://en.wikipedia.org//wiki/Air_Force_Institute_of_Technology), [Azusa Pacific University](https://en.wikipedia.org/wiki/Azusa_Pacific_University), [Long Island University](https://en.wikipedia.org//wiki/LIU_Post	), and the [University of Missouri–St. Louis](https://en.wikipedia.org//wiki/University_of_Missouri%E2%80%93St._Louis	) do not have information on the website on endowment/budget.



#### Total number of students 

On some websites, the total student body is not declared in a comprehensive way because it is split up by the different campusses of the university. Therefore, it is easier to scrape the data by adding the number of postgraduates and undergraduates than to write a function that can add all of the student bodies from different campusses of the individual universities. To ensure that also universities that only have postgraduates or undergraduates, and universities which only have a number for the total student body on their website, the code is modified. Firstly, an if-statement will ensure that for those instances with NA's for either variable postgraduates or undergraduates, the number from the variable without the NA is declared as the total student body. Secondly, for the instances that only include data on the total student body on the website, another webscraping is formulated that scrapes the "Students" data from the Wikipedia website and then adds this to the variable in our scraped data set.


```{r}

# Exercise 2b


### Students

undergraduates_data <- list()
postgraduates_data <- list()

for (hyperlink in hyperlinks_list) {
  university_page <- read_html(hyperlink)
  
  # Undergraduates 
  # Extract the number of undergraduate students
  undergraduates <- university_page %>%
    html_nodes("table.infobox th:contains('Undergraduates') + td") %>%
    html_text() %>%
    unique()

  # Extract only the numbers using regular expressions
  cleaned_undergraduates <- str_extract(undergraduates, "[\\d,]+")
  
  # Assign NA if cleaned_undergraduates is empty
  cleaned_undergraduates <- ifelse(length(cleaned_undergraduates) == 0, NA, cleaned_undergraduates)
  
  undergraduates_data <- append(undergraduates_data, list(cleaned_undergraduates))

  
  # Postgraduates 
  # Extract the number of postgraduate students
  postgraduates <- university_page %>%
    html_nodes("table.infobox th:contains('Postgraduates') + td") %>%
    html_text() %>%
    unique()

  # Extract only the numbers using regular expressions
  cleaned_postgraduates <- str_extract(postgraduates, "[\\d,]+")
  
  # Assign NA if cleaned_undergraduates is empty
  cleaned_postgraduates <- ifelse(length(cleaned_postgraduates) == 0, NA, cleaned_postgraduates)
  
  postgraduates_data <- append(postgraduates_data, list(cleaned_postgraduates))

}

# Remove commas from the extracted data and convert to numeric
empty_tibble$Undergraduates <- as.numeric(gsub(",", "", unlist(undergraduates_data)))

empty_tibble$Postgraduates <- as.numeric(gsub(",", "", unlist(postgraduates_data)))

# Create a new column 'Students' as the sum of Undergraduates and Postgraduates
empty_tibble$Students <- ifelse(is.na(empty_tibble$Undergraduates), 
                                empty_tibble$Postgraduates, 
                                ifelse(is.na(empty_tibble$Postgraduates), 
                                       empty_tibble$Undergraduates, 
                                       empty_tibble$Undergraduates + empty_tibble$Postgraduates))

empty_tibble
```



If both undergraduates and graduates are missing on the respective website, we will scrape "Students" from the website.

```{r}

students_data <- list()

for (hyperlink in hyperlinks_list) {
  university_page <- read_html(hyperlink)
   
  # Extract the students
  students <- university_page %>%
    html_nodes("table.infobox th:contains('Students') + td") %>%
    html_text() %>%
    unique()

  # Extract only the numbers using regular expressions
  cleaned_students <- as.numeric(gsub(",", "", str_extract(students, "[\\d,]+")))
  
  students_data <- append(students_data, list(cleaned_students))
}

# Check if any element in 'Students' is NA before assigning
if (any(is.na(empty_tibble$Students))) {
  empty_tibble$Students[is.na(empty_tibble$Students)] <- unlist(students_data)
}
```


```{r}

# Remove columns named 'Undergraduates' and 'Postgraduates'
empty_tibble <- select(empty_tibble, -Undergraduates, -Postgraduates)
```


### c. Data munging

Download from the course website the `ivyleague.csv` file and store it appropriately on your local machine. We are going to use this file to focus our attention only on the subset of U.S. universities that are known as the "Ivy League."^[We are only narrowing our focus so that we do not do too much scraping for no real research purpose, not because the Ivy League is particularly important or interesting.] This file includes a **shortened** version of each university's name, the County and State in which the university's main campus is located (we will use these variables later), and the university's Employer Identification Number (EIN -- we will use this variable later). Call this file into `R` and create three new variables in your main table: 

ix. An indicator for whether the university is an Ivy League institution
x. The university's county (it would be wise to concatenate both county and state into a single string, separated by ",")
xi. The university's EIN (which can be missing for those universities not in the Ivy League)

#### Read in the data

```{r read in ivyleague.csv}

# Read the CSV file into a data frame
ivyleagues <- read.csv("ivyleague.csv")

# Note that I saved the file "ivyleague.csv" in the same folder as this RMarkdown.
# If the file is located in a different folder, "ivyleague.csv" needs to be modified by including the correct path before reading in the dataa using read.csv

# Display the first few rows of the ivyleagues data
head(ivyleagues)
```

The ivyleagues data contains four variables, namely a **shortened** version of each university's name ("uni_name"), the County ("county") and State ("state") in which the university's main campus is located, and the university's Employer Identification Number ("ein"). There are six observations in the dataset.

#### Ivy League Indicator, County and EIN

Since the names in the ivyleagues data are shortened, writing a function that creates another variable indicating wether the observation in our tibble is an ivy league does not work properly. For instance, if we would check whether the shortened name (in the variable "uni_name") in the ivyleagues data appears in the long name (in the variable "Institution") in our universities data frame, also universities that have similar names but are not ivy leagues will be falsely indicated as ivys. For example, not only the ivy league university "Columbia University" but also the "Teachers College at Columbia University" will be indicated as an ivy. To avoid false indications of ivy leagues, we will add another variable to the ivyleages data which includes the full name of the universities.

```{r add the full names to the universities in ivyleagues}

# Update the full_name column based on conditions
ivyleagues <- ivyleagues %>%
  mutate(
    full_name = case_when(
      uni_name == "Penn" ~ "University of Pennsylvania",
      uni_name == "Brown" ~ "Brown University",
      uni_name == "Columbia" ~ "Columbia University",
      uni_name == "Cornell" ~ "Cornell University",
      uni_name == "Dartmouth" ~ "Dartmouth College",
      uni_name == "Harvard" ~ "Harvard University",
      uni_name == "Princeton" ~ "Princeton University",
      uni_name == "Yale" ~ "Yale University",
    )
  )
```


```{r}

ivyleagues <- ivyleagues %>%
  mutate(County = ifelse(!is.na(county), paste(county, state, sep = ", "), state))

empty_tibble <- empty_tibble %>%
  left_join(ivyleagues %>% select(full_name, ein, County), by = c("Institution" = "full_name")) %>%
  mutate(Ivy_League = ifelse(Institution %in% ivyleagues$full_name, 1, 0))


empty_tibble <- empty_tibble %>% rename(EIN = ein)

# Filter for the entries that are ivy leagues to check whether the function properly assigned the ivy leagues
empty_tibble %>%
  filter(Ivy_League == 1)
```


### d. Writing to your relational database
Once you have combined all of the above data into a single table, ensure that it is in a tidy format where each row is a unique university, and each column is a variable (of which there should be exactly 11). Write this table to your relational database, making sure you give it an **appropriate** and **clear** name. Remember to ensure you have a **primary key** (e.g. the university name) that uniquely identifies each unit (university) in your table. 

Create a function to check for the existence and correct dimensionality of your written table. The function should take two arguments: the name of your database, and the name of your table. If the table exists, the function should report as output the number of rows in the table, the number of columns in the table, and the names of the columns. **For all of Exercise 2** it is sufficient for you to include just the code chunk that defines the function and the output of the function in the main section of your submitted `.html` file.

#### Check if the data is tidy
Firstly, we print out the first few lines of the data frame to ensure it is in a tidy format.

```{r print the first few rows of the dataframe}
print(head(empty_tibble))
```

We can see that each row represents a university. To ensure that each row is representative of a unique observation, we use unique(). If the length of the unique values of the Institution variable in our tibble is equal to the length of the variable without using unique(), we can confirm that each row has a different value for “Institution” and thus does represent a unique observation.


```{r check if each row holds one unique observation}

# Check if the rows in the dataframe each hold one unique university 
if (length(unique(empty_tibble$Institution)) == length(empty_tibble$Institution)) {
  answer <- sprintf("There are %d unique universities in the dataframe 'universities' that are each represented by exactly one row. The data is tidy.", length(unique(empty_tibble$Institution)))
  print(answer)
} else {
  print("Not every row holds the data of a unique university. The data is not tidy.")
}
```

To check whether the columns correspond to the 11 variables that we are interested in, we use summary().

```{r check if each variable is represented by one column}

# Check if each column corresponds to one of the 11 variables
summary(empty_tibble)
```

As we can see, the data is tidy. Each row represents a unique university and each column holds one variables. In total, we have 279 observations and 11 variables.

#### Write the table to the relational database

We write the table to the relational database "db". using a simple query that selects the first eight rows of the data, we check if the addition to the database properly worked. We should receive eight rows of data that each hold values for 12 variables. As the unique key, we will use the name of the universities, which can be found under "Institution".

```{r write the table to the relational database}

# Adding the table to the relational database
# Only include overwrite = TRUE for practicing purpose
dbWriteTable(db, "universities", empty_tibble, overwrite = TRUE)

# Testing that it worked with a simple query
dbGetQuery(db, "SELECT * FROM universities LIMIT 8")
```


#### Function to check the existence and dimensionality

Create a function to check for the existence and correct dimensionality of your written table. The function should take two arguments: the name of your database, and the name of your table. If the table exists, the function should report as output the number of rows in the table, the number of columns in the table, and the names of the columns. **For all of Exercise 2** it is sufficient for you to include just the code chunk that defines the function and the output of the function in the main section of your submitted `.html` file.

```{r write a function that checks the existence and correct dimensionality of the table}
check_table <- function(db, a_table){
  
  # Check if the "universities" table exists
  if (a_table %in% dbListTables(db)) {
    # Execute an SQL query to get the row count
    query <- paste("SELECT COUNT(*) FROM", a_table)
    row_count <- dbGetQuery(db, query)[1, 1]
  
    # Get the column names
    column_names <- list(dbListFields(db, a_table))
  
    # Get the column count
    column_count <- length(unlist(column_names))
  
    # Format the string
    formatted_string <- sprintf("The table exists and has the following dimensions: \nNumber of rows: %s \nNumber of columns: %s \nColumn names: %s", row_count, column_count, paste(column_names, collapse = ", "))
  
    # Return the row and column counts
    return(cat(formatted_string))
    
  } else {
    return(cat("The table does not exist."))
  }
}

check_table(db, "universities")
```



## Exercise 3

We are now going to use the `Rselenium` package to explore the [Academic Ranking of World Universities](https://www.shanghairanking.com/).^[Are university rankings really meaningful? Probably not, but we will explore them for this exercise anyway.]

### a. Scraping annual rank

Create a webscraper that returns, for the **Ivy League university only**:

i. The ARWU ranking for the university for the years 2003, 2013, and 2023. If the university's rank is given as a range e.g. 76-100, convert this to the midpoint of the range -- in this case 88.

Your final table should be in tidy long format, where each row uniquely identifies a combination of university and year (e.g., Harvard-2003). Write the data as a new table -- appropriately and clearly named -- to your relational database. Check for the existence and correct dimensionality of your written table using the function you wrote in Exercise 2.d. Include only the call to the function and the output in the main section of your `.html` file.

#### Launch the browser

```{r launch the browser, echo=FALSE}

# Launch the driver and browser
rD <- rsDriver(browser=c("firefox"), port = free_port(random = TRUE), chromever = NULL) 
driver <- rD$client
```

#### Navigate to the shanghai ranking website

```{r navigate to the website}

# Navigate to the website
url <- "https://www.shanghairanking.com/"
driver$navigate(url)

# Navigate to the 2023 ranking 
academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[2]/div[1]/button')
academic_ranking_23$clickElement()
```

#### Preparation for the scraping

1. Function that searches for each ivy league university in the dataframe

```{r function that searches for the ivys}

# A function that searches for each ivy league university 
search_for <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[1]/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(1)
  search_field$sendKeysToElement(list(key = "enter"))
  
}
```


2. Function that navigates to a different year

```{r Create a function that navigates to a different year }

# A function that navigates to a different year
navigate_year <- function() {
  # Find the element using the provided XPath
  year_selector <- driver$findElement(using = "xpath", value =  '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[1]/img')
  
  # Click on the element
  year_selector$clickElement()
}
```



3. Function that extracts the rank of the ivy leagues given what university we have searched for using search_for() and what year we are currently looking at after having used navigate_year().

```{r functions to extract the world rank and the year}

extract_rank <- function() {
  # A function that extracts the rank from the website
  
  # Find the element on the website and transform it to text directly
  current_university_rank <- driver$findElement(using = "xpath",
                                                value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/table/tbody/tr/td[1]/div')$getElementText()[[1]]
  
  return(current_university_rank)
}

extract_year <- function() {
  # A function that extracts the year we are currently searching for and returns it
  current_year <- driver$findElement(using = "xpath",
                                     value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[1]')$getElementText()[[1]]
  
  return(current_year)
}
```


4. Data frames for each year we are interested in

We will temporarily create three data frame that hold the names of the ivy leagues in their first column. Furthermore, these dataframes have an empty column for the rank and another empty column that will later hold the year. Before scraping the data from the website, the three dataframes look identical. Below, the dataframe for the year 2003 can be seen.

```{r create a dataframe with ivy leagues}

# Create a dataframe that holds all ivy league universities as displayed in the ivyleagues data
ivyleagues_df <- data.frame(
  Institution = ivyleagues$full_name,
  # Add an empty column for the year 
  Year = numeric(length(ivyleagues$full_name)),
  # Add an empty column for the Rank
  Rank = numeric(length(ivyleagues$full_name)),
  stringsAsFactors = FALSE
)

# Create three copies of the ivyleagues_df data frame for each of the three years that we will later scrape ranking data fro
ivyleagues_03 <- ivyleagues_df
ivyleagues_13 <- ivyleagues_df
ivyleagues_23 <- ivyleagues_df

# Display the ivy league dataframes for the three years
print(ivyleagues_03)
```


#### Extract the rank

```{r extract the world rank of each ivy league}

# Scrape the rank for each ivy league university from the website for the years 2003, 2013, 2023


##########
## 2023 ##
##########

# Navigate to 2023
navigate_year()

academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[1]')
academic_ranking_23$clickElement()

# Loop through all ivy league universities in the ivyleagues dataset and call the search_for function for each university
for (i in seq_along(ivyleagues_23$Institution)) {
  
  # Search for the university
  search_for(ivyleagues_23$Institution[i])
  
  # Add a delay to allow the page to load before the next search
  Sys.sleep(2)  
  
  # Extract the rank from the webpage and assign it to the Rank column
  ivyleagues_23$Rank[i] <- extract_rank()
  
  # Assign the year 2023 to the Year column
  #ivyleagues_23$Year[i] <- extract_year()
  ivyleagues_23$Year[i] <- "2023"

}

##########
## 2013 ##
##########

# Navigate to 2013
navigate_year()

academic_ranking_13 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[11]')
academic_ranking_13$clickElement()

for (i in seq_along(ivyleagues_13$Institution)) {
  
  # Search for the university
  search_for(ivyleagues_13$Institution[i])
  
  # Add a delay to allow the page to load before the next search
  Sys.sleep(2)  
  
  # Extract the rank from the webpage and assign it to the Rank column
  ivyleagues_13$Rank[i] <- extract_rank()
  
  # Assign the year 2013 to the Year column
  ivyleagues_13$Year[i] <- "2013"

}

##########
## 2003 ##
##########

# Navigate to 2003
navigate_year()

academic_ranking_03 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[21]')
academic_ranking_03$clickElement()

for (i in seq_along(ivyleagues_03$Institution)) {
  
  # Search for the university
  search_for(ivyleagues_03$Institution[i])
  
  # Add a delay to allow the page to load before the next search
  Sys.sleep(2)  
  
  # Extract the rank from the webpage and assign it to the Rank column
  ivyleagues_03$Rank[i] <- extract_rank()
  
  # Assign the year 2003 to the Year column
  ivyleagues_03$Year[i] <- "2003"

}
```

```{r print the tables}

print(ivyleagues_23)
print(ivyleagues_03)
print(ivyleagues_13)
```






```{r merge the institution columns with the year columns}

ivyleagues_03$Institution <- paste(ivyleagues_03$Institution, ivyleagues_03$Year, sep = "-")
ivyleagues_03$Year <- NULL

ivyleagues_13$Institution <- paste(ivyleagues_13$Institution, ivyleagues_13$Year, sep = "-")
ivyleagues_13$Year <- NULL

ivyleagues_23$Institution <- paste(ivyleagues_23$Institution, ivyleagues_23$Year, sep = "-")
ivyleagues_23$Year <- NULL


# Merge all three data sets

ivyleagues_ranks <- rbind(ivyleagues_03, ivyleagues_13, ivyleagues_23)

# Function to calculate midpoint from a range

calculate_midpoint <- function(rank) {
  for (i in seq_along(rank)) {
  range_midpoint <- mean(as.numeric(strsplit(rank[i], "-")[[1]])) 
  return(range_midpoint)}
}

ivyleagues_ranks$Rank <- sapply(ivyleagues_ranks$Rank, calculate_midpoint)
```


#### Write it to the database

```{r write the ivyleagues_ranks table to the relational database}

# Adding the table to the relational database
# Only include overwrite = TRUE for practicing purpose
dbWriteTable(db, "ivyleagues_ranks", ivyleagues_ranks, overwrite = TRUE)

# Testing that it worked with a simple query
dbGetQuery(db, "SELECT * FROM ivyleagues_ranks LIMIT 8")

```



#### Check the existence of the table

```{r}
check_table(db, "ivyleagues_ranks")
```


### b. Scraping subject ranks for 2023

Extend your webscraper (or create a new one) that gathers for each **Ivy League university only**:

i. The rankings of the university for **every social science** for which the university has been ranked. Again, if a range is given, take the midpoint.

Your final table should be in tidy long format, where each row uniquely identifies a combination of university and discipline (e.g., Harvard-Economics). Write the data as a new table -- appropriately and clearly named -- to your relational database. Check for the existence and correct dimensionality of your written table using the function you wrote in Exercise 2.d. Include only the call to the function and the output in the main section of your `.html` file.






## Close the stuff again




Finally, let us close the driver and browser window before closing R:

```{r}
# close the RSelenium processes:
driver$close()
rD$server$stop()

# close the associated Java processes (if using Mac or Linux this may not be necessary -- Google for correct command)
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
```


```{r}
# Close the database connection
dbDisconnect(db)
```

## Data



## Sources



## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 


```
