---
title: "Assignment 3"
author: "Student ID: 201903536"
date: "30 November 2023"
output: html_document
---

```{r setup, include=FALSE} 
#####################################
# SETUP
#####################################

knitr::opts_chunk$set(echo = FALSE) 

#####################################
# Install/load packages
#####################################

library(rvest)
library(tidyverse)
library(purrr)
library(DBI)
library(RSQLite)
library(RSelenium)
library(netstat)
```


## GitHub

The GitHub repository for **this assignment** can be found [here](https://github.com/emmi3105/472_assignment3).


## Exercise 1

This assignment will take us through a workflow that a data scientist might encounter in the real world, from data collection right through to analysis. Throughout this assignment we are going to use a local relational database to store a variety of different but related tables that we collect and may want to combine in various ways. We will want to ensure that each table within our database can be joined to every other table using a `primary key`. 

We will start by creating an empty local relational database. You should store this new database in a 'database' folder that you create within your assignment folder. Follow these steps:

1. Use the `DBI::dbConnect()` function in `R` to create a new `SQLite` database (either `YOUR_DB_NAME.sqlite` or `YOUR_DB_NAME.db`) in your database folder.
2. Use the `file.exists()` function in `R` to check for the existence of your relational database.  

Include in the main text of your `.html` submission the code that created the database **and** the code that checks for its existence **and** the output of that check.


### Create a local relational database

```{r create database, echo=TRUE}

# Create database
db <- dbConnect(RSQLite::SQLite(), "universities_db.sqlite")
```

### Check for the existence of the database

We will use file.exist() to check for the existence of the relational database. If that returns TRUE, the database exists.


```{r check existence of database, echo=TRUE}

# Check existence of the database
print(file.exists("universities_db.sqlite"))
```



## Exercise 2

### a. Gathering structured data 

**Task**
Write an automatic webscraping function in R that constructs a table (as e.g. a data frame, a tibble, or a data table) of all R1 (Very High Research Activity) **and** R2 (High Research Activity) Research Universities in the United States of America. These data can be found on [wikipedia](https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States). 

**Note**: The solution to Exercise 2a can be found under the solution for Exercise 2b.

### b. Gathering unstructured data

**Task**
Extend your webscraping function (or create a new function) so that it navigates to the dedicated Wikipedia page for each university, and captures three additional variables.


#### Automatic webscraping function for 2a and b

I combined exercise 2a and 2b and directly wrote a webscraping function called scrape_wikipedia_tables that returns a dataframe with the following variables: 

i. The university’s name
ii. It’s status (public or private)
iii. The city in which it is located
iv. The state in which it is located
v. The URL of the university’s dedicated Wikipedia page
vi. The geographic coordinates of the (main) university campus
vii. The endowment of the university in USD dollars
viii. The total number of students (including both undergraduate and postgraduate)

The first few rows of the resulting table "universities_table" are printed out below. Furthermore, in the following two sections, the function is explained in more detail.

```{r wikipedia web scraping function, echo=TRUE}

scrape_wikipedia_tables <- function(url) {
  # Input: URL to wikipedia
  # Function accesses wikipedia articles on universities and scrapes information from the wikipedia pages.
  # Output: A table with information on US American R1 and R2 universities. More precisely, the table holds the university's name, status (private or public), city, state, the url to the university's Wikipedia page, the university's geographic coordinates, endowment, and number of total students.
  
  ### Exercise 2a: scrape information on the name, status, city, state and the url to the wikipedia page
  table_indices <- c(19, 27)
  
  # Storing the URL's HTML code
  html_content <- read_html(url)
  
  # Extracting all tables in the document 
  tab <- html_table(html_content, fill = TRUE)

  # Extract only the tables of interest given the input table_indices
  selected_tables <- tab[1:length(table_indices)]

  # Iterate over tables and bind rows
  empty_tibble <- tibble()
  for (i in 1:length(selected_tables)) {
    current_table <- as_tibble(selected_tables[[i]][, 1:4])
    empty_tibble <- bind_rows(empty_tibble, current_table)
  }

  # Initialize an empty list to store hyperlinks
  hyperlinks_list <- list()

  # Iterate over the table indices and extract hyperlinks
  for (index in table_indices) {
    table_nodes <- html_content %>%
      html_nodes(paste("#mw-content-text > div.mw-content-ltr.mw-parser-output > table:nth-child(", index, ")", sep = ""))
  
    current_node_set <- table_nodes[[1]]
  
    hyperlinks_current_node <- current_node_set %>%
      html_nodes("td:nth-child(1) a") %>%
      html_attr("href")
  
    # Append the hyperlinks to the list
    hyperlinks_list <- c(hyperlinks_list, hyperlinks_current_node)
  }

  # Unlist to flatten the list
  hyperlinks_list <- unlist(hyperlinks_list, recursive = FALSE)

  # Concatenate the base URL to the extracted hyperlinks
  hyperlinks_list <- paste0("https://en.wikipedia.org/", hyperlinks_list)
  
  # Add a new column 'Wikipedia_URL' with the extracted URLs
  empty_tibble$Wikipedia_URL <- hyperlinks_list

    
  ### Exercise 2b: Add the geographic location, endowment and students
  
  #### Geographic location

  university_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the geographic location using the class selector 'geo_default'
    geographic_location <- university_page %>%
      html_nodes(".geo-dec") %>%
      html_text() %>%
      unique()
 
    # Store the scraped information in a list
    university_data <- append(university_data, list(geographic_location))
  
  }

  # Add a new column 'Geographic_Location' with the scraped data
  empty_tibble$Geographic_Location <- university_data

  # Keep the first set of coordinates only
  empty_tibble <- empty_tibble %>%
    mutate(Geographic_Location = map_chr(Geographic_Location, ~ .[1]))
  
  #### Endowment and budget

  endowment_data <- list()

  # Clean text by removing text within brackets
  clean_text <- function(text) {
    # Remove text containing brackets within brackets
    text <- gsub("\\([^()]*\\)", "", text)
    # Remove text within square brackets
    text <- gsub("\\[.*?\\]", "", text)
    # Remove text within round brackets
    text <- gsub("\\(.*?\\)", "", text)
    return(text)
  }

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the endowment based on text content (entries with a dollar sign)
    endowment <- university_page %>%
      html_nodes("table.infobox th:contains('Endowment') + td") %>%
      html_text() %>%
      unique() %>%
      clean_text()
  

    # Check if 'Endowment' entry exists, if not, extract 'Budget'
    if (length(endowment) == 0) {
      budget <- university_page %>%
        html_nodes("table.infobox th:contains('Budget') + td") %>%
        html_text() %>%
        unique() %>%
        clean_text()

      endowment_data <- append(endowment_data, list(budget))
    } else {
      endowment_data <- append(endowment_data, list(endowment))
    }
  }

  # Add a new column 'Endowment' with the scraped data
  empty_tibble$Endowment <- endowment_data
  empty_tibble <- empty_tibble %>%
    mutate(Endowment = map_chr(Endowment, ~ .[1]))


  #### Students
  
  # To avoid issues with students being spread on different campuses, scrape total student information by adding information on all undergraduates to all postgraduates
  
  undergraduates_data <- list()
  postgraduates_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
  
    # Undergraduates 
    # Extract the number of undergraduate students
    undergraduates <- university_page %>%
      html_nodes("table.infobox th:contains('Undergraduates') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_undergraduates <- str_extract(undergraduates, "[\\d,]+")
  
    # Assign NA if cleaned_undergraduates is empty
    cleaned_undergraduates <- ifelse(length(cleaned_undergraduates) == 0, NA, cleaned_undergraduates)
    undergraduates_data <- append(undergraduates_data, list(cleaned_undergraduates))

    # Postgraduates 
    # Extract the number of postgraduate students
    postgraduates <- university_page %>%
      html_nodes("table.infobox th:contains('Postgraduates') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_postgraduates <- str_extract(postgraduates, "[\\d,]+")
  
    # Assign NA if cleaned_postgraduates is empty
    cleaned_postgraduates <- ifelse(length(cleaned_postgraduates) == 0, NA, cleaned_postgraduates)
    postgraduates_data <- append(postgraduates_data, list(cleaned_postgraduates))
  }

  # Remove commas from the extracted data and convert to numeric
  empty_tibble$Undergraduates <- as.numeric(gsub(",", "", unlist(undergraduates_data)))
  empty_tibble$Postgraduates <- as.numeric(gsub(",", "", unlist(postgraduates_data)))

  # Create a new column 'Students' as the sum of Undergraduates and Postgraduates
  empty_tibble$Students <- ifelse(is.na(empty_tibble$Undergraduates), 
                                  empty_tibble$Postgraduates, 
                                  ifelse(is.na(empty_tibble$Postgraduates), 
                                        empty_tibble$Undergraduates, 
                                        empty_tibble$Undergraduates + empty_tibble$Postgraduates))

  # If the website neither holds information on undergraduates nor on postgraduates, look for information on total students

  students_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the students
    students <- university_page %>%
      html_nodes("table.infobox th:contains('Students') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_students <- as.numeric(gsub(",", "", str_extract(students, "[\\d,]+")))
    students_data <- append(students_data, list(cleaned_students))
  }

  # Check if any element in 'Students' is NA before assigning
  if (any(is.na(empty_tibble$Students))) {
    empty_tibble$Students[is.na(empty_tibble$Students)] <- unlist(students_data)
  }

  # Remove columns named 'Undergraduates' and 'Postgraduates'
  empty_tibble <- select(empty_tibble, -Undergraduates, -Postgraduates)
  
  
  # Finally, return the table including the scraped information
  return(empty_tibble)
}

# Run the function using the URL from the Wikipedia page
url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"

universities_table <- scrape_wikipedia_tables(url)
head(universities_table)
```

#### Notes on the function

**Geographic Location**: Some universities have several sets of coordinates on their wikipedia sites. For instance, there are universities with multiple campuses whose geographic locations are all noted down on wikipedia. Therefore, the function above includes a line of code that specifies that only the very first set of coordinates is appended to the dataframe. Thereby, the geographic location column holds simplified data that potentially might not reflect the location of the entire university.

**Endowment**: Not all universities we are interested in include information on the "endowment" on their wikipedia website. However, some university wikipedia articles include data on the university's "budget". Since endowment and budget likely refer to the same thing, I have included an if-statement in the function above, that specifies the following: If a university's wikipedia article does not hold information on the endowment, the function will look whether there is information on the budget and use this data instead. Nonetheless, there are a few universities that have neither the endowment nor the budget declared on wikipedia. For those instances, the endowment variable in our table will hold a missing values as specified in the following section.

**Students**: On some websites, the total student body is not declared in a comprehensive way because it is split up by the different campusses of the university. Therefore, it is easier to scrape the data by adding the number of postgraduates and undergraduates than to write a function that can add all of the student bodies from different campusses of the individual universities. To ensure that also universities that only have postgraduates or undergraduates, and universities which only have a number for the total student body on their website, the code is modified. Firstly, an if-statement will ensure that for those instances with NA's for either variable postgraduates or undergraduates, the number from the variable without the NA is declared as the total student body. Secondly, for the instances that only include data on the total student body on the website, another webscraping is formulated that scrapes the "Students" data from the Wikipedia website and then adds this to the variable in our scraped data set.

#### Notes on missing values

**Geographic Location**: There are missing values in the geographic location column. To be exact, the [University of Colorado Denver](https://en.wikipedia.org//wiki/University_of_Colorado_Denver) and the [University of Mississippi](https://en.wikipedia.org//wiki/University_of_Mississippi) do not have information on the geographic location on their wikipedia websites.

**Endowment**: For some universities, there are no data on the endowment that is available on the wikipedia website. More precisely, the [Airforce Institute of Technology](https://en.wikipedia.org//wiki/Air_Force_Institute_of_Technology), [Azusa Pacific University](https://en.wikipedia.org/wiki/Azusa_Pacific_University), [Long Island University](https://en.wikipedia.org//wiki/LIU_Post	), and the [University of Missouri–St. Louis](https://en.wikipedia.org//wiki/University_of_Missouri%E2%80%93St._Louis	) do not have information on the website on endowment/budget.


### c. Data munging

**Task**:
Download from the course website the `ivyleague.csv` file and store it appropriately on your local machine. Call this file into `R` and create three new variables in your main table: 

ix. An indicator for whether the university is an Ivy League institution
x. The university's county (it would be wise to concatenate both county and state into a single string, separated by ",")
xi. The university's EIN (which can be missing for those universities not in the Ivy League)

#### Read in the data

```{r read in ivyleague.csv}

# Note that the file "ivyleague.csv" muat be stored in the same folder as this RMarkdown.
# If the file is stored in a different folder, the code below must be modified by including the correct path of the "ivyleague.csv* file.

# Read the CSV file into a data frame
ivyleagues <- read.csv("ivyleague.csv")

# Display the first few rows of the ivyleagues data
head(ivyleagues)
```

The ivyleagues data contains four variables, namely a **shortened** version of each university's name ("uni_name"), the County ("county") and State ("state") in which the university's main campus is located, and the university's Employer Identification Number ("ein"). There are six observations in the dataset.

#### Ivy League Indicator, County and EIN

Since the names in the ivyleagues data are shortened, writing a function that creates another variable indicating wether the observation in our tibble is an ivy league does not work properly. For instance, if we would check whether the shortened name (in the variable "uni_name") in the ivyleagues data appears in the long name (in the variable "Institution") in our universities data frame, also universities that have similar names but are not ivy leagues will be falsely indicated as ivys. For example, not only the ivy league university "Columbia University" but also the "Teachers College at Columbia University" will be indicated as an ivy. To avoid false indications of ivy leagues, we will add another variable to the ivyleages data which includes the full name of the universities.

```{r add the full names to the universities in ivyleagues}

# Update the full_name column based on conditions
ivyleagues <- ivyleagues %>%
  mutate(
    full_name = case_when(
      uni_name == "Penn" ~ "University of Pennsylvania",
      uni_name == "Brown" ~ "Brown University",
      uni_name == "Columbia" ~ "Columbia University",
      uni_name == "Cornell" ~ "Cornell University",
      uni_name == "Dartmouth" ~ "Dartmouth College",
      uni_name == "Harvard" ~ "Harvard University",
      uni_name == "Princeton" ~ "Princeton University",
      uni_name == "Yale" ~ "Yale University",
    )
  )
```


```{r}

ivyleagues <- ivyleagues %>%
  mutate(County = ifelse(!is.na(county), paste(county, state, sep = ", "), state))

universities_table <- universities_table %>%
  left_join(ivyleagues %>% select(full_name, ein, County), by = c("Institution" = "full_name")) %>%
  mutate(Ivy_League = ifelse(Institution %in% ivyleagues$full_name, 1, 0))


universities_table <- universities_table %>% rename(EIN = ein)

# Filter for the entries that are ivy leagues to check whether the function properly assigned the ivy leagues
universities_table %>%
  filter(Ivy_League == 1)
```


### d. Writing to your relational database
Once you have combined all of the above data into a single table, ensure that it is in a tidy format where each row is a unique university, and each column is a variable (of which there should be exactly 11). Write this table to your relational database, making sure you give it an **appropriate** and **clear** name. Remember to ensure you have a **primary key** (e.g. the university name) that uniquely identifies each unit (university) in your table. 

Create a function to check for the existence and correct dimensionality of your written table. The function should take two arguments: the name of your database, and the name of your table. If the table exists, the function should report as output the number of rows in the table, the number of columns in the table, and the names of the columns. **For all of Exercise 2** it is sufficient for you to include just the code chunk that defines the function and the output of the function in the main section of your submitted `.html` file.

#### Check if the data is tidy
Firstly, we print out the first few lines of the data frame to ensure it is in a tidy format.

```{r print the first few rows of the dataframe}
print(head(universities_table))
```

We can see that each row represents a university. To ensure that each row is representative of a unique observation, we use unique(). If the length of the unique values of the Institution variable in our tibble is equal to the length of the variable without using unique(), we can confirm that each row has a different value for “Institution” and thus does represent a unique observation.


```{r check if each row holds one unique observation}

# Check if the rows in the dataframe each hold one unique university 
if (length(unique(universities_table$Institution)) == length(universities_table$Institution)) {
  answer <- sprintf("There are %d unique universities in the dataframe 'universities' that are each represented by exactly one row. The data is tidy.", length(unique(universities_table$Institution)))
  print(answer)
} else {
  print("Not every row holds the data of a unique university. The data is not tidy.")
}
```

To check whether the columns correspond to the 11 variables that we are interested in, we use summary().

```{r check if each variable is represented by one column}

# Check if each column corresponds to one of the 11 variables
summary(universities_table)
```

As we can see, the data is tidy. Each row represents a unique university and each column holds one variables. In total, we have 279 observations and 11 variables.

#### Write the table to the relational database

We write the table to the relational database "db". using a simple query that selects the first eight rows of the data, we check if the addition to the database properly worked. We should receive eight rows of data that each hold values for 12 variables. As the unique key, we will use the name of the universities, which can be found under "Institution".

```{r write the table to the relational database}

# Adding the table to the relational database
# Only include overwrite = TRUE for practicing purpose
dbWriteTable(db, "universities", universities_table, overwrite = TRUE)

# Testing that it worked with a simple query
dbGetQuery(db, "SELECT * FROM universities LIMIT 8")
```


#### Function to check the existence and dimensionality

Create a function to check for the existence and correct dimensionality of your written table. The function should take two arguments: the name of your database, and the name of your table. If the table exists, the function should report as output the number of rows in the table, the number of columns in the table, and the names of the columns. **For all of Exercise 2** it is sufficient for you to include just the code chunk that defines the function and the output of the function in the main section of your submitted `.html` file.

```{r write a function that checks the existence and correct dimensionality of the table}
check_table <- function(db, a_table){
  
  # Check if the "universities" table exists
  if (a_table %in% dbListTables(db)) {
    # Execute an SQL query to get the row count
    query <- paste("SELECT COUNT(*) FROM", a_table)
    row_count <- dbGetQuery(db, query)[1, 1]
  
    # Get the column names
    column_names <- list(dbListFields(db, a_table))
  
    # Get the column count
    column_count <- length(unlist(column_names))
  
    # Format the string
    formatted_string <- sprintf("The table exists and has the following dimensions: \nNumber of rows: %s \nNumber of columns: %s \nColumn names: %s", row_count, column_count, paste(column_names, collapse = ", "))
  
    # Return the row and column counts
    return(cat(formatted_string))
    
  } else {
    return(cat("The table does not exist."))
  }
}

check_table(db, "universities")
```



## Exercise 3

We are now going to use the `Rselenium` package to explore the [Academic Ranking of World Universities](https://www.shanghairanking.com/).^[Are university rankings really meaningful? Probably not, but we will explore them for this exercise anyway.]

### a. Scraping annual rank

Create a webscraper that returns, for the **Ivy League university only**:

i. The ARWU ranking for the university for the years 2003, 2013, and 2023. If the university's rank is given as a range e.g. 76-100, convert this to the midpoint of the range -- in this case 88.

Your final table should be in tidy long format, where each row uniquely identifies a combination of university and year (e.g., Harvard-2003). Write the data as a new table -- appropriately and clearly named -- to your relational database. Check for the existence and correct dimensionality of your written table using the function you wrote in Exercise 2.d. Include only the call to the function and the output in the main section of your `.html` file.

#### Launch the browser

```{r launch the browser, results='hide', message=FALSE}

# Launch the driver and browser
invisible(capture.output({
  rD <- rsDriver(browser=c("firefox"), port = free_port(random = TRUE), chromever = NULL) 
  driver <- rD$client
}))
```

#### Navigate to the shanghai ranking website

```{r navigate to the website}

# Navigate to the website
url <- "https://www.shanghairanking.com/"
driver$navigate(url)

# Navigate to the 2023 ranking 
academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[2]/div[1]/button')
academic_ranking_23$clickElement()
```

#### Preparation for the scraping

1. Function that searches for each ivy league university in the dataframe

```{r function that searches for the ivys}

# A function that searches for each ivy league university 
search_for <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[1]/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(1)
  search_field$sendKeysToElement(list(key = "enter"))
  
}
```


2. Function that navigates to a different year

```{r Create a function that navigates to a different year }

# A function that navigates to a different year
navigate_year <- function() {
  # Find the element using the provided XPath
  year_selector <- driver$findElement(using = "xpath", value =  '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[1]/img')
  
  # Click on the element
  year_selector$clickElement()
}
```



3. Function that extracts the rank of the ivy leagues given what university we have searched for using search_for() and what year we are currently looking at after having used navigate_year().

```{r functions to extract the world rank and the year}

extract_rank <- function() {
  # A function that extracts the rank from the website
  
  # Find the element on the website and transform it to text directly
  current_university_rank <- driver$findElement(using = "xpath",
                                                value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/table/tbody/tr/td[1]/div')$getElementText()[[1]]
  
  return(current_university_rank)
}

extract_year <- function() {
  # A function that extracts the year we are currently searching for and returns it
  current_year <- driver$findElement(using = "xpath",
                                     value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[1]')$getElementText()[[1]]
  
  return(current_year)
}
```


4. Data frames for each year we are interested in

We will temporarily create three data frame that hold the names of the ivy leagues in their first column. Furthermore, these dataframes have an empty column for the rank and another empty column that will later hold the year. Before scraping the data from the website, the three dataframes look identical. Below, the dataframe for the year 2003 can be seen.

```{r create a dataframe with ivy leagues}

# Create a dataframe that holds all ivy league universities as displayed in the ivyleagues data
ivyleagues_df <- data.frame(
  Institution = ivyleagues$full_name,
  # Add an empty column for the year 
  Year = numeric(length(ivyleagues$full_name)),
  # Add an empty column for the Rank
  Rank = numeric(length(ivyleagues$full_name)),
  stringsAsFactors = FALSE
)

# Create three copies of the ivyleagues_df data frame for each of the three years that we will later scrape ranking data fro
ivyleagues_03 <- ivyleagues_df
ivyleagues_13 <- ivyleagues_df
ivyleagues_23 <- ivyleagues_df

# Display the ivy league dataframes for the three years
print(ivyleagues_03)
```


#### Extract the rank

```{r extract the world rank of each ivy league}

# Scrape the rank for each ivy league university from the website for the years 2003, 2013, 2023


##########
## 2023 ##
##########

# Navigate to 2023
navigate_year()

academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[1]')
academic_ranking_23$clickElement()

# Loop through all ivy league universities in the ivyleagues dataset and call the search_for function for each university
for (i in seq_along(ivyleagues_23$Institution)) {
  
  # Search for the university
  search_for(ivyleagues_23$Institution[i])
  
  # Add a delay to allow the page to load before the next search
  Sys.sleep(2)  
  
  # Extract the rank from the webpage and assign it to the Rank column
  ivyleagues_23$Rank[i] <- extract_rank()
  
  # Assign the year 2023 to the Year column
  #ivyleagues_23$Year[i] <- extract_year()
  ivyleagues_23$Year[i] <- "2023"

}

##########
## 2013 ##
##########

# Navigate to 2013
navigate_year()

academic_ranking_13 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[11]')
academic_ranking_13$clickElement()

for (i in seq_along(ivyleagues_13$Institution)) {
  
  # Search for the university
  search_for(ivyleagues_13$Institution[i])
  
  # Add a delay to allow the page to load before the next search
  Sys.sleep(2)  
  
  # Extract the rank from the webpage and assign it to the Rank column
  ivyleagues_13$Rank[i] <- extract_rank()
  
  # Assign the year 2013 to the Year column
  ivyleagues_13$Year[i] <- "2013"

}

##########
## 2003 ##
##########

# Navigate to 2003
navigate_year()

academic_ranking_03 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[21]')
academic_ranking_03$clickElement()

for (i in seq_along(ivyleagues_03$Institution)) {
  
  # Search for the university
  search_for(ivyleagues_03$Institution[i])
  
  # Add a delay to allow the page to load before the next search
  Sys.sleep(2)  
  
  # Extract the rank from the webpage and assign it to the Rank column
  ivyleagues_03$Rank[i] <- extract_rank()
  
  # Assign the year 2003 to the Year column
  ivyleagues_03$Year[i] <- "2003"

}
```

```{r print the tables}

#print(ivyleagues_23)
#print(ivyleagues_03)
#print(ivyleagues_13)
```






```{r merge the institution columns with the year columns}


# We actually do not want this -> does not make sense with regard to relational database
#ivyleagues_03$Institution <- paste(ivyleagues_03$Institution, ivyleagues_03$Year, sep = "-")
#ivyleagues_03$Year <- NULL

#ivyleagues_13$Institution <- paste(ivyleagues_13$Institution, ivyleagues_13$Year, sep = "-")
#ivyleagues_13$Year <- NULL

#ivyleagues_23$Institution <- paste(ivyleagues_23$Institution, ivyleagues_23$Year, sep = "-")
#ivyleagues_23$Year <- NULL


# Merge all three data sets

ivyleagues_ranks <- rbind(ivyleagues_03, ivyleagues_13, ivyleagues_23)

# Function to calculate midpoint from a range

calculate_midpoint <- function(rank) {
  for (i in seq_along(rank)) {
  range_midpoint <- mean(as.numeric(strsplit(rank[i], "-")[[1]])) 
  return(range_midpoint)}
}

ivyleagues_ranks$Rank <- sapply(ivyleagues_ranks$Rank, calculate_midpoint)
```


#### Write it to the database

```{r write the ivyleagues_ranks table to the relational database}

# Adding the table to the relational database
# Only include overwrite = TRUE for practicing purpose
dbWriteTable(db, "ivyleagues_ranks", ivyleagues_ranks, overwrite = TRUE)

# Testing that it worked with a simple query
dbGetQuery(db, "SELECT * FROM ivyleagues_ranks LIMIT 8")

```



#### Check the existence of the table

```{r}
check_table(db, "ivyleagues_ranks")
```
#### Navigate back to the home page

```{r navigate to the home page}

# Navigate to the home page to make it easier to keep working on this website
home_page <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[1]/a')
  
# Click on the element
home_page$clickElement()
```


### b. Scraping subject ranks for 2023

Extend your webscraper (or create a new one) that gathers for each **Ivy League university only**:

i. The rankings of the university for **every social science** for which the university has been ranked. Again, if a range is given, take the midpoint.

Your final table should be in tidy long format, where each row uniquely identifies a combination of university and discipline (e.g., Harvard-Economics). Write the data as a new table -- appropriately and clearly named -- to your relational database. Check for the existence and correct dimensionality of your written table using the function you wrote in Exercise 2.d. Include only the call to the function and the output in the main section of your `.html` file.




#### Preparation for the scraping

0. Navigate to page with the data on the university ranking by subject

```{r function that navigates to the subject rankings}

# A function that navigates to the page with the subject rankings
navigate_universities <- function() {
  universities_ranks <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[3]')
  universities_ranks$clickElement()
}

navigate_universities()
```


1. Function that searches for each ivy league university in the dataframe and selects the first result

```{r function that searches for the ivys in the general ranking and sekects it}

# A function that searches for each ivy league university 
search_general_ranking <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[1]/div/div/div/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(2)
  search_field$sendKeysToElement(list(key = "enter"))
  
  # Select the first element
  Sys.sleep(2)
  first_result <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[1]/div[2]') 
  first_result$clickElement()

}

# Test
#navigate_universities() 
#Sys.sleep(2)
#search_general_ranking("University of Pennsylvania")
#navigate_universities() 
#Sys.sleep(2)
#search_general_ranking("Yale University")
#navigate_universities() 
#Sys.sleep(2)
#search_general_ranking("Princeton University")

```


2. Function that filters for the social sciences
```{r filter for social sciences}

# A function that filters for the social sciences in the ranking options (with js)
filter_social_sciences <- function() {
  
  # Click the filter field
  filter_subject <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div/div[2]/div[2]/div[2]/div[1]/div[1]/div[2]/div/div[1]/img')
  filter_subject$clickElement()

  
  partial_inner_html <- "Social Sciences"

  # Construct an XPath that searches for an element containing the specified inner HTML
  xpath_expression <- sprintf('//*/text()[contains(., "%s")]/parent::*', partial_inner_html)

  # Find the element using the constructed XPath
  social_sciences_element <- driver$findElement(using = "xpath", value = xpath_expression)

  # Click on the found element
  social_sciences_element$clickElement()
  
}

#filter_social_sciences()
```

3. Scrape the ranks

```{r}
social_science_ranks <- data.frame(Institution = character(),
                          Subject = character(),
                          Number = character(),
                          stringsAsFactors = FALSE)

extract_social_science_rank <- function(institution_name, existing_table) {
  
  # entire table with ranks
  rank_table <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div/div[2]/div[2]/div[2]/div[1]/div[2]/table/tbody')$getElementText()[[1]]
  
  # Split the data into subjects and numbers
  split_data <- unlist(strsplit(rank_table, "\n"))

  # Add the scraped data to the existing table
  existing_table <- rbind(existing_table,
                          data.frame(Institution = rep(institution_name, length(split_data) / 2),
                                     Subject = split_data[seq(1, length(split_data), by = 2)],
                                     Number = split_data[seq(2, length(split_data), by = 2)],
                                     stringsAsFactors = FALSE))
  
  return(existing_table)
}

#social_science_ranks <- extract_social_science_rank("Princeton University", social_science_ranks)
#social_science_ranks <- extract_social_science_rank("University of Pennsylvania", social_science_ranks)

```


#### Scrape

```{r scrape social science rankings}

# Potentially need another code here depending on where we end up when we run the entire markdown

navigate_universities()
Sys.sleep(2)  

# University of Pennsylvania
# Search for the university
search_general_ranking("University of Pennsylvania")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("University of Pennsylvania", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Brown University
# Search for the university
search_general_ranking("Brown University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Brown University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Columbia University
search_general_ranking("Columbia University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Columbia University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 



# Cornell University
search_general_ranking("Cornell University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Cornell University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 



# Dartmouth College
search_general_ranking("Dartmouth College")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Dartmouth College", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)



# Harvard University
search_general_ranking("Harvard University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Harvard University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)



#	Princeton University
search_general_ranking("Princeton University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Princeton University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)



#	Yale University
search_general_ranking("Yale University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Yale University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)









######################################################################
## Potentially: with a loop?
# Loop through all ivy league universities in the ivyleagues dataset and call the search_for function for each university

#navigate_universities()
#Sys.sleep(2) 

#for (i in seq_along(ivyleagues$full_name)) {
  # Search for the university
#  search_general_ranking(ivyleagues$full_name[i])
  
  # Add a delay to allow the page to load before the next search
#  Sys.sleep(2)  
  
  # Filter for the social sciences
#  filter_social_sciences()
  
  # Extract the social science ranks
#  social_science_ranks <- extract_social_science_rank(ivyleagues$full_name[i], social_science_ranks) 
#  Sys.sleep(2)  

  # Navigate back to all universities
#  navigate_universities()
#  Sys.sleep(2)  
#}


```



#### Calculate the midpoint for the rank

```{r}
social_science_ranks$Number <- sapply(social_science_ranks$Number, calculate_midpoint)
```


#### Write the table to the database

```{r write the social_science_ranks table to the relational database}

# Adding the table to the relational database
# Only include overwrite = TRUE for practicing purpose
dbWriteTable(db, "social_science_ranks", social_science_ranks, overwrite = TRUE)

# Testing that it worked with a simple query
dbGetQuery(db, "SELECT * FROM social_science_ranks LIMIT 20")

```

#### Check the existence of the table

```{r}
check_table(db, "social_science_ranks")
```

#### Navigate back to the homepage

```{r navigate to home page}

# Navigate to the home page to make it easier to keep working on this website
home_page <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[1]/a')
  
# Click on the element
home_page$clickElement()
```











## Close the stuff again


Finally, let us close the driver and browser window before closing R:

```{r}
# close the RSelenium processes:
driver$close()
# close the associated Java processes (if using Mac or Linux this may not be necessary -- Google for correct command)
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
```


```{r}
# Close the database connection
dbDisconnect(db)
```

## Data



## Sources



## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 


```
