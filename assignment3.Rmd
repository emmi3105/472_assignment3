---
title: "Assignment 3"
author: "Student ID: 201903536"
date: "30 November 2023"
output: html_document
---

```{r setup, include=FALSE} 
#####################################
# SETUP
#####################################

knitr::opts_chunk$set(echo = FALSE) 

#####################################
# Install/load packages
#####################################

library(rvest)
library(tidyverse)
library(purrr)
library(DBI)
library(RSQLite)
library(RSelenium)
library(netstat)
library(httr)
library(jsonlite)
library(tidycensus)
library(tigris)
library(stringr)
library(tmap)
#library(ggmap)
library(sf)
library(htmlwidgets)

```


## GitHub

The GitHub repository for **this assignment** can be found [here](https://github.com/emmi3105/472_assignment3).


## Exercise 1

This assignment will take us through a workflow that a data scientist might encounter in the real world, from data collection right through to analysis. Throughout this assignment we are going to use a local relational database to store a variety of different but related tables that we collect and may want to combine in various ways. We will want to ensure that each table within our database can be joined to every other table using a `primary key`. 

We will start by creating an empty local relational database. You should store this new database in a 'database' folder that you create within your assignment folder. Follow these steps:

1. Use the `DBI::dbConnect()` function in `R` to create a new `SQLite` database (either `YOUR_DB_NAME.sqlite` or `YOUR_DB_NAME.db`) in your database folder.
2. Use the `file.exists()` function in `R` to check for the existence of your relational database.  

Include in the main text of your `.html` submission the code that created the database **and** the code that checks for its existence **and** the output of that check.

```{r start exercise 1}

#####################################
# EXERCISE 1
#####################################

```


### Create a local relational database

```{r create database, echo=TRUE}

# Create database
db <- dbConnect(RSQLite::SQLite(), "universities_db.sqlite")
```

### Check for the existence of the database

```{r check existence of database, echo=TRUE}

# Check existence of the database
print(file.exists("universities_db.sqlite"))
```

As the output of the code chunk above is "TRUE", the database exists.


## Exercise 2

### a. Gathering structured data 

```{r start exercise 2}

#####################################
# EXERCISE 2
#####################################

```

**Note**: The solution to Exercise 2a can be found under the solution for Exercise 2b.

### b. Gathering unstructured data

#### Automatic webscraping function for 2a and b

I combined exercise 2a and 2b and directly wrote and executed a webscraping function called scrape_wikipedia_tables() that returns a dataframe with the following eight variables: 

i. The university’s name
ii. It’s status (public or private)
iii. The city in which it is located
iv. The state in which it is located
v. The URL of the university’s dedicated Wikipedia page
vi. The geographic coordinates of the (main) university campus
vii. The endowment of the university in USD dollars
viii. The total number of students (including both undergraduate and postgraduate)

The resulting dataframe is stored under the name "universities_table".


```{r wikipedia web scraping function}

scrape_wikipedia_tables <- function(url) {
  # Input: URL to wikipedia
  # Function accesses wikipedia articles on universities and scrapes information from the wikipedia pages.
  # Output: A table with information on US American R1 and R2 universities. More precisely, the table holds the university's name, status (private or public), city, state, the url to the university's Wikipedia page, the university's geographic coordinates, endowment, and number of total students.
  
  ### Exercise 2a: scrape information on the name, status, city, state and the url to the wikipedia page
  table_indices <- c(19, 27)
  
  # Storing the URL's HTML code
  html_content <- read_html(url)
  
  # Extracting all tables in the document 
  tab <- html_table(html_content, fill = TRUE)

  # Extract only the tables of interest given the input table_indices
  selected_tables <- tab[1:length(table_indices)]

  # Iterate over tables and bind rows
  empty_tibble <- tibble()
  for (i in 1:length(selected_tables)) {
    current_table <- as_tibble(selected_tables[[i]][, 1:4])
    empty_tibble <- bind_rows(empty_tibble, current_table)
  }

  # Initialize an empty list to store hyperlinks
  hyperlinks_list <- list()

  # Iterate over the table indices and extract hyperlinks
  for (index in table_indices) {
    table_nodes <- html_content %>%
      html_nodes(paste("#mw-content-text > div.mw-content-ltr.mw-parser-output > table:nth-child(", index, ")", sep = ""))
  
    current_node_set <- table_nodes[[1]]
  
    hyperlinks_current_node <- current_node_set %>%
      html_nodes("td:nth-child(1) a") %>%
      html_attr("href")
  
    # Append the hyperlinks to the list
    hyperlinks_list <- c(hyperlinks_list, hyperlinks_current_node)
  }

  # Unlist to flatten the list
  hyperlinks_list <- unlist(hyperlinks_list, recursive = FALSE)

  # Concatenate the base URL to the extracted hyperlinks
  hyperlinks_list <- paste0("https://en.wikipedia.org/", hyperlinks_list)
  
  # Add a new column 'Wikipedia_URL' with the extracted URLs
  empty_tibble$Wikipedia_URL <- hyperlinks_list

    
  ### Exercise 2b: Add the geographic location, endowment and students
  
  #### Geographic location

  geographic_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the geographic location using the class selector 'geo_default'
    geographic_location <- university_page %>%
      html_nodes(".geo-dec") %>%
      html_text() %>%
      unique()
 
    # Store the scraped information in a list
    geographic_data <- append(geographic_data, list(geographic_location))
  
  }

  # Add a new column 'Geographic_Location' with the scraped data
  empty_tibble$Geographic_Location <- geographic_data

  # Keep the first set of coordinates only
  empty_tibble <- empty_tibble %>%
    mutate(Geographic_Location = map_chr(Geographic_Location, ~ .[1]))
  
  #### Endowment and budget

  endowment_data <- list()

  # Clean text by removing text within brackets
  clean_text <- function(text) {
    # Remove text containing brackets within brackets
    text <- gsub("\\([^()]*\\)", "", text)
    # Remove text within square brackets
    text <- gsub("\\[.*?\\]", "", text)
    # Remove text within round brackets
    text <- gsub("\\(.*?\\)", "", text)
    return(text)
  }

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the endowment based on text content (entries with a dollar sign)
    endowment <- university_page %>%
      html_nodes("table.infobox th:contains('Endowment') + td") %>%
      html_text() %>%
      unique() %>%
      clean_text()
  

    # Check if 'Endowment' entry exists, if not, extract 'Budget'
    if (length(endowment) == 0) {
      budget <- university_page %>%
        html_nodes("table.infobox th:contains('Budget') + td") %>%
        html_text() %>%
        unique() %>%
        clean_text()

      endowment_data <- append(endowment_data, list(budget))
    } else {
      endowment_data <- append(endowment_data, list(endowment))
    }
  }

  # Add a new column 'Endowment' with the scraped data
  empty_tibble$Endowment <- endowment_data
  empty_tibble <- empty_tibble %>%
    mutate(Endowment = map_chr(Endowment, ~ .[1]))


  #### Students
  
  # To avoid issues with students being spread on different campuses, scrape total student information by adding information on all undergraduates to all postgraduates
  
  undergraduates_data <- list()
  postgraduates_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
  
    # Undergraduates 
    # Extract the number of undergraduate students
    undergraduates <- university_page %>%
      html_nodes("table.infobox th:contains('Undergraduates') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_undergraduates <- str_extract(undergraduates, "[\\d,]+")
  
    # Assign NA if cleaned_undergraduates is empty
    cleaned_undergraduates <- ifelse(length(cleaned_undergraduates) == 0, NA, cleaned_undergraduates)
    undergraduates_data <- append(undergraduates_data, list(cleaned_undergraduates))

    # Postgraduates 
    # Extract the number of postgraduate students
    postgraduates <- university_page %>%
      html_nodes("table.infobox th:contains('Postgraduates') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_postgraduates <- str_extract(postgraduates, "[\\d,]+")
  
    # Assign NA if cleaned_postgraduates is empty
    cleaned_postgraduates <- ifelse(length(cleaned_postgraduates) == 0, NA, cleaned_postgraduates)
    postgraduates_data <- append(postgraduates_data, list(cleaned_postgraduates))
  }

  # Remove commas from the extracted data and convert to numeric
  empty_tibble$Undergraduates <- as.numeric(gsub(",", "", unlist(undergraduates_data)))
  empty_tibble$Postgraduates <- as.numeric(gsub(",", "", unlist(postgraduates_data)))

  # Create a new column 'Students' as the sum of Undergraduates and Postgraduates
  empty_tibble$Students <- ifelse(is.na(empty_tibble$Undergraduates), 
                                  empty_tibble$Postgraduates, 
                                  ifelse(is.na(empty_tibble$Postgraduates), 
                                        empty_tibble$Undergraduates, 
                                        empty_tibble$Undergraduates + empty_tibble$Postgraduates))

  # If the website neither holds information on undergraduates nor on postgraduates, look for information on total students

  students_data <- list()

  for (hyperlink in hyperlinks_list) {
    university_page <- read_html(hyperlink)
   
    # Extract the students
    students <- university_page %>%
      html_nodes("table.infobox th:contains('Students') + td") %>%
      html_text() %>%
      unique()

    # Extract only the numbers using regular expressions
    cleaned_students <- as.numeric(gsub(",", "", str_extract(students, "[\\d,]+")))
    students_data <- append(students_data, list(cleaned_students))
  }

  # Check if any element in 'Students' is NA before assigning
  if (any(is.na(empty_tibble$Students))) {
    empty_tibble$Students[is.na(empty_tibble$Students)] <- unlist(students_data)
  }

  # Remove columns named 'Undergraduates' and 'Postgraduates'
  empty_tibble <- select(empty_tibble, -Undergraduates, -Postgraduates)
  
  
  # Finally, return the table including the scraped information
  return(empty_tibble)
}
```

```{r call the scrape_wikipedia_tables() webscraper,  eval = FALSE}

# Run the function using the URL from the Wikipedia page
url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"
scraped_wikipedia_df <- scrape_wikipedia_tables(url)

# Save it as a global variable
assign("scraped_wikipedia_df", scraped_wikipedia_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_wikipedia_df, file = "scraped_wikipedia_df.RData")
```

```{r create a copy of scraped_wikipedia_universities}

# Load global variables
load("scraped_wikipedia_df.RData")

# Copy the scraped table to avoid modifying the original data
universities_table <- scraped_wikipedia_df
```


```{r transform the endowment values}

# Function to transform endowment values
transform_endowment <- function(endowment_value) {
  # Extract numeric part and multiplier
  numeric_part <- as.numeric(gsub("[^0-9.]", "", endowment_value))
  multiplier <- ifelse(grepl("billion", tolower(endowment_value)), 1e9,
                       ifelse(grepl("million", tolower(endowment_value)), 1e6, 1))
  
  # Return the transformed numeric value
  return(numeric_part * multiplier)
}

# Apply the transformation to the Endowment column
universities_table$Endowment <- sapply(universities_table$Endowment, transform_endowment)
```

#### Notes on the function

**Geographic Location**: Some universities have several sets of coordinates on their wikipedia sites. For instance, there are universities with multiple campuses whose geographic locations are all noted down on wikipedia. Therefore, the function above includes a line of code that specifies that only the very first set of coordinates is appended to the dataframe. Thereby, the geographic location column holds simplified data that potentially might not reflect the location of the entire university.

**Endowment**: Not all universities we are interested in include information on the "endowment" on their wikipedia website. However, some university wikipedia articles include data on the university's "budget". Since endowment and budget likely refer to the same thing, I have included an if-statement in the function above, that specifies the following: If a university's wikipedia article does not hold information on the endowment, the function will look whether there is information on the budget and use this data instead. Nonetheless, there are a few universities that have neither the endowment nor the budget declared on wikipedia. For those instances, the endowment variable in our table will hold a missing values as specified in the following section.

**Students**: On some websites, the total student body is not declared in a comprehensive way because it is split up by the different campusses of the university. Therefore, it is easier to scrape the data by adding the number of postgraduates and undergraduates than to write a function that can add all of the student bodies from different campusses of the individual universities. To ensure that also universities that only have postgraduates or undergraduates, and universities which only have a number for the total student body on their website, the code is modified. Firstly, an if-statement will ensure that for those instances with NA's for either variable postgraduates or undergraduates, the number from the variable without the NA is declared as the total student body. Secondly, for the instances that only include data on the total student body on the website, another webscraping is formulated that scrapes the "Students" data from the Wikipedia website and then adds this to the variable in our scraped data set.

#### Notes on missing values

**Geographic Location**: There are missing values in the geographic location column. To be exact, the [University of Colorado Denver](https://en.wikipedia.org//wiki/University_of_Colorado_Denver) and the [University of Mississippi](https://en.wikipedia.org//wiki/University_of_Mississippi) do not have information on the geographic location on their wikipedia websites.

**Endowment**: For some universities, there are no data on the endowment that is available on the wikipedia website. More precisely, the [Airforce Institute of Technology](https://en.wikipedia.org//wiki/Air_Force_Institute_of_Technology), [Azusa Pacific University](https://en.wikipedia.org/wiki/Azusa_Pacific_University), [Long Island University](https://en.wikipedia.org//wiki/LIU_Post	), and the [University of Missouri–St. Louis](https://en.wikipedia.org//wiki/University_of_Missouri%E2%80%93St._Louis	) do not have information on the website on endowment/budget.


### c. Data munging

#### "ivyleagues" dataframe

```{r read in ivyleague.csv}

# Read the CSV file into a data frame
ivyleagues <- read.csv("ivyleague.csv")
```

We have stored "ivyleague.csv" as a dataframe called "ivyleagues". It contains four variables, namely a **shortened** version of each university's name ("uni_name"), the County ("county") and State ("state") in which the university's main campus is located, and the university's Employer Identification Number ("ein"). There are eight observations in the dataframe.

Since the university names in "ivyleagues" are shortened, yet later on (in our relational database), we want to reference the universities by their full name, we create a new variable in the ivyleagues dataframe that gives the full name of the universities.

```{r add the full names to the universities in ivyleagues}

# Update the full_name column based on conditions
ivyleagues <- ivyleagues %>%
  mutate(
    Institution = case_when(
      uni_name == "Penn" ~ "University of Pennsylvania",
      uni_name == "Brown" ~ "Brown University",
      uni_name == "Columbia" ~ "Columbia University",
      uni_name == "Cornell" ~ "Cornell University",
      uni_name == "Dartmouth" ~ "Dartmouth College",
      uni_name == "Harvard" ~ "Harvard University",
      uni_name == "Princeton" ~ "Princeton University",
      uni_name == "Yale" ~ "Yale University",
    )
  )
```

#### Ivy League Indicator, County and EIN

Now, we use the "iveleagues" dataframe to create three new variables in the "universities_table" dataframe, namely:

ix. "Ivy_League": An binary variable indicating whether the university is an Ivy League institution
x. "County": The university's county and state (only for Ivy Leagues, for all others, "County" is NA)
xi. "EIN": The university's EIN (only for Ivy Leagues, for all others, "EIN" is NA)

```{r clean the ivyleagues table}

# Clean the ivyleagues table
ivyleagues <- ivyleagues %>%
  mutate(County = ifelse(!is.na(county), paste(county, state, sep = ", "), state))

universities_table <- universities_table %>%
  left_join(ivyleagues %>% select(Institution, ein, County), by = c("Institution" = "Institution")) %>%
  mutate(Ivy_League = ifelse(Institution %in% ivyleagues$Institution, 1, 0)) %>%
  rename(EIN = ein)
```


### d. Writing to your relational database

#### Check if the data is tidy

We check whether the data is tidy in two steps:

1. Each row is a unique university

```{r check if each row holds one unique observation}

# Check if the rows in universities_table each hold one unique university 
if (length(unique(universities_table$Institution)) == length(universities_table$Institution)) {
  answer <- sprintf("There are %d unique universities in the dataframe 'universities_df' that are each represented by exactly one row. The data is tidy.", length(unique(universities_table$Institution)))
  print(answer)
} else {
  print("Not every row holds the data of a unique university. The data is not tidy.")
}
```

2. Each column is a variable

```{r check if each variable is represented by one column}

# Check if the columns in universities_table each represent one of the 11 variables
if (ncol(universities_table) == 11) {
  column_names <- colnames(universities_table)
  column_answer <- sprintf("The dataframe has %d columns:", ncol(universities_table))
  formatted_names <- sprintf("Column %d: %s", 
                             seq_along(column_names), 
                             column_names)
  cat(column_answer, formatted_names, sep = "\n")
} else {
  print("The dataframe does not have 11 columns. It is not tidy.")
}
```

As we can see, the data is tidy. Each row represents one of the 279 unique universities and each column holds one of the 11 variables.

#### Write universities_table to the relational database

We write "universities_table" to the relational database "db" under the name "universities". The unique key is the university name, which can be accessed via the "Institution" variable.

```{r write universities_table to the relational database}

# Write universities_table to the relational database
dbWriteTable(db, "universities_df", universities_table, overwrite = TRUE)
```

Now, we want to check the existence and dimensionality of "universities". We do so by defining a function "check_table" that returns the number of rows and columns and the column names of a table if it exists in our relational database.

```{r write a function that checks the existence and correct dimensionality of the table, echo=TRUE}

# Check_table is a function that checks the existence and dimensionality of a table in db
check_table <- function(db, a_table){
  
  # Check if the "a_table" exists
  if (a_table %in% dbListTables(db)) {
    
    # Get the row count
    query <- paste("SELECT COUNT(*) FROM", a_table)
    row_count <- dbGetQuery(db, query)[1, 1]
  
    # Get the column names
    column_names <- as.character(dbListFields(db, a_table))
  
    # Get the column count
    column_count <- length(unlist(column_names))
    
    # Print out dimensions and column names
    formatted_string <- sprintf("The table exists and has the following dimensions: \nNumber of rows: %s \nNumber of columns: %s", row_count, column_count)
    column_answer <- sprintf("Column names: %s", paste(column_names, collapse = ", "))
    
    return(cat(formatted_string, column_answer, sep = "\n"))
    
  } else {
    # If "a_table" does not exist, return this statement:
    return(cat("The table does not exist."))
  }
}

# Call check_table on "universities_df"
check_table(db, "universities_df")
```



## Exercise 3

**Task**
We are now going to use the `Rselenium` package to explore the [Academic Ranking of World Universities](https://www.shanghairanking.com/).^[Are university rankings really meaningful? Probably not, but we will explore them for this exercise anyway.]

```{r start exercise 3}

#####################################
# EXERCISE 3
#####################################

```

### a. Scraping annual rank

**Task**
Create a webscraper that returns, for the **Ivy League university only**:

i. The ARWU ranking for the university for the years 2003, 2013, and 2023. If the university's rank is given as a range e.g. 76-100, convert this to the midpoint of the range -- in this case 88.

Your final table should be in tidy long format, where each row uniquely identifies a combination of university and year (e.g., Harvard-2003). Write the data as a new table -- appropriately and clearly named -- to your relational database. Check for the existence and correct dimensionality of your written table using the function you wrote in Exercise 2.d. Include only the call to the function and the output in the main section of your `.html` file.

#### Launch the browser and navigate to the website

In a first step, we will launch the driver and the browser. Note that we use chromenever = NULL to avoid launching chrome as chrome browsers have shown to be buggy and we prefer using firefox instead.

```{r launch the browser, results='hide', message=FALSE, eval=FALSE}

# Launch the driver and browser
invisible(capture.output({
  rD <- rsDriver(browser=c("firefox"), port = free_port(random = TRUE), chromever = NULL) 
  driver <- rD$client
}))
```

When the browser has been launched, we can navigate to the [Academic Ranking of World Universities Website](https://www.shanghairanking.com/) using the URL provided in the task.

```{r navigate to the website, eval=FALSE}

# Navigate to the website
url <- "https://www.shanghairanking.com/"
driver$navigate(url)

# Navigate to the 2023 ranking 
academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[2]/div[1]/button')
academic_ranking_23$clickElement()
```

#### Preparation for the scraping

Before we can scrape the data on the overall rankings of the ivyleague universities, we create a few functions that will be needed for the webscraper.

##### 1. Function: Search for individual ivy league universities

We create a function that searches for each individual university. The function "search_for()" takes the university's name as the input argument. It does not create or return any data objects but it simply filters out the university of interest.

```{r function that searches for the ivys}

# A function that searches for each ivy league university 
search_for <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[1]/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(1)
  search_field$sendKeysToElement(list(key = "enter"))
  
}
```


##### 2. Function: Navigate to a different year

By default, the Academic Ranking of World Universities Website displays the rankings for the year 2023. However, since we are interested in the overall ranking of the ivyleagues for three different years - namely 2003, 2013 and 2023 - we need a function that can navigate to a different year. "navigate_year()" neither takes any input arguments nor creates or displays any data objects. It simply finds the drop down menu displaying the different options for year filters and opens it up. Note that upon executing "navigate_year()" we need to further specify, which year should be selected.

```{r create a function that navigates to a different year }

# A function that navigates to a different year
navigate_year <- function() {
  # Find the element using the provided XPath
  year_selector <- driver$findElement(using = "xpath", value =  '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[1]/img')
  
  # Click on the element
  year_selector$clickElement()
}
```



##### 3. Function: Extract the overall rank

Finally, we need a function that actually extracts the rank displayed after searching for the university and year of interest. "extract_rank()" extracts and returns the value displayed in the column "World Rank" for the university of interest. 

```{r functions to extract the world rank and the year}

extract_rank <- function() {
  # A function that extracts the rank from the website
  
  # Find the element on the website and transform it to text directly
  current_university_rank <- driver$findElement(using = "xpath",
                                                value = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/table/tbody/tr/td[1]/div')$getElementText()[[1]]
  
  return(current_university_rank)
}

extract_year <- function() {
  # A function that extracts the year we are currently searching for and returns it
  current_year <- driver$findElement(using = "xpath",
                                     value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[1]')$getElementText()[[1]]
  
  return(current_year)
}
```


##### 4. Create empty data frames

Before putting everything together in one scraping function, we will temporarily create three data frame - one for each year of interest - that hold the names of the ivy leagues in their first column. Furthermore, these dataframes have an empty column for the rank and another empty column that will later hold the year. Before scraping the data from the website, the three dataframes look identical. When executing the webscraper, the university names and the respective ranking values will be appended to the dataframe corresponding to the same year. Once the scraper has extracted all of the information we are interested in, we will merge the three dataframes and return the final table.

```{r create a dataframe with ivy leagues}

# Create a dataframe that holds all ivy league universities as displayed in the ivyleagues data
ivyleagues_df <- data.frame(
  Institution = ivyleagues$Institution,
  # Add an empty column for the year 
  Year = numeric(length(ivyleagues$Institution)),
  # Add an empty column for the Rank
  Rank = numeric(length(ivyleagues$Institution)),
  stringsAsFactors = FALSE
)

# Create three copies of the ivyleagues_df data frame for each of the three years that we will later scrape ranking data from
ivyleagues_03 <- ivyleagues_df
ivyleagues_13 <- ivyleagues_df
ivyleagues_23 <- ivyleagues_df
```


#### Scrape the overall rankings

Finally, we can use the functions and dataframes created previously to build a function "scrape_university_ranking_global()" that scrapes the overall ranking for each ivyleague university from the [Academic Ranking of World Universities Website](https://www.shanghairanking.com/) for the years 2003, 2013, and 2023. The function does not take any input arguments, yet notice that it calls other functions such as "search_for()" that do take input arguments. Upon execution, the function returns one dataframe of three columns - "Institution", "Year", and "Rank". As we scraped the ranking of eight universities for three years, we should expect the dataframe to have 24 rows.

The code for the function can be seen in the Appendix.

```{r extract the world rank of each ivy league}

# Scrape the rank for each ivy league university from the website for the years 2003, 2013, 2023

scrape_university_ranking_global <- function() {
  ##########
  ## 2023 ##
  ##########

  # Navigate to 2023
  navigate_year()

  academic_ranking_23 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[1]')
  academic_ranking_23$clickElement()

  # Loop through all ivy league universities in the ivyleagues dataset and call the search_for function for each university
  for (i in seq_along(ivyleagues_23$Institution)) {
  
    # Search for the university
    search_for(ivyleagues_23$Institution[i])
  
    # Add a delay to allow the page to load before the next search
    Sys.sleep(2)  
  
    # Extract the rank from the webpage and assign it to the Rank column
    ivyleagues_23$Rank[i] <- extract_rank()
  
    # Assign the year 2023 to the Year column
    #ivyleagues_23$Year[i] <- extract_year()
    ivyleagues_23$Year[i] <- "2023"

  }

  ##########
  ## 2013 ##
  ##########

  # Navigate to 2013
  navigate_year()

  academic_ranking_13 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[11]')
  academic_ranking_13$clickElement()

  for (i in seq_along(ivyleagues_13$Institution)) {
  
    # Search for the university
    search_for(ivyleagues_13$Institution[i])
  
    # Add a delay to allow the page to load before the next search
    Sys.sleep(2)  
  
    # Extract the rank from the webpage and assign it to the Rank column
    ivyleagues_13$Rank[i] <- extract_rank()
  
    # Assign the year 2013 to the Year column
    ivyleagues_13$Year[i] <- "2013"

  }

  ##########
  ## 2003 ##
  ##########

  # Navigate to 2003
  navigate_year()

  academic_ranking_03 <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div/div[1]/div[2]/div[2]/div/div[1]/div/div[2]/ul/li[21]')
  academic_ranking_03$clickElement()

  for (i in seq_along(ivyleagues_03$Institution)) {
  
    # Search for the university
    search_for(ivyleagues_03$Institution[i])
  
    # Add a delay to allow the page to load before the next search
    Sys.sleep(2)  
  
    # Extract the rank from the webpage and assign it to the Rank column
    ivyleagues_03$Rank[i] <- extract_rank()
  
    # Assign the year 2003 to the Year column
    ivyleagues_03$Year[i] <- "2003"

  }
  
  # Creating a list of data frames
  scraped_ivyleagues_df <- rbind(ivyleagues_03, ivyleagues_13, ivyleagues_23)

  return(scraped_ivyleagues_df)

}
```


#### Call the ARWU ranking webscraper

Now, we will call the function "scrape_university_ranking_global() to scrape the global ranking for the years 2003, 2013, and 2023. The function returns a dataframe, which we will assign to the name "ivyleagues_ranks".


```{r call the scrape_university_ranking_global() webscraper, eval = FALSE}

# Call the global rank scraping function
scraped_ivyleagues_df <- scrape_university_ranking_global()

# Save it as a global variable
assign("scraped_ivyleagues_df", scraped_ivyleagues_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_ivyleagues_df, file = "scraped_ivyleagues_df.RData")
```

```{r create a copy of scraped_ivyleagues_df}

# Load global variables
load("scraped_ivyleagues_df.RData")

# Copy the scraped table to avoid modifying the original data
ivyleagues_ranks <- scraped_ivyleagues_df
```


Before adding the table to the universities_db database, we need to calculate the midpoint for the observations that reported a range for the rank instead of a single number. The "calculate_midpoint()" function can be found in the appendix.

```{r calculate the midpoint for global rank}

# Function to calculate midpoint from a range
calculate_midpoint <- function(rank) {
  for (i in seq_along(rank)) {
  range_midpoint <- mean(as.numeric(strsplit(rank[i], "-")[[1]])) 
  return(range_midpoint)}
}

# Calculate the midpoint
ivyleagues_ranks$Rank <- sapply(ivyleagues_ranks$Rank, calculate_midpoint)
```


#### Write it to the database

Finally, we can write the table to our relational database.

```{r write the ivyleagues_ranks table to the relational database}

# Adding the table to the relational database
# Only include overwrite = TRUE for practicing purpose
dbWriteTable(db, "ivyleagues_ranks", ivyleagues_ranks, overwrite = TRUE)
```


We can test whether the scraping worked properly by using the “check_table()” function. It should return a table of 24 rows and 3 columns, namely "Institution", "Year", and "Rank".

```{r check the existence of the ivyleagues_ranks table, echo=TRUE}

# Check the existence of the ivyleagues_ranks table
check_table(db, "ivyleagues_ranks")
```


#### Navigate back to the home page

Before going over to the next exercise, we will navigate back to the home page of the website to prevent errors while scraping information from a different place within the website.

```{r navigate to the home page, eval=FALSE}

# Navigate to the home page to make it easier to keep working on this website
home_page <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[1]/a')

home_page$clickElement()
```


### b. Scraping subject ranks for 2023

**Task**
Extend your webscraper (or create a new one) that gathers for each **Ivy League university only**:

i. The rankings of the university for **every social science** for which the university has been ranked. Again, if a range is given, take the midpoint.

Your final table should be in tidy long format, where each row uniquely identifies a combination of university and discipline (e.g., Harvard-Economics). Write the data as a new table -- appropriately and clearly named -- to your relational database. Check for the existence and correct dimensionality of your written table using the function you wrote in Exercise 2.d. Include only the call to the function and the output in the main section of your `.html` file.




#### Preparation for the scraping

Before scraping the rankings for different social sciences for the ivyleague universities, we need to create a few functions.

##### 0. Navigate to page with the data on the university ranking by subject

Firstly, we need to navigate to the place on the website where the subject-specific rankings for each university are found. The data we are interested can be found under "Universities" in the navigation bar on top of the home page of the shanghai ranking website. After navigating, we will create the functions needed for the scraping.

```{r function that navigates to the subject rankings}

# A function that navigates to the page with the subject rankings
navigate_universities <- function() {
  universities_ranks <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[1]/div[1]/div/div[2]/ul/li[3]')
  universities_ranks$clickElement()
}
```

```{r navigate to the page with the subject rankings, eval=FALSE}

navigate_universities()
```


##### 1. Search ivyleagues and navigate to individual page

Firstly, we will create a function that searches for each individual university and navigates to the university specific website on which the university's rankings for each subject are published. The function "search_general_ranking()" takes the university's name as the input argument and navigates to its website. It does not create or return any data objects.

```{r function that searches for the ivys in the general ranking and sekects it}

# A function that searches for each ivy league university 
search_general_ranking <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "Harvard University"
  search_field <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[1]/div/div/div/input')
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(2)
  search_field$sendKeysToElement(list(key = "enter"))
  
  # Select the first element
  Sys.sleep(2)
  first_result <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div[1]/div[2]') 
  first_result$clickElement()

}

```


##### 2. Filter for the social sciences

Secondly, we need a function that filters the rankings published on the individual university page for social sciences only. The function "filter_social_sciences()" neither takes any input arguments nor creates or returns any data objects. It simply opens up the drop down menu holding the filter options and selects "Social Sciences".

```{r filter for social sciences}

# A function that filters for the social sciences in the ranking options (with js)
filter_social_sciences <- function() {
  
  # Click the filter field
  filter_subject <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div/div[2]/div[2]/div[2]/div[1]/div[1]/div[2]/div/div[1]/img')
  filter_subject$clickElement()

  
  partial_inner_html <- "Social Sciences"

  # Construct an XPath that searches for an element containing the specified inner HTML
  xpath_expression <- sprintf('//*/text()[contains(., "%s")]/parent::*', partial_inner_html)

  # Find the element using the constructed XPath
  social_sciences_element <- driver$findElement(using = "xpath", value = xpath_expression)

  # Click on the found element
  social_sciences_element$clickElement()
  
}
```

##### 3. Extract the social science ranks

Lastly, we need a function that extracts the ranks of all social sciences for the individual universities. Before defining the function, we create an empty table "social_science_ranks" that has three columns: "Institution", "Subject", and "Number". The function "extract_social_science_rank" takes the university name and the "social_science_ranks" table as an input arguments and appends the scraped ranks to the table. It returns the social_science_ranks table including the scraped data for the university that was the input argument. Note that when executing the function several times, it will continue to append the new data to the same table instead of overwriting previously extracted data.

```{r extract the social science ranks}

# Create a df that will later hold the rankings for the social sciences
social_science_ranks <- data.frame(Institution = character(),
                          Subject = character(),
                          Number = character(),
                          stringsAsFactors = FALSE)

# A function that extracts the social science rankings and appends them to the data frame "social_science_ranks"
extract_social_science_rank <- function(institution_name, existing_table) {
  
  # entire table with ranks
  rank_table <- driver$findElement(using = "xpath", value = '/html/body/div/div/div/div[2]/div[2]/div/div[2]/div[2]/div[2]/div[1]/div[2]/table/tbody')$getElementText()[[1]]
  
  # Split the data into subjects and numbers
  split_data <- unlist(strsplit(rank_table, "\n"))

  # Add the scraped data to the existing table
  existing_table <- rbind(existing_table,
                          data.frame(Institution = rep(institution_name, length(split_data) / 2),
                                     Subject = split_data[seq(1, length(split_data), by = 2)],
                                     Number = split_data[seq(2, length(split_data), by = 2)],
                                     stringsAsFactors = FALSE))
  
  return(existing_table)
}

```


#### Scrape the social science rankings

Now, we can use the previously defined functions to scrape the social science rankings for all ivyleagues.

```{r scrape social science rankings, eval=FALSE}

navigate_universities()
Sys.sleep(2)  

# University of Pennsylvania
# Search for the university
search_general_ranking("University of Pennsylvania")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("University of Pennsylvania", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Brown University
# Search for the university
search_general_ranking("Brown University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Brown University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 


# Columbia University
search_general_ranking("Columbia University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Columbia University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 



# Cornell University
search_general_ranking("Cornell University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Cornell University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2) 



# Dartmouth College
search_general_ranking("Dartmouth College")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Dartmouth College", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)



# Harvard University
search_general_ranking("Harvard University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Harvard University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)



#	Princeton University
search_general_ranking("Princeton University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Princeton University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)



#	Yale University
search_general_ranking("Yale University")
  
# Add a delay to allow the page to load before the next search
Sys.sleep(2)  
  
# Filter for the social sciences
filter_social_sciences()

# Extract the social science ranks
social_science_ranks <- extract_social_science_rank("Yale University", social_science_ranks) 
Sys.sleep(2)  

# Navigate back to all universities
navigate_universities()
Sys.sleep(2)

# Save it as a global variable
assign("social_science_ranks", social_science_ranks, envir = .GlobalEnv)

# Save the global variable to an RData file
save(social_science_ranks, file = "social_science_ranks.RData")
```

```{r create a copy of social_science_ranks}

# Load global variables
load("social_science_ranks.RData")

# Copy the scraped table to avoid modifying the original data
social_science_ranks_df <- social_science_ranks
```



#### Calculate the midpoint 

Using the "calculate_midpoint()" function, we get the midpoint for the observations that reported a range for a specific ranking.

```{r calculate the midpoint for the ranks in social_science_ranks_df}

# Calculate the midpoint for the social science rankings that were displayed as a range
social_science_ranks_df$Number <- sapply(social_science_ranks_df$Number, calculate_midpoint)
```


#### Write the table to the database

Finally, we can write the table to our relational database.

```{r write the social_science_ranks_df table to the relational database}

# Adding the table to the relational database
# Only include overwrite = TRUE for practicing purpose
dbWriteTable(db, "social_science_ranks_df", social_science_ranks_df, overwrite = TRUE)
```

We can test whether the scraping worked properly by using the "check_table()" function. It should return a table of 86 rows and 3 columns, namely "Institution", "Subject", and "Number".

```{r check the existence of the social_science_ranks_df table, echo=TRUE}

# Check the existence of the social_science_ranks table
check_table(db, "social_science_ranks_df")
```

```{r close the RSelenium process, eval=FALSE}

# Close the RSelenium processes:
driver$close()
# Close the associated Java processes
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
```





## Exercise 4

```{r start exercise 4}

#####################################
# EXERCISE 4
#####################################

```

We are now going to gather a variety of additional data for each **Ivy League university** only from two APIs.

### a. Gathering financial data from a raw API

#### Finance dataframe

We create a dataframe "finance_df" that we will later use to add the extracted asset and revenue values to. It has four variables which are represented by one column each:

1. "Institution" holds the full name of the ivyleague.
2. "ein" is the Employer Identification Number as extracted from the ivyleagues.csv file in exercise 2c.
3. "total_revenue" holds the total revenue of the university. As of now, this column is empty.
4. "total_assets" holds the total assets of the university. As of now, this column is empty.

As of now, the dataframe does not contain any observations. After having retrieved the financial data from the API we will add it to the finance_df dataframe. Then, the rows in the data frame should represent the ivyleagues for each year (e.g., Harvard - 2015). Since we are interested in the data for 11 years (asssuming that 2010 - 2020 is inclusive of the years 2010 and 2020) for eight universities, we should expect the final dataframe to have 88 rows.

```{r create an empty dataframe finance_df}

# Create an empty dataframe for the financial information
finance_df <- data.frame(
  Institution = NA,
  # Add a column for the EIN
  ein = NA,
  # Add an empty column for the year
  year = NA,
  # Add an empty column for the total revenue
  total_revenue = NA,
  # Add an empty column for the total assets
  total_assets = NA,
  stringsAsFactors = FALSE
)
```

#### Write functions

The final function that extracts the data on total revenue and total assets should only take one input argument - namely the ein. However, we also need the name of the universities and the url that we need to query the API for data on the university of interest. Therefore, we will first need to write functions that get this information only using the ein as input.

##### 1. Function: get the university name given the ein

We write a function that gets the university name corresponding to an ein as found in the "ivyleagues" data. The function "get_university_name()" takes the ein as the only input argument and returns the university's full name.

```{r get the university name given the ein}

# Function that gets the university name given the ein
get_university_name <- function(ein) {
  # Look for the ein in the dataframe  
  matching_row <- ivyleagues[ivyleagues$ein == ein, ]
  # Check if a match is found
  if (nrow(matching_row) > 0) {
    # Return the corresponding university name
    return(matching_row$Institution)
  } else {
    # Return a message indicating no match found
    return("University not found")
  }
}
```

##### 2. Function: get the data by using individual GET requests from the ein

We also need to create personalised GET requests that retrieve the data for each university given their ein. Therefore, we will need to write a function that uses the ein as input argument to:

- firstly create an individual url: "https://projects.propublica.org/nonprofits/api/v2/organizations/", ein, ".json",
- secondly create an individualised query that retrieves the data for the university of which we have inputted the ein, and
- thirdly parse the content of the query result and return it.

```{r get the data from the API given the ein}

# Function that queries the API for each university's data given their ein
get_data <- function(ein) {
  # Create a url given the ein
  url <- paste0("https://projects.propublica.org/nonprofits/api/v2/organizations/", ein, ".json")
  response <- GET(url)
  api_data <- content(response, "parsed")
  
  return(api_data)
}
```

##### 3. Function: get the revenue and assets for each university given the ein

Lastly, we will need to put everything together. We create a function that takes the ein as input. It executes "get_university_name()" and "get_data()" to retrieve the name of the university and queries the API for the data corresponding to the university. Then, this function should run through said data and retrieve the total asset and total revenue values for all of the years that we have data available. Note that this means that the function does not necessarily give us the data for the years 2010-2020. We will have to later modify the dataframe by deleting potential instances that we are not interested in (e.g., for the year 2021) and by adding rows of NA's for years for which the financial data is not available. 

The function will append this data to the dataframe "finance_df" that we have previously created.

```{r get the finance data using the previously defined functions}

# function that gets the finance data given the ein and using the functions get_university_name and get_data
get_finance_data <- function(ein) {
  # Get the university name given the ein
  name <- get_university_name(ein)
  
  # Get the data given the ein
  dt <- get_data(ein)
  
  # Loop to append values
  for (i in seq(length(dt$filings_with_data))) {
    # Append the values to the dataframe
    finance_df <- rbind(finance_df, data.frame(Institution = name,
                                               ein = ein,
                                               year = dt$filings_with_data[[i]]$tax_prd_yr,
                                               total_assets = dt$filings_with_data[[i]]$totassetsend,
                                               total_revenue = dt$filings_with_data[[i]]$totrevenue))
  }

  # Return the resulting dataframe
  return(finance_df)
}
```


#### Call get_finance_data() to retrieve asset and revenue values

Now, we can use lapply() to execute get_finance_data() on all observations in the ivyleagues dataframe using the ein column.

```{r call the get_finance_data() function, eval=FALSE}

# Call the "get_finance_data()" function
result_list <- lapply(ivyleagues$ein, get_finance_data)
scraped_finance_df <- do.call(rbind, result_list)

# Save it as a global variable
assign("scraped_finance_df", scraped_finance_df, envir = .GlobalEnv)

# Save the global variable to an RData file
save(scraped_finance_df, file = "scraped_finance_df.RData")
```


```{r load scraped_finance_df.RData}

# Load global variables
load("scraped_finance_df.RData")

# Copy the scraped table to avoid modifying the original data
finance_df <- scraped_finance_df
```


#### Missing values

```{r format the finance_df}

# Delete rows that only consist of missing values
finance_df <- finance_df %>%
  na.omit() 

# Fill NA values for missing years
complete_df <- expand.grid(year = 2011:2021, 
                           ein = unique(finance_df$ein))

# Add the "Institution" column to complete_df
complete_df <- complete_df %>%
  left_join(unique(finance_df[, c("ein", "Institution")]), by = "ein") %>%
  arrange(ein, year)

# Merge with the original dataframe
finance_df <- complete_df %>%
  left_join(finance_df, by = c("year", "ein")) %>%
  arrange(ein, year)

finance_df <- finance_df %>%
  select(-Institution.y, Institution = Institution.x)
```

**Note**: The final dataframe has missing values. For instance, the data for total assets and total revenue of Harvard University is unavailable for the year 2020. The observations with missing values for total revenue and total assets were originally omitted from the dataframe. Therefore, we computed these rows and added NA's for their financial information.


#### Write it to the database

Finally, we can write the table to our relational database.

```{r write the finance_df table to the relational database}

# Adding the table to the relational database
# Only include overwrite = TRUE for practicing purpose
dbWriteTable(db, "finance_df", finance_df, overwrite = TRUE)
```


We can test whether the scraping worked properly by using the “check_table()” function. It should return a table of 88 rows and 5 columns, namely "year", "ein", "Institution", "total_revenue", and "total_assets".

```{r check the existence of the finance_df table, echo=TRUE}

# Check the existence of the ivyleagues_ranks table
check_table(db, "finance_df")
```



### b. Gathering local economic data from a packaged API 


```{r set up the api key, message=FALSE}

readRenviron("../../Documents/R_Environs/api_census.env")
apikey <- Sys.getenv("KEY")
census_api_key(apikey)
```

#### Retrieve the median estimated household income

We retrieve the median estimated household income for every county in the US for 2015 and 2020 based on the American Community Survey. 

```{r retrieve median estimated household income, eval=FALSE}

# Median household income 2015
median_household_income_15 <- get_acs(geography = "county", year = 2015, variables = "B19013_001")
median_household_income_20 <- get_acs(geography = "county", year = 2020, variables = "B19013_001")

# Save it as a global variable
assign("median_household_income_15", median_household_income_15, envir = .GlobalEnv)
assign("median_household_income_20", median_household_income_20, envir = .GlobalEnv)

# Save the global variable to an RData file
save(median_household_income_15, file = "median_household_income_15.RData")
save(median_household_income_20, file = "median_household_income_20.RData")
```


```{r load median_household_income_15.RData and median_household_income_20.RData}

# Load global variables
load("median_household_income_15.RData")
load("median_household_income_20.RData")


# Copy the scraped table to avoid modifying the original data
income_15_df <- median_household_income_15
income_20_df <- median_household_income_20
```


#### Create a table for the Ivy Leagues with columns for the median household income 

We create a table "census_df" for the Ivy League universities that has four columns:

1. "Institution": the name of the Ivy League university
2. "County": the county in which the campus of the university is located
3. "Year": the year for which the data has been retrieved (either 2015 or 2020)
4. "Median_Household_Income": the estimated median household income for the County

The values in column 4 are retrieved from the American Community Survey (ACS) in the Census API.

```{r create a dataframe census_df}

# Create an empty dataframe
census_df <- data.frame(
  Institution = ivyleagues$Institution,
  County = ivyleagues$County,
  stringsAsFactors = FALSE
)

# Add the information on the median household income from the census API for 2015 and 2020
census_df <- census_df %>%
  
  # For 2015
  left_join(select(income_15_df, NAME, estimate), by = c("County" = "NAME")) %>%
  mutate("2015" = estimate) %>%
  select(-estimate)  %>%
  
  # For 2020
  left_join(select(income_20_df, NAME, estimate), by = c("County" = "NAME")) %>%
  mutate("2020" = estimate) %>%
  select(-estimate)

# Pivot longer
census_df <- census_df %>%
  pivot_longer(cols = c("2015", "2020"),
               names_to = "Year",
               values_to = "Median_Household_Income")

# Set the year column as numeric
census_df$Year <- as.numeric(census_df$Year)
``` 


#### Write it to the database

Finally, we can write the "census_df" table to our relational database.

```{r write the census_df table to the relational database}

# Write the table to the relational database
dbWriteTable(db, "census_df", census_df, overwrite = TRUE)
```


We can test whether the scraping worked properly by using the “check_table()” function. It should return a table of 16 rows and 4 columns, namely "Institution", "County", "Year", and "Median_Household_Income".

```{r check the existence of the census_df table, echo=TRUE}

# Check the existence of the census_df table
check_table(db, "census_df")
```



## Exercise 5

```{r start exercise 5}

#####################################
# EXERCISE 5
#####################################

```


### a. Analysis and visualisation
Using `SQL` (embedded within `R`), call into `R` from your relational database an analysis table that includes, for the **Ivy League institutions only**: 

i. University name
ii. The average rank of the university across 2003, 2013, and 2023
iii. The average rank of the university's Economics, Political Science, and Sociology programs, if they were ranked
iv. The current endowment per student (total endowment divided by total number of students), in USD
v. The average total revenue per student across the years 2015 - 2020, in USD
vi. The average of the median household income for the County across the years 2015 and 2020, in USD

#### Analysis table

The code for the analysis table can be found in the Appendix.

```{r get the list of table in db}

# Get a list of tables in the database
tables <- dbListTables(db)

# Print the list of tables
cat("Tables in the database:", paste(tables, collapse = ", "), "\n")
```

```{r}
# Testing that it works with a simple query
dbListFields(db, "census_df")
```


```{r sql queries for the analysis table}

# Second column: The average rank of the university across 2003, 2013, and 2023
second_query <- "
  SELECT Institution, AVG(Rank) AS Average_Rank
  FROM ivyleagues_ranks
  WHERE Year IN (2003, 2013, 2023)
  GROUP BY Institution;
"

# Execute the query
second_result <- dbGetQuery(db, second_query)


# Third column: The average rank of the university's Economics, Political Science, and Sociology programs, if they were ranked
third_query <- "
  SELECT Institution, AVG(Number) AS Avg_Econ_PolSc_Soc
  FROM social_science_ranks_df
  WHERE Subject IN ('Economics', 'Political Sciences', 'Sociology')
  GROUP BY Institution;
"

# Execute the query
third_result <- dbGetQuery(db, third_query)


# Fourth column: The current endowment per student (total endowment divided by total number of students), in USD
fourth_query <- "
  SELECT Institution, Endowment / Students AS Endowment_Per_Student
  FROM universities_df
  WHERE Ivy_League = 1;
"

# Execute the query
fourth_result <- dbGetQuery(db, fourth_query)


# Fifth column: The average total revenue per student across the years 2015 - 2020, in USD
fifth_query_students <- "
  SELECT Institution, Students
  FROM universities_df
  WHERE Ivy_League = 1;
"

fifth_query_revenue <- "
  SELECT Institution, AVG(total_revenue) AS Avg_Revenue_2015_2020
  FROM finance_df
  WHERE year BETWEEN 2015 AND 2020
  GROUP BY Institution;
"

# Execute the queries
fifth_result_students<- dbGetQuery(db, fifth_query_students)
fifth_result_revenue <- dbGetQuery(db, fifth_query_revenue)

fifth_result <- merge(fifth_result_students, fifth_result_revenue,
                      by = "Institution", all = TRUE) %>%
  mutate(Rev_Per_Student_2015_2020 = Avg_Revenue_2015_2020 / Students) %>%
  select(Institution, Rev_Per_Student_2015_2020)


# Sixth column: The average of the median household income for the County across the years 2015 and 2020, in USD
sixth_query <- "
  SELECT Institution, AVG(Median_Household_Income) AS Avg_Household_Income_2015_2020
  FROM census_df
  WHERE year IN (2015, 2020)
  GROUP BY Institution;
"

# Execute the queries
sixth_result <- dbGetQuery(db, sixth_query)
```


```{r combine the tables resulting from the six queries}

# Merge the five dataframes retrieved through the sql queries by Institution
list_of_dataframes <- list(second_result, third_result, fourth_result, fifth_result, sixth_result)
analysis_df <- reduce(list_of_dataframes, merge, by = "Institution", all = TRUE)
```



#### Ggplots

Using `ggplot`, include in the main section of your `.html` five compelling, well-labeled plots that show the relationships between:

i. average university ranking and average Econ/PS/Soc ranking
ii. average university ranking and endowment per student
iii. average endowment per student and average median household income
iv. average revenue per student and average median household income

Comment on the relationships you find. Are any of them particularly interesting?  



##### 1. Average university ranking vs. average Econ/PS/Soc ranking

```{r first plot average university ranking and average Econ/PS/Soc ranking, message = FALSE}

# First ggplot: average university ranking and average Econ/PS/Soc ranking
first_plot <- ggplot(analysis_df, aes(x = Average_Rank, y = Avg_Econ_PolSc_Soc, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +  # Add labels
  labs(title = "Average Overall Ranking vs. Subject Ranking for Ivy League Universities",
       x = "Average Overall Ranking (2003, 2013, 2023)",
       y = "Average Ranking for Economics, Political Sciences, and Sociology",
       color = "Ivy League University") +
  theme_minimal() +
  theme(
    # Adjust the font size for the title and the axes
    axis.text = element_text(size = 10),     
    axis.title = element_text(size = 10),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(1, 750), expand = c(0.02, 0)) +  # Adjust the limits and expand
  scale_y_log10(limits = c(1, 200), expand = c(0.02, 0))

# Display the first plot
first_plot
```

**The plot**: Comment on the logarithmic scale of the axes.

**The findings**: There seems to be a positive correlation between the overall ranking and the average ranking for the subjects Economics, Political Sciences, and Sociology, which is not surprising. A university that is ranked highly overall is expected to have high rankings in most subjects as well. It should be noted, though, that the overall ranking displayed on the x-axis is averaged over the years 2003, 2013, and 2023, whilst the average subject ranking displayed on the y-axis only refers to the most recent rankings in the subjects Economics, Political Sciences, and Sociology. Thereby, the possibility that the overall ranking changed significantly since 2003 and thus influenced the overall ranking (either positively or negatively) must thereby be considered. It would probably be more informative to retrieve the subject rankings for the three years of interest and then calculate the average.

```{r R squared for the first plot}

# Fit a linear regression model
model1 <- lm(Avg_Econ_PolSc_Soc ~ Average_Rank, data = analysis_df)

# View the R-squared value
rsquared_value_1 <- summary(model1)$r.squared
cat("R-squared Plot 1:", rsquared_value_1, "\n")
```


##### 2. Average university ranking vs. endowment per student

```{r second plot average university ranking and endowment per student, message = FALSE}

# Second ggplot: average university ranking and endowment per student
second_plot <- ggplot(analysis_df, aes(x = Average_Rank, y = Endowment_Per_Student/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +  # Add labels
  labs(title = "Average Overall Ranking vs. Endowment Per Student",
       x = "Average University Ranking (2003, 2013, 2023)",
       y = "Endowment per Student, in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  theme(
    # Adjust the font size for the title and the axes
    axis.text = element_text(size = 10),     
    axis.title = element_text(size = 10),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(1, 750), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(100, 7500), expand = c(0.02, 0))

# Display the second plot
second_plot
```

**The plot**: Comment on the logarithmic scale of the axes.

**The findings**: The findings are somewhat surprising. One might expect universities with high rankings to have larger endowments per students as they firstly might be able to charge higher student fees due to their good reputation and secondly might be ranked so highly due to the availability of large resources that they can invest in the facilities and the quality of teaching. However, this expectation is flawed as it does not consider other factors that influence the relationship between the endowment per student and the overall ranking: 

1. Resource Allocation: A high endowment doesn't necessarily guarantee effective resource allocation that directly influences rankings. Some universities might have large endowments but allocate resources differently, prioritising spending on specific aspects such as faculty, research, infrastructure, or student programmes. This might impact their rankings differently.

2. Ranking Criteria: University rankings are based on various criteria, including academic reputation, faculty-to-student ratio, research output, and more. Endowment per student is just one financial aspect and might not capture the broader factors influencing rankings.

3. Other factors: Many other factors might explain the size of the endowment in relationship to the university ranking. Notably, Princeton University has a very high endowment per student. Yet one should note that Princeton has a relatively small student body. However, the high endowment might stem from other factors such as historical funding and donations, Alumni support, financial aid and tuition fees etc. The analysis of the correlation between ranking and endowment per student necessitates a more complex analysis of the factors explaining the composition of rankings and endowments.

```{r R squared for the second plot}

# Fit a linear regression model
model2 <- lm(Endowment_Per_Student ~ Average_Rank, data = analysis_df)

# View the R-squared value
rsquared_value_2 <- summary(model2)$r.squared
cat("R-squared Plot 2:", rsquared_value_2, "\n")
```



##### 3. Average endowment per student vs average median household income

```{r third plot average endowment per student and average median household income, message=FALSE}

# Third ggplot: average endowment per student vs average median household income
third_plot <- ggplot(analysis_df, aes(y = Avg_Household_Income_2015_2020/1000, x = Endowment_Per_Student/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +  # Add labels
  labs(title = "Endowment Per Student vs. Average Median Household Income",
       x = "Endowment per Student, in thousand USD",
       y = "Average Median Household Income for the County (2015 and 2020), in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  theme(
    # Adjust the font size for the title and the axes
    axis.text = element_text(size = 8),     
    axis.title = element_text(size = 8),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(300, 6500), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(35, 150), expand = c(0.02, 0))

# Display the third plot
third_plot
```


**The plot**: Comment on the logarithmic scale of the axes.

**The findings**: There seems to be a slight positive correlation between the average median household income for the county and the endowment per student.

```{r R squared for the third plot}

# Fit a linear regression model
model3 <- lm(Avg_Household_Income_2015_2020 ~ Endowment_Per_Student, data = analysis_df)

# View the R-squared value
rsquared_value_3 <- summary(model3)$r.squared
cat("R-squared Plot 3:", rsquared_value_3, "\n")
```


##### 4. Average revenue per student vs average median household income

```{r fourth plot average revenue per student and average median household income, message=FALSE}

# Fourth ggplot: average revenue per student vs. average median household income
fourth_plot <- ggplot(analysis_df, aes(y = Avg_Household_Income_2015_2020/1000, x = Rev_Per_Student_2015_2020/1000, color = factor(Institution))) +
  geom_point(size = 1.5) +
  # Add a regression line
  stat_smooth(method = "lm", se = TRUE, color = "#FEE000") +
  geom_text(aes(label = Institution), hjust = -0.005, vjust = -0.7, size = 2, color = "black") +
  labs(title = "Average Revenue Per Student vs. Average Median Household Income",
       x = "Average Revenue Per Student (2015 - 2020), in thousand USD",
       y = "Average Median Household Income for the County (2015 and 2020), in thousand USD",
       color = "Ivy League University") +
  theme_minimal() +
  theme(
    # Adjust the font size for the title and the axes
    axis.text = element_text(size = 8),     
    axis.title = element_text(size = 8),    
    plot.title = element_text(size = 12)
  ) +
  # Use a logarithmic scale for the x-axis
  scale_x_log10(limits = c(110, 600), expand = c(0.02, 0)) +
  scale_y_log10(limits = c(25, 150), expand = c(0.02, 0))

# Display the fourth plot
fourth_plot
```

```{r R squared for the fourth plot}

# Fit a linear regression model
model4 <- lm(Avg_Household_Income_2015_2020 ~ Rev_Per_Student_2015_2020, data = analysis_df)

# View the R-squared value
rsquared_value_4 <- summary(model4)$r.squared
cat("R-squared Plot 4:", rsquared_value_4, "\n")

```



### b. Visualisation of geographic data

Using `SQL`, call into `R` from your relational database a table that includes, for **every R1 and R2 university**:

i. University name
ii. Geographic coordinates
iii. Status (public vs. private)
iv. Whether the university is an Ivy League institution


#### Analysis table 

```{r sql queries for the geography table}

# Select columns: University name, geographic coordinates, status (private or public), Ivy League indicator
query_geography <- "
  SELECT Institution, Control, Geographic_Location, Ivy_League
  FROM universities_df;
"

# Execute the query
analysis_table_geography <- dbGetQuery(db, query_geography)

analysis_table_coordinates <- separate(analysis_table_geography, Geographic_Location, into = c("latitude", "longitude"), sep = " ", convert = TRUE)

# Extract numeric values from the coordinates
analysis_table_coordinates <- analysis_table_coordinates %>%
  mutate(
    latitude = as.numeric(str_extract(latitude, "([0-9.]+)")),
    longitude = as.numeric(str_extract(longitude, "([0-9.]+)"))
  ) %>%
  mutate(longitude = -longitude)

analysis_table_coordinates <- analysis_table_coordinates[complete.cases(analysis_table_coordinates$latitude, analysis_table_coordinates$longitude), ]

# Convert analysis_table geography into an sf object
geography_sf <- st_as_sf(analysis_table_coordinates, coords = c("longitude", "latitude"), crs = 4326)
```

Retrieve a [shapefile](https://en.wikipedia.org/wiki/Shapefile) of the United States using the `tigris` package (or, if you prefer the `tidycensus` package, but `tigris` is easier). Using either the `tmap` package or the `ggmap` package, include in the main section of your `.html` a visually clear and compelling map that is appropriately labelled which shows:

i. every R1 and R2 university, excluding the Ivy League institutions, as a point
ii. where the colour of the points varies by status (public vs. private)
iii. Ivy League universities as contrasting points 



```{r get the US states shapefile, message = FALSE}

invisible(capture.output({
  # Shapefile with US 
  us_map <- nation()
}))
```


```{r create the map plot, message = FALSE}

# Set the tmap mode
tmap_mode("view")

# Set the bounding box for the US
us_bbox <- c(-175, 17, -63, 73)

# Plot the regions
geographic_plot <- tm_shape(us_map) +
  tm_polygons("NAME", palette=c("United States"="#8DD08D"), alpha = 0.5, legend.show = FALSE) +
  
  # Dots for non-Ivy League universities
  tm_shape(geography_sf[geography_sf$Ivy_League == 0, ]) +
  tm_dots(size=0.05, 
          col="Control", palette=c("Public"='#F781BF', "Private (non-profit)"='#FFC20A'), 
          title="University Status") +
  
  # Dots for Ivy League universities
  tm_shape(geography_sf[geography_sf$Ivy_League == 1, ]) +
  tm_dots(size = 0.1, 
          col = "Ivy_League",
          palette=c("1"= "#FF0000", "0" = "grey"), 
          legend.show = TRUE,
          title = "Ivy League",
          labels = c("1" = "Ivy League", "0" = "Non-Ivy")) +
  
  tm_layout(title = "US R1 and R2 Universities") +
  
  # Set the view to the US bounding box
  tm_view(bbox = us_bbox)

geographic_plot

```



Is there any notable pattern to where the Ivy League universities concentrated? What about private and public universities? Do any parts of the United States appear particularly under-resourced in terms of research universities? How might you explain the patterns you observe?

#### Discussion of the Map

##### 1. Ivy Leagues are all on the East Coast

##### 2. Distribution of public and private universities

Public and private universities seemingly spread across the country. However, when displaying the OpenStreetMap instead of the default Esri.WorldGrayCanvas (can be selected by clicking on the filter icon in the top left corner) and zooming in, we can notice that most private universities are located in close proximity to larger cities (such as Los Angeles, Dallas, Chicago, Washington DC, New York City, Boston).

Several factors might explain this phenomenon, such as:

i. Access to Resources: Larger cities often offer better access to resources such as libraries, research facilities, cultural institutions, and industry partnerships. Private universities, which often rely on tuition and private funding, may be attracted to urban areas where these resources are abundant.

ii. Industry Connections: Proximity to major urban centers provides opportunities for private universities to establish strong connections with industries, businesses, and organisations. This can lead to collaborative research, internships, and employment opportunities for students.

iii. Networking Opportunities: Being close to a city facilitates networking opportunities for students and alumni. Private universities often emphasize networking and alumni relations, and being situated near a city with a robust job market can enhance students' chances of securing internships and jobs.

iv. International Attraction: Cities tend to attract a diverse population, including international students. Private universities, which may have a higher reliance on tuition revenue, may be attracted to urban areas for the potential to attract a broader and more diverse student body.

v. Philanthropy and Donor Base: Major cities are often home to a concentration of wealthy individuals and corporations. Private universities may be strategically located in proximity to potential donors, facilitating fundraising efforts and building a strong financial foundation.

However, it should be noted that in order to explain why private universities seemingly choose more urban locations, further research of the proposed factors would be necessary.

##### 3. Under-researched areas

It is noticeable, that the universities are mainly concentrated along the coastlines and in the East of the US. The Midwest/West and Alaska are relatively under-researched. Several factors might explain that observation:

i. Historical Development: Many of the oldest and most prestigious universities in the United States, such as Harvard and Princeton, were established in the colonial period and early years of the nation's history (Source: [Wikipedia](https://en.wikipedia.org/wiki/History_of_higher_education_in_the_United_States#:~:text=Explosive%20growth%20in%20the%20number,regional%20campuses%20around%20the%20state.)). These institutions often originated in the eastern part of the country. Potentially, the presence of these early institutions might have influenced the establishment of additional universities in nearby areas.

ii. Population Centers: The eastern and western coasts of the United States are home to some of the country's largest and most populous cities (Source: [Wikipedia](https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population)). Potentially, Universities might tend to develop in areas with large population centers.

iii. Economic Opportunities: Coastal areas might to have larger economic opportunities, including access to industries, businesses, and financial centers, potentially inspiring the establishment of higher education institutions.

iv. Transportation Infrastructure: As visualised in this [map of the US railway system](https://github.com/emmi3105/472_assignment3/blob/385882f6fed3a176ddc109860cf51011527df8c8/railway_map.png) the US American public transportation infrastructure is better developed in the Eastern part of the US compared to the Midwest. Potentially, access to public transportation is a motivating factor for establishing higher education institutions at specific places.






## Data

```{r close the database connection}

# Close the database connection
dbDisconnect(db)
```


## Sources



## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 


```
