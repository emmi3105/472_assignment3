---
title: "Assignment 3"
author: "Student ID: 201903536"
date: "30 November 2023"
output: html_document
---

```{r setup, include=FALSE} 
#####################################
# SETUP
#####################################

knitr::opts_chunk$set(echo = FALSE) 

#####################################
# Install/load packages
#####################################

library(rvest)
library(tidyverse)

```


## GitHub

The GitHub repository for **this assignment** can be found [here](https://github.com/emmi3105/472_assignment3).


## Exercise 1




## Exercise 2

### a. Gathering structured data
Write an automatic webscraping function in R that constructs a table (as e.g. a data frame, a tibble, or a data table) of all R1 (Very High Research Activity) **and** R2 (High Research Activity) Research Universities in the United States of America. These data can be found on [wikipedia](https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States). 

Your initial scraper should collect five variables:

i. The university’s name
ii. It’s status (public or private)
iii. The city in which it is located
iv. The state in which it is located
v. The URL of the university’s dedicated Wikipedia page


#### Write the function


```{r combine the two tables using for loops with url}

scrape_wikipedia_tables <- function(url, table_indices) {
  # Storing the URL's HTML code
  html_content <- read_html(url)
  
  # Extracting all tables in the document 
  tab <- html_table(html_content, fill = TRUE)

  # Extract only the tables of interest given the input table_indices
  selected_tables <- tab[1:length(table_indices)]

  # Iterate over tables and bind rows
  empty_tibble <- tibble()
  for (i in 1:length(selected_tables)) {
    current_table <- as_tibble(selected_tables[[i]][, 1:4])
    empty_tibble <- bind_rows(empty_tibble, current_table)
  }

  # Initialize an empty list to store hyperlinks
  hyperlinks_list <- list()

  # Iterate over the table indices and extract hyperlinks
  for (index in table_indices) {
    table_nodes <- html_content %>%
      html_nodes(paste("#mw-content-text > div.mw-content-ltr.mw-parser-output > table:nth-child(", index, ")", sep = ""))
  
    # Assuming you want to extract hyperlinks from the first table
    current_node_set <- table_nodes[[1]]
  
    hyperlinks_current_node <- current_node_set %>%
      html_nodes("td:nth-child(1) a") %>%
      html_attr("href")
  
    # Append the hyperlinks to the list
    hyperlinks_list <- c(hyperlinks_list, hyperlinks_current_node)
  }

  # Unlist to flatten the list
  hyperlinks_list <- unlist(hyperlinks_list, recursive = FALSE)


  # Concatenate the base URL to the extracted hyperlinks
  hyperlinks_list <- paste0("https://en.wikipedia.org/", hyperlinks_list)
  
  # Add a new column 'Wikipedia_URL' with the extracted URLs
  empty_tibble$Wikipedia_URL <- hyperlinks_list

  return(empty_tibble)
}

# Test the function using the url from the Wikipedia website and the two tables of interest:
url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"
indices_of_interest <- c(19, 27)

result <- scrape_wikipedia_tables(url, indices_of_interest)
print(head(result))

```





                   
## Exercise 3



## Data



## Sources



## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 


```
